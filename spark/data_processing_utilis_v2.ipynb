{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# Standard library\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "from typing import Optional, List, Dict, Any, Tuple, Set\n",
    "\n",
    "# PySpark SQL\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    current_timestamp,\n",
    "    hash as spark_hash,\n",
    "    lit,\n",
    "    regexp_replace,\n",
    "    to_date,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Delta Lake\n",
    "from delta.tables import DeltaTable\n",
    "from delta.exceptions import ConcurrentAppendException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Data processing utils - version 3.14\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(str(mssparkutils.env.getJobId()))\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "def extract_storage_account_name(url):\n",
    "    # Split the URL by '.' and get the first part\n",
    "    return url.split('.')[0].split('//')[-1]\n",
    "\n",
    "\n",
    "def mount_workspace_point(abfss_path, linked_service_name, mount_point_name):\n",
    "    mounts = mssparkutils.fs.mounts()\n",
    "\n",
    "    filtered_mount = next((mount for mount in mounts if mount.mountPoint == mount_point_name), None)\n",
    "\n",
    "    if filtered_mount is None:\n",
    "        mount_abfss_path = \"\"\n",
    "        mount_linkedService = \"\"\n",
    "    else:\n",
    "        mount_abfss_path = filtered_mount.source\n",
    "        mount_linkedService = filtered_mount.linkedService\n",
    "\n",
    "    ##Compare if there are no changes in moint point settings\n",
    "    if (mount_abfss_path != abfss_path or mount_linkedService != linked_service_name):\n",
    "        \n",
    "        try:\n",
    "            mssparkutils.fs.unmount(mount_point)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        mssparkutils.fs.mount(\n",
    "            f\"{abfss_path}\", \n",
    "            f\"{mount_point_name}\", \n",
    "            # We use here the linked service credentials\n",
    "            {\"linkedService\":f\"{linked_service_name}\", \"scope\":\"workspace\"} \n",
    "        )\n",
    "        \n",
    "        print(f\"Mounted: {mount_point_name} for {abfss_path}. Scope: workspace\")\n",
    "        logger.info(f\"Mounted: {mount_point_name} for {abfss_path}. Scope: workspace\")\n",
    "    else:\n",
    "        print(f\"Mount point {mount_point_name} EXIST for {abfss_path}. Scope: workspace\")\n",
    "        logger.info(f\"Mount point {mount_point_name} EXIST for {abfss_path}. Scope: workspace\")\n",
    "        \n",
    "def mount_from_linkedservice(linked_service_name, storage_container_name, scope = \"workspace\", env = None):\n",
    "    endpoint = json.loads(mssparkutils.credentials.getPropertiesAll(linked_service_name))['Endpoint']\n",
    "\n",
    "    storage_account_name = extract_storage_account_name(endpoint)\n",
    "\n",
    "    abfss_path = f\"abfss://{storage_container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "    mount_point = get_mount_name(linked_service_name, storage_container_name, scope, env)\n",
    "\n",
    "    #Unmount if exists and job scope\n",
    "    if scope==\"job\":\n",
    "        try:\n",
    "            mssparkutils.fs.unmount(mount_point)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "           \n",
    "            if scope==\"workspace\":\n",
    "                # Pernament mount\n",
    "                mount_workspace_point(abfss_path, linked_service_name, mount_point)\n",
    "             \n",
    "            else:\n",
    "                # Temporarily mount the prod container using to access all the required fields\n",
    "                mssparkutils.fs.mount(\n",
    "                    f\"{abfss_path}\", \n",
    "                    f\"{mount_point}\", \n",
    "                    # We use here the linked service credentials\n",
    "                    {\"linkedService\":f\"{linked_service_name}\", \"scope\":\"job\"} \n",
    "                )\n",
    "                print(f\"Mounted: {mount_point} for {abfss_path}. Scope: {scope}\")\n",
    "                logger.info(f\"Mounted: {mount_point} for {abfss_path}. Scope: {scope}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Write conflict detected on attempt {attempt+1}. Error: {e}. Retrying...\")\n",
    "            #Unmount if exists\n",
    "            try:\n",
    "                mssparkutils.fs.unmount(mount_point)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            time.sleep(30 ** attempt)  # Exponential backoff\n",
    "\n",
    "    spark_version = spark.version\n",
    "    main_version = \".\".join(spark_version.split(\".\")[:2])\n",
    "    job_id = mssparkutils.env.getJobId()\n",
    "\n",
    "\n",
    "    source_file_base_path = get_workspace_mount_synfs_path(mount_point, scope, env)\n",
    "   \n",
    "    # return mount and synfs path\n",
    "    return mount_point, source_file_base_path\n",
    "\n",
    "#Get synfs path for workspace mounts\n",
    "def get_workspace_mount_synfs_path(mount_point, scope = \"job\", env = None):\n",
    "    spark_version = spark.version\n",
    "    main_version = \".\".join(spark_version.split(\".\")[:2])\n",
    "    job_id = mssparkutils.env.getJobId()\n",
    "\n",
    "    if scope==\"workspace\":\n",
    "        synfs_path = f\"synfs:/workspace{mount_point}\"\n",
    "    else:\n",
    "        # Define synfs path for spark version\n",
    "        if (main_version=='3.3'):\n",
    "            synfs_path = f\"synfs:/{job_id}{mount_point}\"\n",
    "        elif (main_version=='3.4'):\n",
    "            synfs_path = f\"synfs:/notebook/{job_id}{mount_point}\"\n",
    "        else:\n",
    "            synfs_path = f\"synfs:/notebook/{job_id}{mount_point}\"\n",
    "\n",
    "    return synfs_path\n",
    "\n",
    "def get_mount_name(linked_service_name, storage_container_name, scope = \"job\", env = None):\n",
    "    if env == None:\n",
    "        mount_point = \"/\"+scope+\"/prod/\"+linked_service_name+\"/\"+storage_container_name+\"/\"\n",
    "    else: \n",
    "        mount_point = \"/\"+scope+\"/\"+env+\"/\"+linked_service_name+\"/\"+storage_container_name+\"/\"\n",
    "\n",
    "    return mount_point\n",
    "\n",
    "# Function to check if a Delta table exists\n",
    "def delta_table_exists(tableName):\n",
    "    try:\n",
    "        spark.table(tableName).limit(1).collect()\n",
    "        return True\n",
    "    except:\n",
    "        return False  \n",
    "\n",
    "def get_latest_path_from_yyyy_mm_dd(file_path):\n",
    "    for _ in range(0,3):\n",
    "        latest_folder = max(mssparkutils.fs.ls(file_path), key=lambda f:f.name)\n",
    "        file_path = latest_folder.path\n",
    "    return file_path\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "def create_table(metadata_json, dataframe, replace_where = None, table_name_suffix = None , skip_data_lineage = None, vacuum_table = None):\n",
    "\n",
    "    '''\n",
    "    Params:\n",
    "        target_table - name of table if saving as managed table in Synapse or saving to ADLS (linked servie)\n",
    "\n",
    "        target_schema - schema name if saving as managed table in Synapse\n",
    "\n",
    "            this will mount connection to ADLS\n",
    "        target_linked_service - Linked Service name where file file will be created\n",
    "        target_path - path name where file file will be created\n",
    "        target_container - container name where file file will be created\n",
    "\n",
    "    '''\n",
    "    #Read json metadata\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    #Get current notebook name\n",
    "    notebook_name = mssparkutils.runtime.context['currentNotebookName']\n",
    "\n",
    "    #Get evnironment\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    project_name = metadata[\"project_name\"]\n",
    "    \n",
    "    #Overwrite project name with parameter from target_table\n",
    "    target_project_name = metadata[\"target_table\"].get(\"project_name\",None)\n",
    "    if target_project_name:\n",
    "        project_name = target_project_name\n",
    "\n",
    "    incremental_column_name = metadata.get(\"incremental_column_name\",None)\n",
    "\n",
    "    admin_schema_name = metadata.get(\"admin_schema_name\",\"admin\")\n",
    "    admin_table_name = metadata.get(\"admin_table_name\",None)\n",
    "\n",
    "    partitionBy = metadata[\"target_table\"].get(\"partitionBy\",[])\n",
    "    format_type = metadata[\"target_table\"].get(\"format\",\"delta\")\n",
    "\n",
    "    logger.info(f\"ENV: {env}\")\n",
    "    print(f\"ENV: {env}\")\n",
    "    logger.info(f\"PROJECT_NAME: {project_name}\")\n",
    "    print(f\"PROJECT_NAME: {project_name}\")\n",
    "\n",
    "    #Target table defined only once\n",
    "    target_schema_name = metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "    if table_name_suffix:\n",
    "        target_table_name = target_table_name + table_name_suffix\n",
    "\n",
    "    target_ignore_env = metadata[\"target_table\"].get(\"params\",{}).get(\"ignore_env\",False) \n",
    "    target_ignore_project = metadata[\"target_table\"].get(\"params\",{}).get(\"ignore_project\",False) \n",
    "    target_drop_table = metadata[\"target_table\"].get(\"params\",{}).get(\"drop_table\",False) \n",
    "\n",
    "    target_linked_service = metadata[\"target_table\"].get(\"target_linked_service\",\"\") \n",
    "    target_path = metadata[\"target_table\"].get(\"target_path\",\"\") \n",
    "    target_container = metadata[\"target_table\"].get(\"target_container\",\"\") \n",
    "\n",
    "    synapse_target_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, target_ignore_env, target_ignore_project)\n",
    "\n",
    "\n",
    "    #Operation type\n",
    "    # Support both correct \"operation_type\" and legacy typo \"opertation_type\"\n",
    "    opertation_type = metadata[\"target_table\"].get(\"operation_type\", metadata[\"target_table\"].get(\"opertation_type\", None))\n",
    "\n",
    "    print(opertation_type)\n",
    "    logger.info(opertation_type)\n",
    "    if partitionBy:\n",
    "        print(f\"Partition by: {partitionBy}\")\n",
    "\n",
    "\n",
    "    ####        ---------------------------------------------------------------------\n",
    "    ####        ---------------------   \"Managed Table Type\"    ---------------------\n",
    "\n",
    "    if opertation_type == \"Managed Table Type\":\n",
    "\n",
    "        if target_drop_table:\n",
    "            ## Dropping the table\n",
    "            print(f\"Drop table {synapse_target_table_name}\")\n",
    "            logger.info(f\"Drop table {synapse_target_table_name}\")\n",
    "            spark.sql(\n",
    "                f\"DROP TABLE IF EXISTS {synapse_target_table_name}\"\n",
    "            )  # drop every time for now until everything will be checked (to avoid mergeschema)\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            logger.info(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            if format_type == 'parquet':\n",
    "                # Save as parquet\n",
    "                dataframe.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "            elif format_type == 'delta':\n",
    "                # Save as delta\n",
    "                dataframe.write.format(\"delta\").mode(\"overwrite\").option(\"parquet.vorder.enabled \",\"true\").option(\"overwriteSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            if \"Can not create the managed table\" in str(e):\n",
    "                #match = re.search(r'abfss://[\\w\\.-]+/[\\w\\.-]+', str(e))\n",
    "                #abfss_path = match.group(0)\n",
    "                #print(abfss_path)\n",
    "                \n",
    "                #Regular expression to extract the ABFSS path\n",
    "                pattern = r\"The associated location\\('([^']+)'\\)\"\n",
    "\n",
    "                # Search for the pattern in the error message\n",
    "                match = re.search(pattern, str(e))\n",
    "                if match:\n",
    "                    abfss_path = match.group(1)\n",
    "                    print(f\"Deleting files from ABFSS path: {abfss_path}\")\n",
    "                    mssparkutils.fs.rm(abfss_path, True)\n",
    "                    print(\"Existing files deleted. You can now recreate the table.\")\n",
    "                else:\n",
    "                    print(\"ABFSS path could not be extracted.\")\n",
    "                \n",
    "            #Create if location exists but managed table does not\n",
    "            print(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            logger.info(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "\n",
    "            if format_type == 'parquet':\n",
    "                # Save as parquet\n",
    "                dataframe.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "            elif format_type == 'delta':\n",
    "                # Save as delta\n",
    "                dataframe.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").option(\"parquet.vorder.enabled \",\"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "\n",
    "        ## Validating if table is populated\n",
    "        ## This would result to an error; halting the cell\n",
    "        assert spark.sql(f\"SELECT COUNT(*) as total_rows from {synapse_target_table_name}\").collect()[0]['total_rows'] >= 0\n",
    "\n",
    "\n",
    "    ####        -----------------------------------------------------------------------\n",
    "    ####        ---------------------    \"Linked Service Type\"    ---------------------\n",
    "\n",
    "    elif opertation_type == \"Linked Service Type\":\n",
    "\n",
    "        mount_scope = \"workspace\"\n",
    "        mount_point_name = get_mount_name(target_linked_service, target_container, mount_scope, env)\n",
    "\n",
    "        mount_point[mount_point_name], mount_synfs_path[mount_point_name] = mount_from_linkedservice(target_linked_service, target_container, mount_scope, env)\n",
    "\n",
    "        target_file_synfs_path = f\"{mount_synfs_path[mount_point_name]}{target_path}/{target_table_name}\"\n",
    "\n",
    "\n",
    "        if format_type == 'parquet':\n",
    "            # Save as parquet\n",
    "            print(f\"Save dataframe as parquet to: {target_file_synfs_path}\")\n",
    "            logger.info(f\"Save dataframe as parquet to: {target_file_synfs_path}\")\n",
    "            dataframe.coalesce(1).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partitionBy).parquet(target_file_synfs_path)\n",
    "        elif format_type == 'delta':\n",
    "            # Save as delta\n",
    "            print(f\"Save dataframe as delta to: {target_file_synfs_path}\")\n",
    "            logger.info(f\"Save dataframe as delta to: {target_file_synfs_path}\")\n",
    "            dataframe.coalesce(1).write.format(\"delta\").option(\"parquet.vorder.enabled \",\"true\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partitionBy).save(target_file_synfs_path)\n",
    "        elif format_type == 'csv':\n",
    "            # Save as csv\n",
    "            print(f\"Save dataframe as csv to: {target_file_synfs_path}\")\n",
    "            logger.info(f\"Save dataframe as csv to: {target_file_synfs_path}\")\n",
    "            csv_options = metadata[\"target_table\"].get(\"csv_options\",None) \n",
    "            count_rows = dataframe.count()\n",
    "            if count_rows == 0:\n",
    "                dataframe.coalesce(1).write.mode(\"overwrite\").options(**csv_options).csv(target_file_synfs_path)\n",
    "            else: \n",
    "                dataframe.coalesce(1).write.mode(\"overwrite\").options(**csv_options).partitionBy(*partitionBy).csv(target_file_synfs_path)\n",
    "        elif format_type == 'xlsx':\n",
    "            # Save as excel\n",
    "            print(f\"Save dataframe as excel to: {target_file_synfs_path}\")\n",
    "            logger.info(f\"Save dataframe as excel to: {target_file_synfs_path}\")\n",
    "            xlsx_options = metadata[\"target_table\"].get(\"xlsx_options\",None)\n",
    "            spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "            dataframe.write.format(\"com.crealytics.spark.excel\").options(**xlsx_options).mode(\"overwrite\").save(f\"{target_file_synfs_path}.xlsx\")\n",
    "        elif format_type == 'big_xlsx':\n",
    "            # Save as excel: Generate xlsx file using xlsxwriter with ZIP64 extensions enabled - Use this when the excel file is too big ~ 300k MiB - 400k+ MiB\n",
    "            print(f\"Save dataframe as excel to: {target_file_synfs_path}\")\n",
    "            logger.info(f\"Save dataframe as excel to: {target_file_synfs_path}\")\n",
    "            xlsx_options = metadata[\"target_table\"].get(\"xlsx_options\",None)\n",
    "\n",
    "            try:\n",
    "                # Convert to pandas DataFrame\n",
    "                import pandas as pd\n",
    "                pandas_df = dataframe.toPandas().reset_index(drop=True)\n",
    "\n",
    "                # Define the final destination path\n",
    "                endpoint = json.loads(mssparkutils.credentials.getPropertiesAll(target_linked_service))['Endpoint']\n",
    "\n",
    "                storage_account_name = extract_storage_account_name(endpoint)\n",
    "\n",
    "                abfss_path = f\"abfss://{target_container}@{storage_account_name}.dfs.core.windows.net{target_path}/{target_table_name}\"\n",
    "\n",
    "                excel_path =  f\"{abfss_path}.xlsx\"\n",
    "                print(f\"The excel path is: {excel_path}\")\n",
    "\n",
    "                with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "                    pandas_df.to_excel(writer, **xlsx_options)\n",
    "                    # Enable ZIP64\n",
    "                    writer.book.use_zip64()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during Excel conversion: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "\n",
    "    ####        ------------------------------------------------------------------------------------\n",
    "    ####        ---------------------   \"Managed Table Type - Merge by Key\"    ---------------------\n",
    "\n",
    "    elif opertation_type == \"Managed Table Type - Merge by Key\":   \n",
    "\n",
    "        merge_condition = metadata[\"target_table\"].get(\"merge_condition\",None)\n",
    "\n",
    "        target_table_exists = delta_table_exists(synapse_target_table_name)\n",
    "    \n",
    "        if not target_table_exists:\n",
    "            #Create if location exists but managed table does not\n",
    "            print(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            logger.info(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            dataframe.write.format(\"delta\").option(\"parquet.vorder.enabled \",\"true\").mode(\"overwrite\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "            #Set table exists to true\n",
    "            target_table_exists = True\n",
    "            print(\"Table created\") \n",
    "            logger.info(\"Table created\") \n",
    "\n",
    "        else:\n",
    "            target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "\n",
    "            source_columns = set(dataframe.columns)\n",
    "            target_columns = set(target_delta_table.toDF().columns)\n",
    "\n",
    "            # Find columns that are in the source but not in the target\n",
    "            new_columns = source_columns - target_columns\n",
    "            # Find columns that are in the target but not in the source\n",
    "            deleted_columns = target_columns - source_columns\n",
    "\n",
    "            # Check if there are new columns\n",
    "            if len(new_columns)>0:\n",
    "                print(f\"New columns found: {new_columns}\")\n",
    "                logger.info(f\"New columns found: {new_columns}\")\n",
    "                #Just add new columns without any data\n",
    "                dataframe.limit(0)\\\n",
    "                                .write\\\n",
    "                                .format(\"delta\")\\\n",
    "                                .option(\"mergeSchema\", \"true\")\\\n",
    "                                .mode(\"append\")\\\n",
    "                                .saveAsTable(synapse_target_table_name)\n",
    "                target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "\n",
    "            # Check if there are new columns\n",
    "            if len(deleted_columns)>0:\n",
    "                \n",
    "                df_drop_columns = target_delta_table.toDF()\n",
    "                # Remove columns from Delta table\n",
    "                for col_name in deleted_columns:\n",
    "                    df_drop_columns = df_drop_columns.drop(col(col_name))\n",
    "                    print(f\"Remove columns: {col_name}\")\n",
    "                    logger.info(f\"Remove columns: {col_name}\")\n",
    "\n",
    "                #Overwrite delta table with new schema\n",
    "                df_drop_columns\\\n",
    "                            .write\\\n",
    "                            .format(\"delta\")\\\n",
    "                            .mode(\"overwrite\")\\\n",
    "                            .option(\"overwriteSchema\", \"true\")\\\n",
    "                            .saveAsTable(synapse_target_table_name)\n",
    "                target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "                print(target_delta_table.toDF().columns)\n",
    "                logger.info(target_delta_table.toDF().columns)\n",
    "\n",
    "            print(f\"Merge to table: {synapse_target_table_name}\")\n",
    "            logger.info(f\"Merge to table: {synapse_target_table_name}\")\n",
    "\n",
    "            #Merge to target file\n",
    "            target_delta_table.alias(\"target\").merge(\n",
    "                dataframe.alias('source'),\n",
    "                merge_condition\n",
    "                ).whenNotMatchedInsertAll().whenMatchedUpdateAll().execute()\n",
    "\n",
    "\n",
    "    ####        ---------------------------------------------------------------------\n",
    "    ####        ---------------------   \"Managed Table Type - Append\"    ------------\n",
    "\n",
    "    if opertation_type == \"Managed Table Type - Append\":\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(f\"Append data to: {synapse_target_table_name} {format_type} table\")\n",
    "            logger.info(f\"Append data to: {synapse_target_table_name} {format_type} table\")\n",
    "            if format_type == 'parquet':\n",
    "                # Save as parquet\n",
    "                dataframe.write.mode(\"append\").option(\"mergeSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "            elif format_type == 'delta':\n",
    "                # Save as delta\n",
    "                dataframe.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            if \"Can not create the managed table\" in str(e):\n",
    "                #match = re.search(r'abfss://[\\w\\.-]+/[\\w\\.-]+', str(e))\n",
    "                #abfss_path = match.group(0)\n",
    "                #print(abfss_path)\n",
    "                \n",
    "                #Regular expression to extract the ABFSS path\n",
    "                pattern = r\"The associated location\\('([^']+)'\\)\"\n",
    "\n",
    "                # Search for the pattern in the error message\n",
    "                match = re.search(pattern, str(e))\n",
    "                if match:\n",
    "                    abfss_path = match.group(1)\n",
    "                    print(f\"Deleting files from ABFSS path: {abfss_path}\")\n",
    "                    mssparkutils.fs.rm(abfss_path, True)\n",
    "                    print(\"Existing files deleted. You can now recreate the table.\")\n",
    "                else:\n",
    "                    print(\"ABFSS path could not be extracted.\")\n",
    "                \n",
    "            #Create if location exists but managed table does not\n",
    "            print(f\"Append data to: {synapse_target_table_name} {format_type} table\")\n",
    "            logger.info(f\"Append data to: {synapse_target_table_name} {format_type} table\")\n",
    "\n",
    "            if format_type == 'parquet':\n",
    "                # Save as parquet\n",
    "                dataframe.write.mode(\"append\").option(\"mergeSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "            elif format_type == 'delta':\n",
    "                # Save as delta\n",
    "                dataframe.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "\n",
    "        if vacuum_table:\n",
    "            spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "            spark.sql(f\"VACUUM {synapse_target_table_name} RETAIN 0 HOURS\")\n",
    "\n",
    "        ## Validating if table is populated\n",
    "        ## This would result to an error; halting the cell\n",
    "        assert spark.sql(f\"SELECT COUNT(*) as total_rows from {synapse_target_table_name}\").collect()[0]['total_rows'] >= 0\n",
    "\n",
    "\n",
    "    elif opertation_type == \"Managed Table Type - Repace where\":\n",
    "\n",
    "        target_table_exists = delta_table_exists(synapse_target_table_name)\n",
    "\n",
    "        if not target_table_exists:\n",
    "            \n",
    "            #Create if location exists but managed table does not\n",
    "            print(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            logger.info(f\"Create {format_type} table: {synapse_target_table_name}\")\n",
    "            dataframe.write.format(\"delta\").option(\"parquet.vorder.enabled \",\"true\").mode(\"overwrite\").partitionBy(*partitionBy).saveAsTable(synapse_target_table_name)\n",
    "            #Set table exists to true\n",
    "            target_table_exists = True\n",
    "            print(\"Table created\") \n",
    "            logger.info(\"Table created\")\n",
    "\n",
    "        else:\n",
    "            target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "\n",
    "            source_columns = set(dataframe.columns)\n",
    "            target_columns = set(target_delta_table.toDF().columns)\n",
    "\n",
    "            # Find columns that are in the source but not in the target\n",
    "            new_columns = source_columns - target_columns\n",
    "            # Find columns that are in the target but not in the source\n",
    "            deleted_columns = target_columns - source_columns\n",
    "\n",
    "            # Check if there are new columns\n",
    "            if len(new_columns)>0:\n",
    "                print(f\"New columns found: {new_columns}\")\n",
    "                logger.info(f\"New columns found: {new_columns}\")\n",
    "                #Just add new columns without any data\n",
    "                dataframe.limit(0)\\\n",
    "                                .write\\\n",
    "                                .format(\"delta\")\\\n",
    "                                .option(\"mergeSchema\", \"true\")\\\n",
    "                                .mode(\"append\")\\\n",
    "                                .saveAsTable(synapse_target_table_name)\n",
    "                target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "\n",
    "            # Check if there are new columns\n",
    "            if len(deleted_columns)>0:\n",
    "                \n",
    "                df_drop_columns = target_delta_table.toDF()\n",
    "                # Remove columns from Delta table\n",
    "                for col_name in deleted_columns:\n",
    "                    df_drop_columns = df_drop_columns.drop(col(col_name))\n",
    "                    print(f\"Remove columns: {col_name}\")\n",
    "                    logger.info(f\"Remove columns: {col_name}\")\n",
    "\n",
    "                #Overwrite delta table with new schema\n",
    "                df_drop_columns\\\n",
    "                            .write\\\n",
    "                            .format(\"delta\")\\\n",
    "                            .mode(\"overwrite\")\\\n",
    "                            .option(\"overwriteSchema\", \"true\")\\\n",
    "                            .saveAsTable(synapse_target_table_name)\n",
    "                target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "                print(target_delta_table.toDF().columns)\n",
    "                logger.info(target_delta_table.toDF().columns)\n",
    "\n",
    "            #Create if location exists but managed table does not\n",
    "            print(f\"Create {format_type} table: {synapse_target_table_name} replace where {replace_where}\")\n",
    "            logger.info(f\"Create {format_type} table: {synapse_target_table_name} replace where {replace_where}\")\n",
    "\n",
    "\n",
    "            dataframe.write.format(\"delta\").option(\"parquet.vorder.enabled \",\"true\").mode(\"overwrite\").option(\"replaceWhere\", replace_where).partitionBy(*partitionBy).saveAsTable(finalTableName)\n",
    "\n",
    "        ## Validating if table is populated\n",
    "        ## This would result to an error; halting the cell\n",
    "        assert spark.sql(f\"SELECT COUNT(*) as total_rows from {synapse_target_table_name}\").collect()[0]['total_rows'] >= 0\n",
    "\n",
    "\n",
    "    ####        ---------------------------------------------------------------------\n",
    "    ####        -----------------   \"Incremental Table\"    --------------------------\n",
    "\n",
    "    elif opertation_type == \"Incremental Table\":\n",
    "        \"\"\"\n",
    "        Incremental table operation - detects Insert/Update/Delete changes between source and target.\n",
    "        Uses metadata structure with incremental_params:\n",
    "        {\n",
    "            \"id_key_column\": \"unique_id\",\n",
    "            \"included_columns_for_hash\": [...],\n",
    "            \"excluded_columns_for_hash\": [...],\n",
    "            \"log_history\": true,\n",
    "            \"history_table_name\": \"...\",\n",
    "            \"history_retention_days\": 30,\n",
    "            \"ignore_new_columns_as_change\": true\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract incremental parameters from metadata\n",
    "        incremental_params = metadata[\"target_table\"].get(\"incremental_params\", {})\n",
    "\n",
    "        id_key_column = incremental_params.get(\"id_key_column\")\n",
    "        if not id_key_column:\n",
    "            raise ValueError(\"Incremental Table operation requires 'id_key_column' in incremental_params\")\n",
    "\n",
    "        included_columns_for_hash = incremental_params.get(\"included_columns_for_hash\", None)\n",
    "        excluded_columns_for_hash = incremental_params.get(\"excluded_columns_for_hash\", None)\n",
    "        log_history = incremental_params.get(\"log_history\", False)\n",
    "        history_table_name = incremental_params.get(\"history_table_name\", None)\n",
    "        history_retention_days = incremental_params.get(\"history_retention_days\", None)\n",
    "        ignore_new_columns_as_change = incremental_params.get(\"ignore_new_columns_as_change\", True)\n",
    "\n",
    "        # Get additional create_table params\n",
    "        create_table_params = metadata[\"target_table\"].get(\"params\", {})\n",
    "        skip_data_lineage_param = create_table_params.get(\"skip_data_lineage\", False)\n",
    "\n",
    "        # Override with function parameter if provided\n",
    "        if skip_data_lineage is not None:\n",
    "            skip_data_lineage_param = skip_data_lineage\n",
    "\n",
    "        print(f\"Incremental Table operation for {synapse_target_table_name}\")\n",
    "        print(f\"  - ID key column: {id_key_column}\")\n",
    "        print(f\"  - Log history: {log_history}\")\n",
    "        if log_history and history_table_name:\n",
    "            print(f\"  - History table: {history_table_name}\")\n",
    "\n",
    "        logger.info(f\"Incremental Table operation for {synapse_target_table_name}\")\n",
    "        logger.info(f\"  ID key: {id_key_column}, log_history: {log_history}\")\n",
    "\n",
    "        # Check if target table exists\n",
    "        target_exists = delta_table_exists(synapse_target_table_name)\n",
    "\n",
    "        # ====================================================================================\n",
    "        # PHASE 1: FULL LOAD (target doesn't exist)\n",
    "        # ====================================================================================\n",
    "        if not target_exists:\n",
    "            print(f\"Target table does not exist. Performing FULL LOAD...\")\n",
    "            logger.info(f\"Target table does not exist. Performing FULL LOAD for {synapse_target_table_name}\")\n",
    "\n",
    "            # Add operation_type and last_update_dt columns for full load\n",
    "            from pyspark.sql import functions as F\n",
    "\n",
    "            df_full_load = dataframe\\\n",
    "                .withColumn(\"operation_type\", F.lit(\"I\"))\\\n",
    "                .withColumn(\"last_update_dt\", F.current_timestamp())\n",
    "\n",
    "            print(f\"  - Creating target table with {df_full_load.count()} records\")\n",
    "            logger.info(f\"Creating target table with {df_full_load.count()} records\")\n",
    "\n",
    "            # Create the table using existing create_table logic\n",
    "            # Build metadata for full load\n",
    "            metadata_for_full = metadata.copy()\n",
    "            metadata_for_full[\"target_table\"] = metadata[\"target_table\"].copy()\n",
    "            metadata_for_full[\"target_table\"][\"opertation_type\"] = \"Managed Table Type\"\n",
    "            metadata_for_full[\"target_table\"][\"operation_type\"] = \"Managed Table Type\"\n",
    "\n",
    "            # Use source_tables from metadata\n",
    "            metadata_json_full = json.dumps(metadata_for_full)\n",
    "\n",
    "            # Write using standard Managed Table Type\n",
    "            df_full_load.write.format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .option(\"parquet.vorder.enabled\", \"true\")\\\n",
    "                .option(\"overwriteSchema\", \"true\")\\\n",
    "                .partitionBy(*partitionBy)\\\n",
    "                .saveAsTable(synapse_target_table_name)\n",
    "\n",
    "            print(f\"âœ“ Full load completed for {synapse_target_table_name}\")\n",
    "            logger.info(f\"Full load completed for {synapse_target_table_name}\")\n",
    "\n",
    "            # Set return dataframe\n",
    "            dataframe = df_full_load\n",
    "\n",
    "        # ====================================================================================\n",
    "        # PHASE 2: DELTA LOAD (target exists)\n",
    "        # ====================================================================================\n",
    "        else:\n",
    "            print(f\"Target table exists. Performing DELTA LOAD...\")\n",
    "            logger.info(f\"Target table exists. Performing DELTA LOAD for {synapse_target_table_name}\")\n",
    "\n",
    "            from pyspark.sql import functions as F\n",
    "            from pyspark.sql.window import Window\n",
    "            from delta.tables import DeltaTable\n",
    "            import hashlib\n",
    "\n",
    "            # Load target table\n",
    "            target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "            df_target = target_delta_table.toDF()\n",
    "\n",
    "            # Get source from dataframe (already created by make_env_tables)\n",
    "            df_source = dataframe\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 1: Handle schema evolution - add new columns to target\n",
    "            # -------------------------------------------------------------------------\n",
    "            source_columns = set(df_source.columns)\n",
    "            target_columns = set(df_target.columns)\n",
    "\n",
    "            # Remove metadata columns from comparison\n",
    "            metadata_columns = {\"operation_type\", \"last_update_dt\", \"update_type\"}\n",
    "            source_columns_for_comparison = source_columns - metadata_columns\n",
    "            target_columns_for_comparison = target_columns - metadata_columns\n",
    "\n",
    "            new_columns = source_columns_for_comparison - target_columns_for_comparison\n",
    "\n",
    "            if len(new_columns) > 0:\n",
    "                print(f\"  - New columns detected: {new_columns}\")\n",
    "                logger.info(f\"New columns detected in source: {new_columns}\")\n",
    "\n",
    "                # Add new columns to target with NULL values\n",
    "                df_source.limit(0)\\\n",
    "                    .write\\\n",
    "                    .format(\"delta\")\\\n",
    "                    .option(\"mergeSchema\", \"true\")\\\n",
    "                    .mode(\"append\")\\\n",
    "                    .saveAsTable(synapse_target_table_name)\n",
    "\n",
    "                # Reload target\n",
    "                target_delta_table = DeltaTable.forName(spark, synapse_target_table_name)\n",
    "                df_target = target_delta_table.toDF()\n",
    "\n",
    "                print(f\"  - New columns added to target table\")\n",
    "                logger.info(f\"New columns added to target table\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 2: Determine hash columns\n",
    "            # -------------------------------------------------------------------------\n",
    "            # Get all columns excluding metadata\n",
    "            all_source_cols = [c for c in df_source.columns if c not in metadata_columns]\n",
    "\n",
    "            # Determine columns to track for changes\n",
    "            if included_columns_for_hash:\n",
    "                tracked_columns = [c for c in included_columns_for_hash if c in all_source_cols]\n",
    "            else:\n",
    "                tracked_columns = all_source_cols.copy()\n",
    "\n",
    "            # Apply exclusions\n",
    "            if excluded_columns_for_hash:\n",
    "                tracked_columns = [c for c in tracked_columns if c not in excluded_columns_for_hash]\n",
    "\n",
    "            # If new columns should not trigger changes, remove them from tracked columns\n",
    "            if ignore_new_columns_as_change and len(new_columns) > 0:\n",
    "                tracked_columns = [c for c in tracked_columns if c not in new_columns]\n",
    "\n",
    "            print(f\"  - Tracking {len(tracked_columns)} columns for changes\")\n",
    "            logger.info(f\"Tracking columns for changes: {tracked_columns}\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 3: Create hashed DataFrames\n",
    "            # -------------------------------------------------------------------------\n",
    "            def create_hash_column(df, columns_to_hash, hash_col_name):\n",
    "                \"\"\"Create hash column from specified columns\"\"\"\n",
    "                if not columns_to_hash:\n",
    "                    return df.withColumn(hash_col_name, F.lit(None))\n",
    "\n",
    "                # Concatenate all columns and create MD5 hash\n",
    "                concat_expr = F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"NULL\")) for c in columns_to_hash])\n",
    "                return df.withColumn(hash_col_name, F.md5(concat_expr))\n",
    "\n",
    "            # Hash for source\n",
    "            df_source_hashed = create_hash_column(df_source, tracked_columns, \"_hash_tracked\")\n",
    "            df_source_hashed = create_hash_column(df_source_hashed, all_source_cols, \"_hash_all\")\n",
    "\n",
    "            # Hash for target (only active records: I or U)\n",
    "            df_target_active = df_target.filter(F.col(\"operation_type\").isin([\"I\", \"U\"]))\n",
    "            df_target_hashed = create_hash_column(df_target_active, tracked_columns, \"_hash_tracked\")\n",
    "            df_target_hashed = create_hash_column(df_target_hashed, all_source_cols, \"_hash_all\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 4: Detect INSERTIONS (in source, not in target)\n",
    "            # -------------------------------------------------------------------------\n",
    "            df_insertions = df_source_hashed\\\n",
    "                .join(df_target_hashed, df_source_hashed[id_key_column] == df_target_hashed[id_key_column], \"left_anti\")\\\n",
    "                .select(df_source_hashed[\"*\"])\\\n",
    "                .withColumn(\"operation_type\", F.lit(\"I\"))\\\n",
    "                .withColumn(\"last_update_dt\", F.current_timestamp())\\\n",
    "                .withColumn(\"update_type\", F.lit(\"Insert\"))\n",
    "\n",
    "            num_insertions = df_insertions.count()\n",
    "            print(f\"  - Insertions detected: {num_insertions}\")\n",
    "            logger.info(f\"Insertions: {num_insertions}\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 5: Detect DELETIONS (in target, not in source)\n",
    "            # -------------------------------------------------------------------------\n",
    "            df_deletions = df_target_hashed\\\n",
    "                .join(df_source_hashed, df_target_hashed[id_key_column] == df_source_hashed[id_key_column], \"left_anti\")\\\n",
    "                .select(df_target_hashed[\"*\"])\\\n",
    "                .withColumn(\"operation_type\", F.lit(\"D\"))\\\n",
    "                .withColumn(\"update_type\", F.lit(\"Delete\"))\n",
    "                # Keep original last_update_dt for deletions\n",
    "\n",
    "            num_deletions = df_deletions.count()\n",
    "            print(f\"  - Deletions detected: {num_deletions}\")\n",
    "            logger.info(f\"Deletions: {num_deletions}\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 6: Detect UPDATES (tracked columns changed)\n",
    "            # -------------------------------------------------------------------------\n",
    "            df_updates_tracked = df_source_hashed.alias(\"src\")\\\n",
    "                .join(df_target_hashed.alias(\"tgt\"),\n",
    "                      F.col(\"src.\" + id_key_column) == F.col(\"tgt.\" + id_key_column),\n",
    "                      \"inner\")\\\n",
    "                .filter(F.col(\"src._hash_tracked\") != F.col(\"tgt._hash_tracked\"))\\\n",
    "                .select(\"src.*\")\\\n",
    "                .withColumn(\"operation_type\", F.lit(\"U\"))\\\n",
    "                .withColumn(\"last_update_dt\", F.current_timestamp())\\\n",
    "                .withColumn(\"update_type\", F.lit(\"Update-tracked\"))\n",
    "\n",
    "            num_updates_tracked = df_updates_tracked.count()\n",
    "            print(f\"  - Updates (tracked columns): {num_updates_tracked}\")\n",
    "            logger.info(f\"Updates (tracked): {num_updates_tracked}\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 7: Detect UPDATES (only untracked columns changed)\n",
    "            # -------------------------------------------------------------------------\n",
    "            df_updates_untracked = df_source_hashed.alias(\"src\")\\\n",
    "                .join(df_target_hashed.alias(\"tgt\"),\n",
    "                      F.col(\"src.\" + id_key_column) == F.col(\"tgt.\" + id_key_column),\n",
    "                      \"inner\")\\\n",
    "                .filter(\n",
    "                    (F.col(\"src._hash_tracked\") == F.col(\"tgt._hash_tracked\")) &\n",
    "                    (F.col(\"src._hash_all\") != F.col(\"tgt._hash_all\"))\n",
    "                )\\\n",
    "                .select([F.col(\"src.\" + c).alias(c) for c in df_source_hashed.columns])\\\n",
    "                .withColumn(\"operation_type\", F.lit(\"U\"))\\\n",
    "                .withColumn(\"last_update_dt\", F.col(\"tgt.last_update_dt\"))\\  # Keep original timestamp\n",
    "                .withColumn(\"update_type\", F.lit(\"Update-untracked\"))\n",
    "\n",
    "            num_updates_untracked = df_updates_untracked.count()\n",
    "            print(f\"  - Updates (untracked columns only): {num_updates_untracked}\")\n",
    "            logger.info(f\"Updates (untracked): {num_updates_untracked}\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 8: Detect REACTIVATIONS (previously deleted records returning)\n",
    "            # -------------------------------------------------------------------------\n",
    "            df_target_deleted = df_target.filter(F.col(\"operation_type\") == \"D\")\n",
    "\n",
    "            df_reactivations = df_source_hashed.alias(\"src\")\\\n",
    "                .join(df_target_deleted.alias(\"tgt\"),\n",
    "                      F.col(\"src.\" + id_key_column) == F.col(\"tgt.\" + id_key_column),\n",
    "                      \"inner\")\\\n",
    "                .select(\"src.*\")\\\n",
    "                .withColumn(\"operation_type\", F.lit(\"U\"))\\\n",
    "                .withColumn(\"last_update_dt\", F.current_timestamp())\\\n",
    "                .withColumn(\"update_type\", F.lit(\"Reactivate\"))\n",
    "\n",
    "            num_reactivations = df_reactivations.count()\n",
    "            if num_reactivations > 0:\n",
    "                print(f\"  - Reactivations detected: {num_reactivations}\")\n",
    "                logger.info(f\"Reactivations: {num_reactivations}\")\n",
    "\n",
    "            # -------------------------------------------------------------------------\n",
    "            # STEP 9: Combine all changes\n",
    "            # -------------------------------------------------------------------------\n",
    "            df_all_changes = df_insertions\\\n",
    "                .union(df_updates_tracked)\\\n",
    "                .union(df_updates_untracked)\\\n",
    "                .union(df_reactivations)\\\n",
    "                .union(df_deletions)\n",
    "\n",
    "            # Drop hash columns\n",
    "            df_all_changes = df_all_changes.drop(\"_hash_tracked\", \"_hash_all\")\n",
    "\n",
    "            total_changes = df_all_changes.count()\n",
    "            print(f\"  - Total changes to apply: {total_changes}\")\n",
    "            logger.info(f\"Total changes: {total_changes}\")\n",
    "\n",
    "            if total_changes == 0:\n",
    "                print(f\"âœ“ No changes detected. Target table is up to date.\")\n",
    "                logger.info(f\"No changes detected for {synapse_target_table_name}\")\n",
    "                dataframe = None\n",
    "            else:\n",
    "                # -------------------------------------------------------------------------\n",
    "                # STEP 10: Log to history table (optional)\n",
    "                # -------------------------------------------------------------------------\n",
    "                if log_history and history_table_name:\n",
    "                    print(f\"  - Logging changes to history table: {history_table_name}\")\n",
    "                    logger.info(f\"Logging to history: {history_table_name}\")\n",
    "\n",
    "                    # Get fully qualified history table name\n",
    "                    history_schema = metadata[\"target_table\"].get(\"target_schema\", target_schema_name)\n",
    "                    history_table_full = get_table_env_name(history_schema, history_table_name, env, project_name, target_ignore_env, target_ignore_project)\n",
    "\n",
    "                    # Add audit columns\n",
    "                    df_history = df_all_changes\\\n",
    "                        .withColumn(\"_audit_timestamp\", F.current_timestamp())\\\n",
    "                        .withColumn(\"_audit_operation\", F.col(\"update_type\"))\n",
    "\n",
    "                    # Append to history table\n",
    "                    df_history.write\\\n",
    "                        .format(\"delta\")\\\n",
    "                        .mode(\"append\")\\\n",
    "                        .option(\"mergeSchema\", \"true\")\\\n",
    "                        .saveAsTable(history_table_full)\n",
    "\n",
    "                    print(f\"  âœ“ Logged {total_changes} changes to history\")\n",
    "                    logger.info(f\"Logged {total_changes} changes to history table\")\n",
    "\n",
    "                    # Apply retention policy if specified\n",
    "                    if history_retention_days:\n",
    "                        retention_date = F.current_date() - F.expr(f\"INTERVAL {history_retention_days} DAYS\")\n",
    "                        history_delta = DeltaTable.forName(spark, history_table_full)\n",
    "                        history_delta.delete(F.col(\"_audit_timestamp\") < retention_date)\n",
    "                        print(f\"  âœ“ Applied retention policy: {history_retention_days} days\")\n",
    "                        logger.info(f\"Applied retention policy: {history_retention_days} days\")\n",
    "\n",
    "                # -------------------------------------------------------------------------\n",
    "                # STEP 11: Merge changes to target table\n",
    "                # -------------------------------------------------------------------------\n",
    "                print(f\"  - Merging changes to target table...\")\n",
    "                logger.info(f\"Merging {total_changes} changes to {synapse_target_table_name}\")\n",
    "\n",
    "                # Prepare final DataFrame for merge (drop update_type helper column)\n",
    "                df_for_merge = df_all_changes.drop(\"update_type\")\n",
    "\n",
    "                # Create merge condition\n",
    "                merge_condition = f\"target.{id_key_column} = source.{id_key_column}\"\n",
    "\n",
    "                # Perform merge\n",
    "                target_delta_table.alias(\"target\")\\\n",
    "                    .merge(df_for_merge.alias(\"source\"), merge_condition)\\\n",
    "                    .whenMatchedUpdateAll()\\\n",
    "                    .whenNotMatchedInsertAll()\\\n",
    "                    .execute()\n",
    "\n",
    "                print(f\"âœ“ Delta load completed for {synapse_target_table_name}\")\n",
    "                print(f\"  Summary: {num_insertions} inserts, {num_updates_tracked + num_updates_untracked} updates, {num_deletions} deletes, {num_reactivations} reactivations\")\n",
    "                logger.info(f\"Delta load completed: {num_insertions}I, {num_updates_tracked + num_updates_untracked}U, {num_deletions}D, {num_reactivations}R\")\n",
    "\n",
    "                # Set return dataframe\n",
    "                dataframe = df_all_changes\n",
    "\n",
    "        # Validate table is populated\n",
    "        row_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {synapse_target_table_name}\").collect()[0]['cnt']\n",
    "        print(f\"  - Final row count: {row_count}\")\n",
    "        logger.info(f\"Final row count in {synapse_target_table_name}: {row_count}\")\n",
    "        assert row_count >= 0\n",
    "\n",
    "\n",
    "    #If incremetnal load then add row to metadata \n",
    "    if incremental_column_name:\n",
    "        ## Log to metadata max of processing data from target table (column incremental_column_name)\n",
    "\n",
    "        processing_date_to_log = get_max_of_incremental_column_name(metadata_json)\n",
    "\n",
    "        log_data = get_log_data(metadata_json, processing_date_to_log)\n",
    "\n",
    "        log_to_metadata(log_data,metadata_json,admin_schema_name, admin_table_name)\n",
    "\n",
    "    if not skip_data_lineage:\n",
    "        log_to_datalineage(metadata_json, admin_schema_name, data_lineage_table_name)\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "def make_env_tables(metadata_json):\n",
    "    \n",
    "    \"\"\"\n",
    "    Instantiates temporary views and defines a target table from JSON metadata, supporting environment\n",
    "    and project customization. It enables specific source tables to bypass global environment (`env`)\n",
    "    or project name (`project_name`) settings, useful for integrating tables that do not conform to\n",
    "    standard naming or partitioning schemes.\n",
    "\n",
    "    Parameters:\n",
    "    - metadata_json (str): JSON string with details of target and source tables, environment (`env`),\n",
    "      and project name (`project_name`). `source_tables` includes each table's details and optionally,\n",
    "      a flag to ignore environment (`ignore_env`) or project (`ignore_project`) for that table.\n",
    "\n",
    "    The `ignore_env` and `ignore_project` options allow individual source tables to exclude global\n",
    "    environment or project context, facilitating flexibility in data source integration.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing :\n",
    "        - \"target_table\" key with synapse_target_table_name value\n",
    "        - \"source_tables\" with view name + full resolved schema/table names \n",
    "        - \"skipped_optional_view\" : list of views marked as 'optional' and which couldn't be loaded/found\n",
    "        - additional keys can be added as needed ...\n",
    "    - structure : {\n",
    "        \"target_table\": \"<resolved target>\",\n",
    "        \"source_tables\": { \"<view_name>\": \"<resolved source ref>\", ... },\n",
    "        \"skipped_optional_views\": [ ... ]\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a result dictionary to return\n",
    "    result = {\"skipped_optional_views\": {}, \"source_tables\": {}}\n",
    "\n",
    "    #Read json metadata\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    #Get job id to read data with synfs\n",
    "    job_id = mssparkutils.env.getJobId()\n",
    "\n",
    "    #Get project name\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "    #Get environment\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "\n",
    "    target_ignore_env = metadata[\"target_table\"].get(\"params\",{}).get(\"ignore_env\",False) \n",
    "    target_ignore_project = metadata[\"target_table\"].get(\"params\",{}).get(\"ignore_project\",False) \n",
    "\n",
    "    #Admin table name\n",
    "    final_admin_table_name =  get_table_env_name(metadata.get(\"admin_schema_name\",\"\"), metadata.get(\"admin_table_name\",\"\"), env, project_name, False, ignore_project = True)\n",
    " \n",
    "    \n",
    "    target_schema_name = metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "    target_opertation_type = metadata[\"target_table\"].get(\"opertation_type\",None)\n",
    "\n",
    "    if target_opertation_type == \"Linked Service Type\":\n",
    "        target_linked_service = metadata[\"target_table\"].get(\"target_linked_service\",\"\") \n",
    "        target_path = metadata[\"target_table\"].get(\"target_path\",\"\") \n",
    "        target_container = metadata[\"target_table\"].get(\"target_container\",\"\") \n",
    "\n",
    "        synapse_target_table_name = f\"{target_container}/{target_linked_service}/{target_path}/\"\n",
    "    else:\n",
    "        synapse_target_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, target_ignore_env, target_ignore_project)\n",
    "\n",
    "\n",
    "    #Multiple source tables\n",
    "    for source_table in metadata[\"source_tables\"]:\n",
    "\n",
    "        linked_service_name = source_table.get(\"linked_service_name\",None)\n",
    "        \n",
    "        #Set parameters\n",
    "        sql_where = None\n",
    "        temp_view_name = None\n",
    "        temp_view_project_name = None\n",
    "        source_project_name = None \n",
    "        source_ignore_env = False\n",
    "        source_ignore_project = False\n",
    "        skip = False\n",
    "        source_ref = None\n",
    "\n",
    "        temp_view_name = source_table.get(\"view_name\",None)\n",
    "        if temp_view_name is None:\n",
    "            temp_view_name = source_table.get(\"view\",None)\n",
    "\n",
    "        temp_view_project_name = source_table.get(\"project_name\",None)\n",
    "\n",
    "        #If project not declared then use \n",
    "        source_project_name = temp_view_project_name if temp_view_project_name else project_name\n",
    "\n",
    "        source_ignore_env       = source_table.get(\"params\",{}).get(\"ignore_env\",False) \n",
    "        source_ignore_project   = source_table.get(\"params\",{}).get(\"ignore_project\",False) \n",
    "        source_optional         = source_table.get(\"params\",{}).get(\"optional\", False)\n",
    "        \n",
    "        opertation_type = source_table.get(\"opertation_type\",\"lake database view\")\n",
    "\n",
    "        try:\n",
    "            ####        ---------------------------------------------------------------------\n",
    "            ####        ---------------------   \"Linked Service\"    -------------------------\n",
    "\n",
    "            if opertation_type == \"Linked Service\":\n",
    "                #Inicialize mount point if not exists in global variables\n",
    "\n",
    "                container = source_table.get(\"container\",None)\n",
    "                file_format = source_table.get(\"file_format\",\"parquet\")\n",
    "                mount_scope = source_table.get(\"mount_scope\",\"workspace\")\n",
    "\n",
    "                mount_point_name = get_mount_name(linked_service_name, container, mount_scope, env)\n",
    "\n",
    "                mount_point[mount_point_name], mount_synfs_path[mount_point_name] = mount_from_linkedservice(linked_service_name, container, mount_scope, env)\n",
    "\n",
    "                path = source_table.get(\"path\",None)\n",
    "                target_file_synfs_path = f\"{mount_synfs_path[mount_point_name]}{path}/\"\n",
    "                    \n",
    "                if source_table.get(\"load_type\",None) == \"lastest by YYYY/MM/DD\":\n",
    "                    target_file_synfs_path = get_latest_path_from_yyyy_mm_dd(target_file_synfs_path)\n",
    "\n",
    "                if source_table.get(\"path_suffix\",None):\n",
    "                    target_file_synfs_path = target_file_synfs_path+\"/\"+source_table.get(\"path_suffix\",\"\")\n",
    "\n",
    "                print(\"Create temp view: \"+temp_view_name+\" as linked service path: \"+ target_file_synfs_path)\n",
    "                logger.info(\"Create temp view: \"+temp_view_name+\" as linked service path: \"+ target_file_synfs_path)\n",
    "                source_ref = target_file_synfs_path\n",
    "                \n",
    "                if source_optional and not mssparkutils.fs.exists(target_file_synfs_path):\n",
    "                    skip = True\n",
    "                else:\n",
    "                    if file_format==\"parquet\":\n",
    "                        df = spark.read.option(\"ignoreCorruptFiles\", \"true\").parquet(target_file_synfs_path)\n",
    "                    elif file_format==\"delta\":\n",
    "                        df = spark.read.format(\"delta\").load(target_file_synfs_path)\n",
    "                    elif file_format==\"csv\":\n",
    "                        csv_options = source_table.get(\"csv_options\",None)\n",
    "                        df = spark.read.format(\"csv\").options(**csv_options).load(target_file_synfs_path)\n",
    "                    else:\n",
    "                        raise Exception(\"File format not supported\")\n",
    "\n",
    "                    # df is final product \n",
    "\n",
    "\n",
    "            ####        ---------------------------------------------------------------------\n",
    "            ####        -----------   \"delta sharing protocol - spark (JAR)\"    -------------\n",
    "\n",
    "            elif opertation_type == \"delta sharing\":\n",
    "                # Connect to delta sharing server \n",
    "                # params: key vault linked_service_name, secret with profile to connect to delta sharing protocol, table name\n",
    "\n",
    "                table_name = source_table.get(\"table\",None) # delta sharing table name  <share name>.<schema name>.<table name>\n",
    "                key_vaule_ls = source_table.get(\"key_vaule_ls\",None) # azure key vaule linked service name\n",
    "                secret_with_profile = source_table.get(\"profile\",None)  # akv secret with delta sharing profile (json)\n",
    "\n",
    "                try:\n",
    "                    profile = mssparkutils.credentials.getSecretWithLS(key_vaule_ls, secret_with_profile)\n",
    "                    table_url = profile + \"#\" + table_name\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred in detla sharing table adress {table_name}\")\n",
    "                    logger.info(f\"An error occurred in detla sharing table adress {table_name}\")\n",
    "                    raise\n",
    "\n",
    "                print(\"Create temp view: \"+temp_view_name+\" from delta sharing table: \"+ table_name)\n",
    "                logger.info(\"Create temp view: \"+temp_view_name+\" from delta sharing table: \"+ table_name)\n",
    "\n",
    "                df = spark.read.format(\"deltaSharing\").load(table_url)\n",
    "                source_ref = table_url\n",
    "\n",
    "\n",
    "            elif opertation_type == \"delta sharing 1.2.0\":\n",
    "                # Connect to delta sharing server \n",
    "                # params: key vault linked_service_name, secret with profile to connect to delta sharing protocol, table name\n",
    "\n",
    "                table_name = source_table.get(\"table\",None) # delta sharing table name  <share name>.<schema name>.<table name>\n",
    "                key_vaule_ls = source_table.get(\"key_vaule_ls\",None) # azure key vaule linked service name\n",
    "                secret_with_profile = source_table.get(\"profile\",None)  # akv secret with delta sharing profile (json)\n",
    "\n",
    "                if table_name is None:\n",
    "                    raise ValueError(\"table cannot be None\")\n",
    "                if key_vaule_ls is None:\n",
    "                    raise ValueError(\"key_vaule_ls cannot be None\")\n",
    "                if secret_with_profile is None:\n",
    "                    raise ValueError(\"profile cannot be None\")\n",
    "\n",
    "                table_url = f\"{key_vaule_ls};{secret_with_profile}#{table_name}\"\n",
    "\n",
    "                print(\"Create temp view: \"+temp_view_name+\" from delta sharing table: \"+ table_name)\n",
    "                logger.info(\"Create temp view: \"+temp_view_name+\" from delta sharing table: \"+ table_name)\n",
    "\n",
    "                df = spark.createDataFrame(delta_sharing.load_as_pandas(table_url))\n",
    "                source_ref = table_url\n",
    "\n",
    "\n",
    "            ####        ---------------------------------------------------------------------\n",
    "            ####        ---------------------   \"serverless jdbc ls\"    ---------------------\n",
    "\n",
    "            elif opertation_type == \"serverless jdbc ls\":\n",
    "                # Connect to serverless database with JDBC\n",
    "                # params: linked_service_name, view, table name\n",
    "\n",
    "                table_name = source_table.get(\"table\",None)\n",
    "\n",
    "                print(\"Create temp view: \"+temp_view_name+\" as JDBC linked service \"+linked_service_name+\" from table: \"+ table_name)\n",
    "                logger.info(\"Create temp view: \"+temp_view_name+\" as JDBC linked service \"+linked_service_name+\" from table: \"+ table_name)\n",
    "                df = getJDBCdataWithLinkedService(linked_service_name, table_name)\n",
    "\n",
    "            elif opertation_type == \"serverless jdbc ls sqlauth\":\n",
    "                # Connect to serverless database with JDBC\n",
    "                # params: linked_service_name, view, table name\n",
    "\n",
    "                table_name = source_table.get(\"table\",None)\n",
    "\n",
    "                print(\"Create temp view: \"+temp_view_name+\" as JDBC linked service \"+linked_service_name+\" from table: \"+ table_name)\n",
    "                logger.info(\"Create temp view: \"+temp_view_name+\" as JDBC linked service \"+linked_service_name+\" from table: \"+ table_name)\n",
    "                df = getJDBCdataWithLinkedServiceSQLAuth(linked_service_name, table_name)\n",
    "                source_ref = table_url\n",
    "\n",
    "                # df is final product \n",
    "\n",
    "\n",
    "            ####        ------------------------------Default--------------------------------\n",
    "            ####        ---------------------   \"lake database view\"    ---------------------\n",
    "\n",
    "            # Default operation type\n",
    "            elif opertation_type == \"lake database view\":\n",
    "                source_schema_name = source_table.get(\"schema\",None)\n",
    "                source_table_name = source_table.get(\"table\",None)\n",
    "\n",
    "                synapse_source_table_name = get_table_env_name(source_schema_name, source_table_name, env, source_project_name, source_ignore_env, source_ignore_project)\n",
    "\n",
    "                print(\"Create temp view: \"+temp_view_name+\" from table: \"+ synapse_source_table_name)\n",
    "                logger.info(\"Create temp view: \"+temp_view_name+\" from table: \"+ synapse_source_table_name)\n",
    "                source_ref = synapse_source_table_name\n",
    "\n",
    "                # If optional, skip cleanly when table is missing\n",
    "                if source_optional and not delta_table_exists(synapse_source_table_name):\n",
    "                    skip = True\n",
    "                else:\n",
    "                    df = spark.table(synapse_source_table_name)\n",
    "                \n",
    "                    # df is final product \n",
    "\n",
    "\n",
    "            # DEFINE TEMP VIEW\n",
    "\n",
    "            if skip:\n",
    "                print(f\"Skip optional view {temp_view_name} : table/path not found\")\n",
    "                logger.info(f\"Skip optional view {temp_view_name} : table/path not found\")\n",
    "                result[\"skipped_optional_views\"][temp_view_name] = source_ref\n",
    "                continue\n",
    "\n",
    "            #Get last processing date and filter source data\n",
    "            if source_table.get(\"load_type\",None)==\"incremental - by last target table processing date\":\n",
    "\n",
    "                synapse_source_table_name = None\n",
    "                if source_table.get(\"opertation_type\",None) == \"Linked Service\":\n",
    "                    linked_service_name = source_table.get(\"linked_service_name\",None)\n",
    "                    container = source_table.get(\"container\",None)\n",
    "                    path = source_table.get(\"path\",None)\n",
    "                    synapse_source_table_name = f\"{container}/{linked_service_name}/{path}/\"\n",
    "                elif source_table.get(\"opertation_type\",None) == \"serverless jdbc ls\":\n",
    "                    table_name = source_table.get(\"table\",None)\n",
    "                    synapse_source_table_name = get_table_env_name(source_schema_name, source_table_name, env, source_project_name, source_ignore_env, source_ignore_project)\n",
    "                elif source_table.get(\"opertation_type\",None) == \"serverless jdbc ls sqlauth\":\n",
    "                    table_name = source_table.get(\"table\",None)\n",
    "                    synapse_source_table_name = get_table_env_name(source_schema_name, source_table_name, env, source_project_name, source_ignore_env, source_ignore_project)\n",
    "                else:\n",
    "                    source_schema_name = source_table.get(\"schema\",None)\n",
    "                    source_table_name = source_table.get(\"table\",None)\n",
    "                    synapse_source_table_name = get_table_env_name(source_schema_name, source_table_name, env, source_project_name, source_ignore_env, source_ignore_project)\n",
    "\n",
    "                source_ignore_incremental_load = source_table.get(\"ignore_incremental_load\",\"N\")\n",
    "\n",
    "                if source_ignore_incremental_load == \"N\":\n",
    "                    # Incremental load\n",
    "                    # Target table defined only once\n",
    "                    target_schema_name = project_name+'_'+metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "                    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "                    process_date = metadata.get(\"process_date\",\"1900-01-01\")\n",
    "                    incremental_column_name = source_table.get(\"incremental_column_name\",\"LOAD_DATE\")\n",
    "\n",
    "                    delta_metadata_table = DeltaTable.forName(spark, final_admin_table_name).toDF().createOrReplaceTempView(\"metadata_temp_view\")\n",
    "                    metadata_df = spark.sql(f\"\"\"\n",
    "                        SELECT  COALESCE(MAX(modify_datetime),'1900-01-01') modify_datetime\n",
    "                        FROM    metadata_temp_view\n",
    "                        WHERE   lower(synapse_target_table_name) = lower('{synapse_target_table_name}')\n",
    "                        AND     lower(synapse_source_table_name) = lower('{synapse_source_table_name}')\n",
    "                        AND     load_type = 'incremental - by last target table processing date'\n",
    "                    \"\"\")\n",
    "                    metadata_modify_datetime = metadata_df.collect()[0]['modify_datetime']\n",
    "                    \n",
    "                    if process_date < metadata_modify_datetime:\n",
    "                        print(f\"Override processing date with {process_date} value\")\n",
    "                        logger.info(f\"Override processing date with {process_date} value\")\n",
    "                        query_processing_datetime = process_date\n",
    "                    else:\n",
    "                        query_processing_datetime = metadata_modify_datetime\n",
    "\n",
    "                    sql_where = f\"`{incremental_column_name}` > '{query_processing_datetime}'\"\n",
    "\n",
    "                elif source_ignore_incremental_load == \"Y\":\n",
    "                    #Ignore incremental load\n",
    "                    print(f\"Full load -> ignore incremental load\")\n",
    "\n",
    "            else:\n",
    "                sql_where = source_table.get(\"sql_where\",None)\n",
    "\n",
    "            if sql_where:\n",
    "                print(f\"Apply sql_where condition: {sql_where}\")\n",
    "                logger.info(f\"Apply sql_where condition: {sql_where}\")\n",
    "                df = df.filter(sql_where)\n",
    "\n",
    "            print(f\"Create view {temp_view_name}\")\n",
    "            logger.info(f\"Create view {temp_view_name}\")\n",
    "            # Create view\n",
    "            df.createOrReplaceTempView(temp_view_name)\n",
    "            if source_ref is not None:\n",
    "                result[\"source_tables\"][temp_view_name] = source_ref\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if source_optional:\n",
    "                print(f\"Skip optional view {temp_view_name} ; Error while making table {source_table} : {msg}\")\n",
    "                logger.info(f\"Skip optional view {temp_view_name} ; Error while making table {source_table} : {msg}\")\n",
    "                result[\"skipped_optional_views\"].append(temp_view_name)\n",
    "                continue\n",
    "            print(f\"Error while making table={source_table}, error: {msg}\")\n",
    "            raise\n",
    "\n",
    "    result[\"target_table\"] = synapse_target_table_name\n",
    "    return result\n",
    "    \n",
    "#############################################################################################################\n",
    "\n",
    "def log_to_metadata(filtered_rows:list, metadata_json, admin_schema_name = None, admin_table_name = None, max_retries = 5):\n",
    "   \n",
    "    #Metadata store\n",
    "\n",
    "    metadata = json.loads(metadata_json)\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "    #Target table defined only once\n",
    "    target_schema_name = metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "\n",
    "    if not admin_schema_name:\n",
    "        admin_schema_name = metadata[\"admin_schema_name\"]\n",
    "    if not admin_table_name:\n",
    "        admin_table_name = metadata[\"admin_table_name\"]\n",
    "\n",
    "    finalAdminTableName = get_table_env_name(admin_schema_name, admin_table_name, env, project_name, False, True)\n",
    "    \n",
    "    target_opertation_type = metadata[\"target_table\"].get(\"opertation_type\",None)\n",
    "\n",
    "    if target_opertation_type == \"Linked Service Type\":\n",
    "        target_linked_service = metadata[\"target_table\"].get(\"target_linked_service\",\"\") \n",
    "        target_path = metadata[\"target_table\"].get(\"target_path\",\"\") \n",
    "        target_container = metadata[\"target_table\"].get(\"target_container\",\"\") \n",
    "        synapse_target_table_name = f\"{target_container}/{target_linked_service}/{target_path}/\"\n",
    "    else:\n",
    "        synapse_target_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, False, False)\n",
    "\n",
    "\n",
    "    schema = StructType([   \n",
    "        StructField(\"project_name\", StringType(), False, {'comment': \"Name of project\"}),\n",
    "        StructField(\"load_type\", StringType(), True, {'comment': \"delta/full\"}),\n",
    "        StructField(\"synapse_source_table_name\", StringType(), True, {'comment': \"Name of source table in Synapse\"}),\n",
    "        StructField(\"synapse_target_table_name\", StringType(), True, {'comment': \"Name of target table in Synapse\"}),\n",
    "        StructField(\"params\", StringType(), True, {'comment': \"Partitions, merge conditions\"}),\n",
    "        StructField(\"modify_datetime\", TimestampType(), True, {'comment': \"Source file modification date\"}),\n",
    "        StructField(\"processing_date\", DateType(), False, {'comment': \"File processing date\"})\n",
    "    ])\n",
    "\n",
    "    merge_to_metadata_df = spark.createDataFrame(filtered_rows, schema)\n",
    "\n",
    "    delta_metadata_table = DeltaTable.forName(spark, finalAdminTableName)\n",
    "\n",
    "    #Write processing info to metadata column\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "\n",
    "            delta_metadata_table.alias(\"metadata\").merge(\n",
    "                merge_to_metadata_df.alias(\"processed\"),\n",
    "                f\"metadata.synapse_source_table_name = processed.synapse_source_table_name and metadata.params = processed.params and metadata.synapse_target_table_name = '{synapse_target_table_name}'\"\n",
    "            ).whenMatchedUpdate(\n",
    "                condition = \"processed.modify_datetime != '1900-01-01'\", \n",
    "                set = {\n",
    "                    \"project_name\": \"processed.project_name\",\n",
    "                    \"load_type\": \"processed.load_type\",\n",
    "                    \"synapse_source_table_name\": \"processed.synapse_source_table_name\",\n",
    "                    \"synapse_target_table_name\": \"processed.synapse_target_table_name\",\n",
    "                    \"params\": \"processed.params\",\n",
    "                    \"modify_datetime\": \"processed.modify_datetime\",\n",
    "                    \"processing_date\": \"processed.processing_date\"\n",
    "                }\n",
    "            ).whenMatchedUpdate(\n",
    "                condition = \"processed.modify_datetime = '1900-01-01'\",\n",
    "                set = {\n",
    "                    \"project_name\": \"processed.project_name\",\n",
    "                    \"load_type\": \"processed.load_type\",\n",
    "                    \"synapse_source_table_name\": \"processed.synapse_source_table_name\",\n",
    "                    \"synapse_target_table_name\": \"processed.synapse_target_table_name\",\n",
    "                    \"params\": \"processed.params\",\n",
    "                    \"processing_date\": \"processed.processing_date\"\n",
    "                    # Nie aktualizujemy modify_datetime gdy jest 1900-01-01\n",
    "                }\n",
    "            ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "            print(f\"Write metadata successful to {finalAdminTableName}\")\n",
    "            logger.info(f\"Write metadata successful to {finalAdminTableName}\")\n",
    "            break\n",
    "        except ConcurrentAppendException as e:\n",
    "            print(f\"Write conflict detected on attempt {attempt+1}. Error: {e}. Retrying...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            logger.info(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "def create_sql_condition(json_strings):\n",
    "    \"\"\"\n",
    "    Generates a SQL condition string from a list of JSON strings that contain a 'partitions' dictionary.\n",
    "    \n",
    "    Each JSON string should contain a dictionary with a single key 'partitions', inside of which there is\n",
    "    another dictionary with key-value pairs. The function identifies all unique keys within 'partitions'\n",
    "    across all given JSON strings, collects unique values for each key, and generates a part of a SQL\n",
    "    query that selects records where the key matches any of the collected values. If multiple keys are\n",
    "    found, their conditions are combined with the 'OR' operator.\n",
    "\n",
    "    Parameters:\n",
    "    - json_strings (list of str): A list of strings, where each string is a JSON representation of a dictionary\n",
    "      with a 'partitions' key.\n",
    "\n",
    "    Returns:\n",
    "    - str: A SQL condition string that can be used in a WHERE clause, containing conditions for all unique\n",
    "      keys found in the input JSON strings, combined with 'OR' if necessary.\n",
    "\n",
    "    Raises:\n",
    "    - json.JSONDecodeError: If any of the strings in the input list is not a valid JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_values_by_key = {}\n",
    "\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            for key, value in data[\"partitions\"].items():\n",
    "                if key not in unique_values_by_key:\n",
    "                    unique_values_by_key[key] = set()\n",
    "                unique_values_by_key[key].add(value)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise json.JSONDecodeError(f\"Error deserializing JSON: {json_str}\") from e\n",
    "\n",
    "    # Creating SQL conditions for each key\n",
    "    sql_parts = []\n",
    "    for key, values in unique_values_by_key.items():\n",
    "        values_str = \",\".join(f\"'{v}'\" for v in values)\n",
    "        sql_parts.append(f\"`{key}` IN ({values_str})\")\n",
    "\n",
    "    # Combining all conditions with 'OR' if there are multiple keys\n",
    "    sql_query = \" AND \".join(sql_parts)\n",
    "\n",
    "    return sql_query\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "def partitions_to_process(metadata_json, admin_schema_name, admin_table_name_to_process, admin_table_name_to_raw):\n",
    "\n",
    "    #Metadata store\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "    target_schema_name = project_name+'_'+metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "    ## Formulating the target table name\n",
    "    if env:\n",
    "        admin_schema_name = admin_schema_name +\"_\"+ env\n",
    "        target_schema_name = target_schema_name +\"_\"+ env\n",
    "    final_admin_table_name_to_process = f\"{admin_schema_name}.{admin_table_name_to_process}\"\n",
    "    final_admin_table_name_to_raw = f\"{admin_schema_name}.{admin_table_name_to_raw}\"\n",
    "    finalTableName = f\"{target_schema_name}.{target_table_name}\"\n",
    "\n",
    "    print(\"Get partitions to process...\")\n",
    "    logger.info(\"Get partitions to process...\")\n",
    "\n",
    "    spark.table(final_admin_table_name_to_process).createOrReplaceTempView(\"admin_process\")\n",
    "    spark.table(final_admin_table_name_to_raw).createOrReplaceTempView(\"admin_raw\")\n",
    "\n",
    "\n",
    "    schema = StructType([   \n",
    "        StructField(\"project_name\", StringType(), False, {'comment': \"Name of project\"}),\n",
    "        StructField(\"load_type\", StringType(), True, {'comment': \"delta/full\"}),\n",
    "        StructField(\"synapse_source_table_name\", StringType(), True, {'comment': \"Name of source table in Synapse\"}),\n",
    "        StructField(\"synapse_target_table_name\", StringType(), True, {'comment': \"Name of target table in Synapse\"}),\n",
    "        StructField(\"params\", StringType(), True, {'comment': \"Partitions, merge conditions\"}),\n",
    "        StructField(\"modify_datetime\", DateType(), True, {'comment': \"Source file modification date\"}),\n",
    "        StructField(\"processing_date\", DateType(), False, {'comment': \"File processing date\"})\n",
    "    ])\n",
    "    union_df = spark.createDataFrame([], schema)\n",
    "\n",
    "    #Multiple source tables\n",
    "    for source_table in metadata[\"source_tables\"]:\n",
    "\n",
    "        schema_source_name = source_table.get(\"schema\",None)\n",
    "        table_source_name = source_table.get(\"table\",None)\n",
    "        temp_view_name = source_table.get(\"view\",None)\n",
    "        temp_view_project_name = source_table.get(\"project_name\",None)\n",
    "\n",
    "        project_name = temp_view_project_name if temp_view_project_name else project_name\n",
    "\n",
    "        param = source_table.get(\"params\",None) \n",
    "\n",
    "        if param:\n",
    "            env = None if param.get(\"ignore_env\",False) else env\n",
    "            project_name = None if param.get(\"ignore_project\",False) else project_name\n",
    "\n",
    "        ## Formulating the target table name\n",
    "\n",
    "        if project_name:\n",
    "            schema_source_name = project_name+\"_\"+schema_source_name\n",
    "\n",
    "        if env:\n",
    "            schema_source_name = schema_source_name +\"_\"+ env\n",
    "        finalSourceTableName = f\"{schema_source_name}.{table_source_name}\"\n",
    "\n",
    "\n",
    "        partition_to_process_df = spark.sql(f\"\"\"\n",
    "                SELECT       r.project_name\n",
    "                            ,r.load_type\n",
    "                            ,lower(r.synapse_name) as synapse_source_table_name\n",
    "                            ,lower('{finalTableName}') as synapse_target_table_name\n",
    "                            ,r.params\n",
    "                            ,r.modify_datetime\n",
    "                            ,r.processing_date\n",
    "                FROM        admin_raw r \n",
    "                LEFT JOIN   admin_process p \n",
    "                ON          r.project_name = p.project_name\n",
    "                AND         r.load_type = p.load_type\n",
    "                AND         lower(r.synapse_name) = lower(p.synapse_source_table_name)\n",
    "                AND         lower(synapse_target_table_name) = lower('{finalTableName}')\n",
    "                AND         r.params = p.params\n",
    "                AND         r.processing_date = p.processing_date\n",
    "                WHERE       lower(r.synapse_name) = lower('{finalSourceTableName}')\n",
    "                AND         p.project_name IS NULL\n",
    "            \"\"\")\n",
    "\n",
    "        print(f\"{finalTableName} -> Partitions to process: {partition_to_process_df.count()}\")\n",
    "        logger.info(f\"{finalTableName} -> Partitions to process: {partition_to_process_df.count()}\")\n",
    "\n",
    "\n",
    "        union_df = union_df.union(partition_to_process_df)\n",
    "\n",
    "    c = union_df.cache().collect()\n",
    "    sorted_rows = sorted(c, key=lambda row: row.params)\n",
    "\n",
    "    return c\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def getJDBCdataWithLinkedService(linked_service_name, tableName) -> DataFrame:\n",
    "    \"\"\" \n",
    "    Retrieves data from a specified table in a SQL database via JDBC using credentials from a linked service in Azure Synapse.\n",
    "\n",
    "    This function extracts the database endpoint, name, and access token from the specified linked service. \n",
    "    It then constructs a JDBC URL and uses it to read the specified table into a DataFrame using Spark's JDBC capabilities.\n",
    "\n",
    "    Parameters:\n",
    "        linked_service_name (str): The name of the linked service in Azure Synapse which contains the connection information.\n",
    "        tableName (str): The name of the table from which to fetch the data. Ex: \"dbo.v_table_name\"\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the data from the specified table in the database.\n",
    "    \"\"\"\n",
    "    db_properties={}  \n",
    "    linked_service_params = json.loads(mssparkutils.credentials.getPropertiesAll(linked_service_name))\n",
    "\n",
    "    endpoint = linked_service_params['Endpoint']\n",
    "    # The 'Endpoint' represents the fully qualified domain name (FQDN) or IP address of the SQL Server.\n",
    "    # Example: \"sqlserver.database.windows.net\"\n",
    "\n",
    "    database = linked_service_params['Database']\n",
    "    # The 'Database' specifies the name of the SQL Server database to connect to.\n",
    "    # Example: \"my_database\"\n",
    "\n",
    "    db_properties[\"accessToken\"] = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
    "    # The \"accessToken\" retrieves an access token for the database connection from the linked service.\n",
    "    # This token is used in scenarios where token-based authentication (e.g., Azure Active Directory) is required.\n",
    "    # It provides a secure alternative to using static credentials like username and password.\n",
    "\n",
    "    db_properties[\"driver\"] = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    # The \"driver\" specifies the JDBC driver class for connecting to SQL Server.\n",
    "    # In this case, it uses the Microsoft SQL Server JDBC Driver.\n",
    "    # This driver must be available in the Spark environment to establish a connection.\n",
    "\n",
    "    print(mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name))\n",
    "    #print(f\"Load data from Database: {database}, table: {tableName}\")\n",
    "    logger.info(f\"Load data from Database: {database}, table: {tableName}\")\n",
    "\n",
    "    df = spark.read.jdbc(f\"jdbc:sqlserver://{endpoint};databaseName={database}\",tableName,properties=db_properties)\n",
    "\n",
    "    return df\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def getJDBCdataWithLinkedServiceSQLAuth(linked_service_name, tableName) -> DataFrame:\n",
    "    \"\"\" \n",
    "    Retrieves data from a specified table in a SQL database via JDBC using credentials from a linked service in Azure Synapse.\n",
    "\n",
    "    This function extracts connection parameters such as the database endpoint, name, user credentials, and authentication key \n",
    "    from the specified linked service in Azure Synapse. It then constructs a JDBC URL and fetches the specified table into \n",
    "    a Spark DataFrame using Spark's JDBC capabilities.\n",
    "\n",
    "    Parameters:\n",
    "        linked_service_name (str): The name of the linked service in Azure Synapse which contains the connection information.\n",
    "        tableName (str): The name of the table from which to fetch the data. Ex: \"dbo.v_table_name\"\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the data from the specified table in the database.\n",
    "    \"\"\"\n",
    "    db_properties={}  \n",
    "    linked_service_params = json.loads(mssparkutils.credentials.getPropertiesAll(linked_service_name))\n",
    "    print(linked_service_params)\n",
    "    # Extract connection parameters from the linked service\n",
    "    endpoint = linked_service_params['Endpoint']\n",
    "    # The 'Endpoint' represents the fully qualified domain name (FQDN) or IP address of the SQL Server.\n",
    "    # Example: \"sqlserver.database.windows.net\"\n",
    "\n",
    "    database = linked_service_params['Database']\n",
    "    # The 'Database' specifies the name of the SQL Server database to connect to.\n",
    "    # Example: \"my_database\"\n",
    "\n",
    "    user = linked_service_params['Id']\n",
    "    # The 'Id' corresponds to the username used for authentication with the SQL Server.\n",
    "    # This is retrieved from the linked service configuration in Azure Synapse.\n",
    "\n",
    "    password = linked_service_params['AuthKey']\n",
    "    # The 'AuthKey' is the password or authentication key associated with the username (Id).\n",
    "    # It is securely stored in the linked service and fetched programmatically.\n",
    "    #db_properties[\"accessToken\"] = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
    "\n",
    "    db_properties[\"driver\"] = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    # The \"driver\" specifies the JDBC driver class for connecting to SQL Server.\n",
    "    # In this case, it uses the Microsoft SQL Server JDBC Driver.\n",
    "    # This driver must be available in the Spark environment to establish a connection.\n",
    "    \n",
    "    print(mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name))\n",
    "    #print(f\"Load data from Database: {database}, table: {tableName}\")\n",
    "    logger.info(f\"Load data from Database: {database}, table: {tableName}\")\n",
    "\n",
    "    df = spark.read.jdbc(f\"jdbc:sqlserver://{endpoint};databaseName={database};user={user};password={password}\",tableName)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def format_column_names(df, forbidden_characters=\"+/? .,;{}()\\n\\t=\"):\n",
    "    \"\"\"\n",
    "    Function take dataframe and replaces forbidden characters with underscores in column names.\n",
    "    It also replaces & sign with _and_.\n",
    "    It also replaces % sign with _percent_\n",
    "    Then it removes any leading, trailing and double underscores from column names.\n",
    "\n",
    "    Parameters:\n",
    "        df: spark dataframe as input\n",
    "        forbidden_characters: characters to be removed from column names\n",
    "\n",
    "    Returns:\n",
    "        df: formated spark dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df_columns = df.columns\n",
    "    forbidden_chars = [\n",
    "        forbidden_characters[i] for i in range(len(forbidden_characters))\n",
    "    ]\n",
    "    for column in df_columns:\n",
    "        old_column_name = str(column)\n",
    "        for c in forbidden_chars:  # replace forbidden strings\n",
    "            column = column.replace(c, \"_\")\n",
    "        column = column.strip(\"_\")  # trim `_` at the start and end of the name\n",
    "        column = column.replace(\"&\", \"_and_\")\n",
    "        column = column.replace(\"%\", \"_percent_\")\n",
    "        while \"__\" in column:  # remove any `__` pairs\n",
    "            column = column.replace(\"__\", \"_\")\n",
    "        column = column.lower()\n",
    "        if column != old_column_name:  # rename column if name has changed\n",
    "            df = df.withColumnRenamed(old_column_name, column)\n",
    "    return df\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def get_table_env_name(schema_name, table_name, env, project_name, ignore_env = None, ignore_project = None):\n",
    "    \"\"\"\n",
    "    Generates a fully qualified table name by combining schema, table, environment, and project name information.\n",
    "\n",
    "    Parameters:\n",
    "    schema_name (str): The base name of the schema. This is the initial part of the schema name that will be combined \n",
    "                       with the project and environment names if provided.\n",
    "    table_name (str): The name of the table. This is the final part of the fully qualified table name.\n",
    "    env (str): The environment name (e.g., 'dev', 'prod', 'test'). This will be appended to the schema name \n",
    "               unless `ignore_env` is set to `True`.\n",
    "    project_name (str): The name of the project. This will be prefixed to the schema name unless `ignore_project` is \n",
    "                        set to `True`.\n",
    "    ignore_env (bool, optional): If set to `True`, the environment name will not be appended to the schema name. \n",
    "                                 Defaults to `None`.\n",
    "    ignore_project (bool, optional): If set to `True`, the project name will not be prefixed to the schema name. \n",
    "                                     Defaults to `None`.\n",
    "\n",
    "    Returns:\n",
    "    str: The fully qualified table name in the format `schema_name.table_name`, where `schema_name` is optionally \n",
    "         prefixed by the project name and/or suffixed by the environment name, depending on the provided parameters \n",
    "         and flags.\n",
    "\n",
    "    Function Logic:\n",
    "    1. Check `ignore_env` and `ignore_project` Flags:\n",
    "        - If `ignore_env` is `True`, set `env` to `None`.\n",
    "        - If `ignore_project` is `True`, set `project_name` to `None`.\n",
    "\n",
    "    2. Formulate the Schema Name:\n",
    "        - If `project_name` is provided and `ignore_project` is not `True`, prefix `schema_name` with `project_name`.\n",
    "        - If `env` is provided and `ignore_env` is not `True`, suffix `schema_name` with `env`.\n",
    "\n",
    "    3. Combine Schema Name and Table Name:\n",
    "        - Construct the final table name in the format `schema_name.table_name`.\n",
    "\n",
    "    4. Return the Final Table Name:\n",
    "        - Return the constructed fully qualified table name.\n",
    "    \"\"\"\n",
    "\n",
    "    if ignore_env:\n",
    "        env = None\n",
    "    if ignore_project:\n",
    "        project_name = None\n",
    "\n",
    "    ## Formulating the target table name\n",
    "\n",
    "    if project_name:\n",
    "        schema_name = project_name+\"_\"+schema_name\n",
    "\n",
    "    if env:\n",
    "        schema_name = schema_name +\"_\"+ env\n",
    "    finalTableName = f\"{schema_name}.{table_name}\"\n",
    "\n",
    "    return finalTableName\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def get_max_of_incremental_column_name(metadata_json):\n",
    "\n",
    "    #Metadata store\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "    target_schema_name = metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "    target_ignore_env = metadata[\"target_table\"].get(\"params\",{}).get(\"ignore_env\",False) \n",
    "    target_ignore_project = metadata[\"target_table\"].get(\"params\",{}).get(\"ignore_project\",False) \n",
    "    target_drop_table = metadata[\"target_table\"].get(\"params\",{}).get(\"drop_table\",False) \n",
    "\n",
    "    target_linked_service = metadata[\"target_table\"].get(\"target_linked_service\",\"\") \n",
    "    target_path = metadata[\"target_table\"].get(\"target_path\",\"\") \n",
    "    target_container = metadata[\"target_table\"].get(\"target_container\",\"\") \n",
    "\n",
    "    opertation_type = metadata[\"target_table\"].get(\"opertation_type\",None)\n",
    "    incremental_column_name =  metadata.get(\"incremental_column_name\",\"LOAD_DATE\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        if opertation_type == \"Linked Service Type\":\n",
    "\n",
    "            mount_scope = \"workspace\"\n",
    "            mount_point_name = get_mount_name(target_linked_service, target_container, mount_scope, env)\n",
    "            mount_point[mount_point_name], mount_synfs_path[mount_point_name] = mount_from_linkedservice(target_linked_service, target_container, mount_scope, env)\n",
    "            target_file_synfs_path = f\"{mount_synfs_path[mount_point_name]}{target_path}/{target_table_name}\"    \n",
    "            format_type = metadata[\"target_table\"].get(\"format\",\"delta\")\n",
    "            \n",
    "            if format_type == 'parquet':\n",
    "                dataframe = spark.read.parquet(target_file_synfs_path)\n",
    "            elif format_type == 'delta':\n",
    "                dataframe = spark.read.format(\"delta\").load(target_file_synfs_path)\n",
    "            elif format_type == 'csv':\n",
    "                csv_options = metadata[\"target_table\"].get(\"csv_options\",None)\n",
    "                dataframe = spark.read.options(**csv_options).csv(target_file_synfs_path)\n",
    "            elif format_type == 'xlsx':\n",
    "                xlsx_options = metadata[\"target_table\"].get(\"xlsx_options\",None)\n",
    "                dataframe = spark.read.format(\"com.crealytics.spark.excel\").options(**xlsx_options).load(f\"{target_file_synfs_path}.xlsx\")\n",
    "            synapse_target_table_name = \"target_view\"\n",
    "            dataframe.createOrReplaceTempView(synapse_target_table_name)\n",
    "\n",
    "        else: \n",
    "            synapse_target_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, target_ignore_env, target_ignore_project)\n",
    "\n",
    "\n",
    "\n",
    "        max_value_from_table = spark.sql(f\"\"\"SELECT MAX(`{incremental_column_name}`) FROM {synapse_target_table_name}\"\"\").collect()[0][0]\n",
    "        print(f\"Max value of {incremental_column_name} columns is: {max_value_from_table}\")\n",
    "        logger.info(f\"Max value of {incremental_column_name} columns is: {max_value_from_table}\")\n",
    "    except:\n",
    "        print(f\"No max value found for {incremental_column_name} columns\")\n",
    "        logger.info(f\"No max value found for {incremental_column_name} columns\")\n",
    "        max_value_from_table = None\n",
    "\n",
    "\n",
    "    return max_value_from_table\n",
    "#####################################################################\n",
    "\n",
    "def get_log_data(metadata_json, processing_date_to_log):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates log data for each source table defined in the provided metadata JSON.\n",
    "\n",
    "    Parameters:\n",
    "    metadata_json (str): A JSON string containing metadata information. \n",
    "\n",
    "    Returns:\n",
    "    list: A list of Spark Row objects, each containing logging information for a source table.\n",
    "\n",
    "    Function Logic:\n",
    "    1. Parse the metadata JSON to extract necessary information.\n",
    "    2. Generate the fully qualified target table name using the `get_table_env_name` function.\n",
    "    3. Iterate over each source table in the metadata and determine the source table's fully qualified name based on the operation type.\n",
    "    4. Create a Spark Row object for each source table with relevant logging information and add it to the rows list.\n",
    "    5. Return the list of Row objects.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Metadata store\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "    target_schema_name = metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "\n",
    "    target_opertation_type = metadata[\"target_table\"].get(\"opertation_type\",None)\n",
    "\n",
    "    if target_opertation_type == \"Linked Service Type\":\n",
    "        target_linked_service = metadata[\"target_table\"].get(\"target_linked_service\",\"\") \n",
    "        target_path = metadata[\"target_table\"].get(\"target_path\",\"\") \n",
    "        target_container = metadata[\"target_table\"].get(\"target_container\",\"\") \n",
    "        synapse_target_table_name = f\"{target_container}/{target_linked_service}/{target_path}/\"\n",
    "    else:\n",
    "        synapse_target_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, None, None)\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    for source_table in metadata[\"source_tables\"]:\n",
    "\n",
    "        source_opertation_type = source_table.get(\"opertation_type\",None)\n",
    "        source_opertation_type = 'lake database view' if not source_opertation_type else source_opertation_type\n",
    "        load_type = source_table.get(\"load_type\",source_opertation_type)\n",
    "    \n",
    "        linked_service_name = source_table.get(\"linked_service_name\",None)\n",
    "\n",
    "        #Get evnironment\n",
    "        env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "        #Get project name\n",
    "        project_name = metadata[\"project_name\"]\n",
    "        param = source_table.get(\"params\",None) \n",
    "\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        process_date_str = metadata.get(\"process_date\", current_date)\n",
    "        if process_date_str == 'None':\n",
    "            process_date = datetime.strptime(current_date, '%Y-%m-%d')\n",
    "        else:\n",
    "            try:\n",
    "                process_date = datetime.strptime(process_date_str, '%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    process_date = datetime.strptime(process_date_str, '%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    # Jako ostatniÄ… opcjÄ™, sprÃ³buj wyodrÄ™bniÄ‡ tylko datÄ™\n",
    "                    try:\n",
    "                        date_only = process_date_str.split()[0]\n",
    "                        process_date = datetime.strptime(date_only, '%Y-%m-%d')\n",
    "                    except ValueError:\n",
    "                        raise ValueError(f\"Not supported date format: {process_date_str}\")\n",
    "                        \n",
    "        ignore_env = None\n",
    "        ignore_project = None\n",
    "        if param:\n",
    "            ignore_env =  param.get(\"ignore_env\",False)\n",
    "            ignore_project =  param.get(\"ignore_project\",False) \n",
    "    \n",
    "        synapse_source_table_name = None\n",
    "        if source_opertation_type == \"Linked Service\":\n",
    "            linked_service_name = source_table.get(\"linked_service_name\",None)\n",
    "            container = source_table.get(\"container\",None)\n",
    "            path = source_table.get(\"path\",None)\n",
    "            synapse_source_table_name = f\"{container}/{linked_service_name}/{path}/\"\n",
    "        elif source_opertation_type == \"serverless jdbc ls\":\n",
    "            table_name = source_table.get(\"table\",None)\n",
    "            synapse_source_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, ignore_env, ignore_project)\n",
    "        elif source_opertation_type == \"serverless jdbc ls sqlauth\":\n",
    "            table_name = source_table.get(\"table\",None)\n",
    "            synapse_source_table_name = get_table_env_name(target_schema_name, target_table_name, env, project_name, ignore_env, ignore_project)\n",
    "        else:\n",
    "            source_schema_name = source_table.get(\"schema\",None)\n",
    "            source_table_name = source_table.get(\"table\",None)\n",
    "            synapse_source_table_name = get_table_env_name(source_schema_name, source_table_name, env, project_name, ignore_env, ignore_project)\n",
    "\n",
    "\n",
    "        if isinstance(processing_date_to_log, str):\n",
    "            try:\n",
    "                modify_datetime = datetime.strptime(processing_date_to_log, '%Y-%m-%d %H:%M:%S')\n",
    "            except ValueError:\n",
    "                try:          \n",
    "                    modify_datetime = datetime.strptime(processing_date_to_log, '%Y-%m-%d')\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"Not supported date format: {processing_date_to_log}\")\n",
    "        else:\n",
    "            modify_datetime = processing_date_to_log\n",
    "\n",
    "        row = Row(  \n",
    "                project_name = project_name, \n",
    "                load_type = load_type,\n",
    "                synapse_source_table_name = synapse_source_table_name,\n",
    "                synapse_target_table_name = synapse_target_table_name,\n",
    "                params = load_type,\n",
    "                modify_datetime = modify_datetime,\n",
    "                processing_date = process_date\n",
    "                )\n",
    "     \n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def repartition(finalTableName, sort_order = None, partitionBy = []):\n",
    "\n",
    "    \"\"\"\n",
    "    Repartitions a Delta table based on the optimal partition size.\n",
    "\n",
    "    Parameters:\n",
    "    finalTableName (str): The name of the table to repartition.\n",
    "    sort_order (str, optional): Column names to sort by during optimization. Defaults to None.\n",
    "    partitionBy (list, optional): List of column names to partition by. Defaults to an empty list.\n",
    "\n",
    "    Function Logic:\n",
    "    1. Calculate the optimal number of partitions based on the size of the table.\n",
    "    2. If the current number of partitions is outside the optimal range, repartition the table.\n",
    "    3. Create a temporary table with the new partitions.\n",
    "    4. Rename the original table to a backup name and the temporary table to the final table name.\n",
    "    5. Drop the backup table.\n",
    "    6. Perform VACUUM on the final table to clean up old files.\n",
    "    \"\"\"\n",
    "\n",
    "    finalTableName_tmp = finalTableName+\"_tmp\"\n",
    "    finalTableName_bcp = finalTableName+\"_bcp\"\n",
    "\n",
    "    details = spark.sql(f\"DESCRIBE DETAIL {finalTableName}\").collect()[0]\n",
    "    size_in_bytes = details['sizeInBytes']\n",
    "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "\n",
    "    optimal_partition_size_mb = 60 ## Optimal 128 but with 128 it could create 130 MB size files\n",
    "    num_partitions = math.ceil(size_in_mb / optimal_partition_size_mb)\n",
    "\n",
    "    print(f\"Size of table {finalTableName} is {size_in_mb} MB\")\n",
    "    print(f\"Number of partitions will be created: {num_partitions}\")\n",
    "\n",
    "    df = spark.table(finalTableName)\n",
    "    current_num_partitions = df.rdd.getNumPartitions()\n",
    "\n",
    "    lower_bound = num_partitions * 0.95\n",
    "    upper_bound = num_partitions * 1.05\n",
    "\n",
    "    print(f\"Current number of partitions: {current_num_partitions}\")\n",
    "    print(f\"Optimal number of partitions: {num_partitions}\")\n",
    "\n",
    "    if not (lower_bound <= current_num_partitions <= upper_bound):\n",
    "\n",
    "        try:\n",
    "            print(f\"Repartition table {finalTableName}\")\n",
    "            df = spark.table(finalTableName)\n",
    "            df.repartition(num_partitions).write.option(\"parquet.vorder.enabled \",\"true\").format(\"delta\").mode(\"overwrite\").partitionBy(*partitionBy).saveAsTable(finalTableName_tmp)\n",
    "            spark.sql(f\"ALTER TABLE {finalTableName} RENAME TO {finalTableName_bcp}\")\n",
    "            spark.sql(f\"ALTER TABLE {finalTableName_tmp} RENAME TO {finalTableName}\")\n",
    "            spark.sql(f\"DROP TABLE {finalTableName_bcp}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "        print(f\"VACUUM table {finalTableName}\")\n",
    "        spark.sql(f\"VACUUM {finalTableName}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"No need for table repartitioning: {finalTableName}\")\n",
    "\n",
    "\n",
    "    #if sort_order:\n",
    "    #    print(f\"OPTIMIZE table {finalTableName}\")\n",
    "    #    spark.sql(f\"OPTIMIZE {finalTableName} ZORDER BY ({sort_order}) VORDER\")\n",
    "\n",
    "#####################################################################11\n",
    "\n",
    "def log_to_datalineage(metadata_json, admin_schema_name, data_lineage_table_name):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Logs data lineage information into a specified Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    metadata_json (str): A JSON string containing metadata information. \n",
    "    admin_schema_name (str): The schema name for the administrative tables.\n",
    "    data_lineage_table_name (str): The name of the table where data lineage information will be logged.\n",
    "\n",
    "    Function Logic:\n",
    "    1. Parse the metadata JSON to extract necessary information.\n",
    "    2. Generate the fully qualified target table name based on the environment and project context.\n",
    "    3. Iterate over each source table in the metadata and determine the source table's fully qualified name based on the operation type.\n",
    "    4. Create a list of tuples containing the data lineage information.\n",
    "    5. Create a DataFrame from the list of tuples and write it to a temporary view.\n",
    "    6. Merge the new data lineage information into the specified Delta table with retry logic for handling concurrent write conflicts.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    max_retries = 3\n",
    "    #Read json metadata\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    data = []\n",
    "    from datetime import date\n",
    "    today = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    logger.info(f\"Log data lineage\")\n",
    "    print(f\"Log data lineage\")\n",
    "\n",
    "    notebook_name = mssparkutils.runtime.context['currentNotebookName']\n",
    "\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    env_for_table = \"prod\" if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "    \n",
    "\n",
    "    #Target table\n",
    "    target_schema_name = project_name+'_'+metadata[\"target_table\"].get(\"target_schema\",\"\") \n",
    "    target_table_name = metadata[\"target_table\"].get(\"target_table\",\"\")\n",
    "    target_opertation_type = metadata[\"target_table\"].get(\"opertation_type\",None)\n",
    "\n",
    "\n",
    "    if target_opertation_type == \"Linked Service Type\":\n",
    "        target_linked_service = metadata[\"target_table\"].get(\"target_linked_service\",\"None\") \n",
    "        target_path = metadata[\"target_table\"].get(\"target_path\",\"None\") \n",
    "        target_container = metadata[\"target_table\"].get(\"target_container\",\"None\") \n",
    "        finalTableName = f\"{target_linked_service}/{target_container}/{target_path}\"\n",
    "    else:\n",
    "        if env:\n",
    "            target_schema_name = target_schema_name +\"_\"+ env\n",
    "        finalTableName = f\"{target_schema_name}.{target_table_name}\"\n",
    "\n",
    "    if env:\n",
    "        admin_schema_name = admin_schema_name +\"_\"+ env\n",
    "    final_data_lineage_table_name = f\"{admin_schema_name}.{project_name}_{data_lineage_table_name}\"\n",
    "\n",
    "    for source_table in metadata['source_tables']:\n",
    "\n",
    "        schema = source_table.get(\"source\",None)\n",
    "        table = source_table.get(\"table\",None)\n",
    "        source_opertation_type = source_table.get(\"opertation_type\",\"lake database view\")\n",
    "        source_params = \"\"\n",
    "\n",
    "        if source_opertation_type == \"Linked Service\":\n",
    "            source_linked_service_name = source_table.get(\"linked_service_name\",\"None\")\n",
    "            source_container = source_table.get(\"container\",\"None\")\n",
    "            source_path = source_table.get(\"path\",\"None\")\n",
    "            finalSourceTableName = f\"{source_linked_service_name}/{source_container}/{source_path}\"\n",
    "\n",
    "        elif source_opertation_type == \"serverless jdbc ls\":\n",
    "            finalSourceTableName = source_table.get(\"table\",None)\n",
    "            source_linked_service_name = source_table.get(\"linked_service_name\",\"None\")\n",
    "\n",
    "        elif source_opertation_type == \"serverless jdbc ls sqlauth\":\n",
    "            finalSourceTableName = source_table.get(\"table\",None)\n",
    "            source_linked_service_name = source_table.get(\"linked_service_name\",\"None\")\n",
    "\n",
    "        elif source_opertation_type == \"lake database view\":\n",
    "            source_schema_name = project_name+'_'+source_table.get(\"schema\",None)\n",
    "            source_table_name = source_table.get(\"table\",None)\n",
    "            # Check if env is not None and ignore_env is False\n",
    "            ignore_env = source_table.get(\"params\", {}).get(\"ignore_env\", False)\n",
    "            if env and not ignore_env:\n",
    "                source_schema_name = source_schema_name +\"_\"+ env\n",
    "            finalSourceTableName = f\"{source_schema_name}.{source_table_name}\"\n",
    "\n",
    "        elif source_opertation_type == \"lake database view\":\n",
    "            source_schema_name = project_name+'_'+source_table.get(\"schema\",None)\n",
    "            source_table_name = source_table.get(\"table\",None)\n",
    "            # Check if env is not None and ignore_env is False\n",
    "            ignore_env = source_table.get(\"params\", {}).get(\"ignore_env\", False)\n",
    "            if env and not ignore_env:\n",
    "                source_schema_name = source_schema_name +\"_\"+ env\n",
    "            finalSourceTableName = f\"{source_schema_name}.{source_table_name}\"\n",
    "\n",
    "        elif source_opertation_type == \"delta sharing\":\n",
    "            source_table_name = source_table.get(\"table\",None) \n",
    "            source_schema_name = source_table.get(\"profile\",None)  \n",
    "            source_linked_service_name = source_table.get(\"key_vaule_ls\",None)\n",
    "            ignore_env = source_table.get(\"params\", {}).get(\"ignore_env\", False)\n",
    "            if env and not ignore_env:\n",
    "                source_schema_name = source_schema_name +\"_\"+ env\n",
    "            finalSourceTableName = f\"{source_schema_name}.{source_table_name}\"\n",
    "\n",
    "        elif source_opertation_type == \"delta sharing 1.2.0\":\n",
    "            source_table_name = source_table.get(\"table\",None) \n",
    "            source_schema_name = source_table.get(\"profile\",None)  \n",
    "            source_linked_service_name = source_table.get(\"key_vaule_ls\",None)\n",
    "            ignore_env = source_table.get(\"params\", {}).get(\"ignore_env\", False)\n",
    "            if env and not ignore_env:\n",
    "                source_schema_name = source_schema_name +\"_\"+ env\n",
    "            finalSourceTableName = f\"{source_schema_name}.{source_table_name}\"\n",
    "\n",
    "\n",
    "        data.append((notebook_name, \n",
    "                    env_for_table, \n",
    "                    finalTableName, \n",
    "                    json.dumps(metadata['target_table']), \n",
    "                    finalSourceTableName, \n",
    "                    json.dumps(source_table), \n",
    "                    today))\n",
    "\n",
    "        #Add additional row for \"serverless jdbc ls\"\n",
    "        if source_opertation_type == \"serverless jdbc ls\" or source_opertation_type == \"serverless jdbc ls sqlauth\":\n",
    "\n",
    "            data.append((notebook_name, \n",
    "                    env_for_table, \n",
    "                    finalSourceTableName, \n",
    "                    \"\",\n",
    "                    source_linked_service_name, \n",
    "                    source_params,\n",
    "                    today))\n",
    "\n",
    "    schema = StructType([   \n",
    "        StructField(\"notebook_name\", StringType(), False),\n",
    "        StructField(\"env\", StringType(), True),\n",
    "        StructField(\"target_table_name\", StringType(), False),\n",
    "        StructField(\"target_params\", StringType(), True),\n",
    "        StructField(\"source_table_name\", StringType(), False),\n",
    "        StructField(\"source_params\", StringType(), True),\n",
    "        StructField(\"_ModifiedDate\", StringType(), False)\n",
    "    ])    \n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df.createOrReplaceTempView(\"new_data_lineage_data\")\n",
    "\n",
    "    print(f\"Data lineage table: {final_data_lineage_table_name}\")\n",
    "    logger.info(f\"Data lineage table: {final_data_lineage_table_name}\")\n",
    "\n",
    "\n",
    "    #Write processing info to metadata column\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {final_data_lineage_table_name} AS target\n",
    "                USING new_data_lineage_data AS source\n",
    "                ON target.target_table_name = source.target_table_name\n",
    "                AND target.source_table_name = source.source_table_name\n",
    "                AND target.target_table_name = '{finalTableName}'\n",
    "                WHEN MATCHED THEN\n",
    "                UPDATE SET\n",
    "                    target.notebook_name = source.notebook_name,\n",
    "                    target.env = source.env,\n",
    "                    target.target_params = source.target_params,\n",
    "                    target.source_params = source.source_params,\n",
    "                    target._ModifiedDate = source._ModifiedDate\n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT (notebook_name, env, target_table_name, target_params, source_table_name, source_params, _ModifiedDate)\n",
    "                VALUES (source.notebook_name, source.env, source.target_table_name, source.target_params, source.source_table_name, source.source_params, source._ModifiedDate)\n",
    "            \"\"\")\n",
    "            break\n",
    "        except ConcurrentAppendException as e:\n",
    "            print(f\"Write conflict detected on attempt {attempt+1}. Error: {e}. Retrying...\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            logger.info(f\"An error occurred: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Custom rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_packrule_by_columns(static_rules_temp_view_name, packrule_FUNCTION, packrule_CRITERIA, KEY_COLUMNS_LIST: list, VALUE_COLUMN_LIST: list, FN_COLUMN_LIST: list, OVERRIDE_COLUMN):\n",
    "   \n",
    "    \"\"\"\n",
    "    Generates a SQL case statement based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "    packrule_function (str): The function to apply.\n",
    "    packrule_criteria (str): Criteria for filtering.\n",
    "    key_columns (list): Key columns for filtering.\n",
    "    value_columns (list): Corresponding value columns.\n",
    "    fn_columns (list): Function columns.\n",
    "    override_column (str): Column name for override values.\n",
    "\n",
    "    Returns:\n",
    "    str: A SQL case statement.\n",
    "    \"\"\"\n",
    "\n",
    "    # Declare tables to read base on ENV \n",
    "    # VRP_RAW_NAME, VRP_PROCESS_NAME, VRP_NORMALIZED_NAME, VRP_OUTPUT_NAME\n",
    "\n",
    "\n",
    "    #KEY_COLUMNS_LIST - list of columns used to filter data\n",
    "    #VALUE_COLUMN_LIST - list of columns that will be equal to KEY columns\n",
    "    #OVERRIDE_COLUMN - column with override value\n",
    "\n",
    "    #allowed columns\n",
    "    allowed_KEY_COLUMNS_list    = ['KEY1','KEY2','KEY3','KEY4','KEY5','KEY6']\n",
    "    allowed_VALUE_COLUMN_list   = ['VALUE1','VALUE2','VALUE3','VALUE4','VALUE5','VALUE6']\n",
    "    allowed_FN_COLUMN_list      = ['FN1','FN2','FN3','FN4','FN5','FN6']\n",
    "    allowed_RESULT_COLUMN_list  = ['RESULT1','RESULT2','RESULT3','RESULT4','RESULT5','RESULT6']\n",
    "    #Map RESULT to RESULT TYPE column\n",
    "    column_mapping_RESULT1_TYPE = {\n",
    "        \"RESULT1\": \"RESULT1_TYPE\",\n",
    "        \"RESULT2\": \"RESULT2_TYPE\",\n",
    "        \"RESULT3\": \"RESULT3_TYPE\",\n",
    "        \"RESULT4\": \"RESULT4_TYPE\",\n",
    "        \"RESULT5\": \"RESULT5_TYPE\",\n",
    "        \"RESULT6\": \"RESULT6_TYPE\"\n",
    "    }\n",
    "\n",
    "    #Test parameters\n",
    "    if not isinstance(KEY_COLUMNS_LIST, list):\n",
    "        raise TypeError\n",
    "    if not isinstance(VALUE_COLUMN_LIST, list):\n",
    "        raise TypeError\n",
    "    if not isinstance(FN_COLUMN_LIST, list):\n",
    "        raise TypeError\n",
    "\n",
    "    if not len(KEY_COLUMNS_LIST) == len(VALUE_COLUMN_LIST):\n",
    "        raise Exception(\"Number of KEY columns does not equal to number of VALUE columns\")\n",
    "\n",
    "    if not all(item in allowed_KEY_COLUMNS_list for item in KEY_COLUMNS_LIST):\n",
    "        raise Exception(\"Not allowed KEY column name. Allowed: \", ', '.join(allowed_KEY_COLUMNS_list))\n",
    "\n",
    "    if not all(item in allowed_VALUE_COLUMN_list for item in VALUE_COLUMN_LIST):\n",
    "        raise Exception(\"Not allowed VALUE column name. Allowed: \", ', '.join(allowed_VALUE_COLUMN_list))    \n",
    "    \n",
    "    if not all(item in allowed_FN_COLUMN_list for item in FN_COLUMN_LIST):\n",
    "        raise Exception(\"Not allowed FN column name. Allowed: \", ', '.join(allowed_FN_COLUMN_list))    \n",
    "\n",
    "    if not OVERRIDE_COLUMN in allowed_RESULT_COLUMN_list:\n",
    "        raise Exception(\"Not allowed RESULT column name. Allowed: \", ', '.join(allowed_RESULT_COLUMN_list))    \n",
    "\n",
    "    RESULT_TYPE = column_mapping_RESULT1_TYPE.get(OVERRIDE_COLUMN, None)\n",
    "\n",
    "\n",
    "    columns_to_select = ', '.join(KEY_COLUMNS_LIST)+', '+', '.join(VALUE_COLUMN_LIST)+', '+', '.join(FN_COLUMN_LIST)+', '+OVERRIDE_COLUMN+', '+RESULT_TYPE\n",
    "\n",
    "    #If criteria is provided then filder packrule table based on CRITERIA value\n",
    "    if packrule_CRITERIA is None: \n",
    "        filter_CRITERIA_query = \"\"\n",
    "    else:\n",
    "        filter_CRITERIA_query = f\"\"\"AND CRITERIA = \"{packrule_CRITERIA}\" \"\"\"\n",
    "\n",
    "    packrules_cnt = spark.sql(f\"\"\"SELECT SEQUENCE, {columns_to_select} FROM {static_rules_temp_view_name} WHERE FUNCTION = \"{packrule_FUNCTION}\" {filter_CRITERIA_query} ORDER BY SEQUENCE ASC \"\"\")\n",
    "\n",
    "    pd_packrules_df = packrules_cnt.toPandas()\n",
    "\n",
    "    #Initate case statement\n",
    "    case_query = \"CASE \"\n",
    "\n",
    "    #Default end of case statement\n",
    "    case_query_end = \"ELSE NULL END \"\n",
    "\n",
    "    #create case statement\n",
    "    for ind, r in pd_packrules_df.iterrows():\n",
    "\n",
    "        case = \"\"\n",
    "        if_else_condition = False\n",
    "\n",
    "        item_no = 0\n",
    "        \n",
    "        for key in KEY_COLUMNS_LIST:\n",
    "            if r[1+item_no] is not None:\n",
    "                if r[1+item_no+len(KEY_COLUMNS_LIST)] == \"[Any/Other]\":\n",
    "                    case = case\n",
    "                elif r[1+item_no] == \"[Else]\":\n",
    "                    ##if delcared [Else] condition\n",
    "                    if r[RESULT_TYPE]==\"[column]\":\n",
    "                        case_query_end = f\"\"\"ELSE `{r[OVERRIDE_COLUMN]}`  END \"\"\"\n",
    "                    elif r[RESULT_TYPE]==\"[value]\":\n",
    "                        case_query_end = f\"\"\"ELSE \"{r[OVERRIDE_COLUMN]}\"  END \"\"\"\n",
    "                    else:\n",
    "                        case_query_end = f\"\"\"ELSE \"{r[OVERRIDE_COLUMN]}\"  END \"\"\"\n",
    "                    if_else_condition = True\n",
    "                else:\n",
    "                    if r[RESULT_TYPE]==\"[column]\":\n",
    "                        ##if result type is columns then use ``\n",
    "                        case += f\"\"\" AND `{r[1+item_no]}` {r[1+item_no+(2*len(KEY_COLUMNS_LIST))]} `{r[1+item_no+len(KEY_COLUMNS_LIST)]}` \"\"\" \n",
    "                    elif r[RESULT_TYPE]==\"[value]\":\n",
    "                        ##if result type is columns then use \"\"\n",
    "                        case += f\"\"\" AND `{r[1+item_no]}` {r[1+item_no+(2*len(KEY_COLUMNS_LIST))]} \"{r[1+item_no+len(KEY_COLUMNS_LIST)]}\" \"\"\" \n",
    "                    else:\n",
    "                        ##else use default if not declared \"\"\n",
    "                        case += f\"\"\" AND `{r[1+item_no]}` {r[1+item_no+(2*len(KEY_COLUMNS_LIST))]} \"{r[1+item_no+len(KEY_COLUMNS_LIST)]}\" \"\"\" \n",
    "                item_no += 1\n",
    "\n",
    "        \n",
    "        #Creation of case query\n",
    "        if not if_else_condition:\n",
    "            case_query += f\"\"\"WHEN {case[5:-1]} THEN \"{r[OVERRIDE_COLUMN]}\" \n",
    "            \"\"\"\n",
    "\n",
    "        \n",
    "    case_query += case_query_end\n",
    "    return case_query\n",
    "\n",
    "\n",
    "#Example\n",
    "#get_packrule_by_columns(\"static_ods_rules\",\"MDG_MARS_SEGMENT_AND_DIVISION\", \"MARS_SEGMENT_DIVISION\", ['KEY1','KEY2'], ['VALUE1','VALUE2'], ['FN1', 'FN2'], 'RESULT1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ods_rule(rules_temp_view_name, packrule_FUNCTION, packrule_CRITERIA, KEY_COLUMNS_LIST: list, VALUE_COLUMN_LIST: list, FN_COLUMN_LIST: list, OVERRIDE_COLUMN):\n",
    "    \"\"\"\n",
    "    Generates a SQL CASE statement for the new_country column based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        rules_temp_view_name (str): The name of the table or view containing the rules.\n",
    "        packrule_FUNCTION (str): The function to filter the rules by (e.g., \"COUNTRY_OF_USE_OVERRIDE\").\n",
    "        packrule_CRITERIA (str): The criteria to filter the rules by (e.g., \"BY_DRIVER\").\n",
    "        KEY_COLUMNS_LIST (list): List of key columns used to filter data.\n",
    "        VALUE_COLUMN_LIST (list): List of value columns that correspond to the key columns.\n",
    "        FN_COLUMN_LIST (list): List of function columns that define the operation (e.g., \"=\").\n",
    "        OVERRIDE_COLUMN (str): The column that contains the result value / overriden column.\n",
    "\n",
    "    Returns:\n",
    "        str: A SQL CASE statement.\n",
    "    \"\"\"\n",
    "\n",
    "    #allowed columns\n",
    "    allowed_KEY_COLUMNS_list    = ['KEY1','KEY2','KEY3','KEY4','KEY5','KEY6']\n",
    "    allowed_VALUE_COLUMN_list   = ['VALUE1','VALUE2','VALUE3','VALUE4','VALUE5','VALUE6']\n",
    "    allowed_FN_COLUMN_list      = ['FN1','FN2','FN3','FN4','FN5','FN6']\n",
    "    allowed_RESULT_COLUMN_list  = ['RESULT1','RESULT2','RESULT3','RESULT4','RESULT5','RESULT6']\n",
    "    #Map RESULT to RESULT TYPE column\n",
    "    column_mapping_RESULT_TYPE1 = {\n",
    "        \"RESULT1\": \"RESULTTYPE1\",\n",
    "        \"RESULT2\": \"RESULTTYPE2\",\n",
    "        \"RESULT3\": \"RESULTTYPE3\",\n",
    "        \"RESULT4\": \"RESULTTYPE4\",\n",
    "        \"RESULT5\": \"RESULTTYPE5\",\n",
    "        \"RESULT6\": \"RESULTTYPE6\"\n",
    "    }\n",
    "\n",
    "    # Validate input parameters\n",
    "    if not isinstance(KEY_COLUMNS_LIST, list):\n",
    "        raise TypeError(\"KEY_COLUMNS_LIST must be a list\")\n",
    "    if not isinstance(VALUE_COLUMN_LIST, list):\n",
    "        raise TypeError(\"VALUE_COLUMN_LIST must be a list\")\n",
    "    if not isinstance(FN_COLUMN_LIST, list):\n",
    "        raise TypeError(\"FN_COLUMN_LIST must be a list\")\n",
    "\n",
    "    if not len(KEY_COLUMNS_LIST) == len(VALUE_COLUMN_LIST):\n",
    "        raise Exception(\"Number of KEY columns does not equal to number of VALUE columns\")\n",
    "\n",
    "    if not all(item in allowed_KEY_COLUMNS_list for item in KEY_COLUMNS_LIST):\n",
    "        raise Exception(\"Not allowed KEY column name. Allowed: \", ', '.join(allowed_KEY_COLUMNS_list))\n",
    "\n",
    "    if not all(item in allowed_VALUE_COLUMN_list for item in VALUE_COLUMN_LIST):\n",
    "        raise Exception(\"Not allowed VALUE column name. Allowed: \", ', '.join(allowed_VALUE_COLUMN_list))    \n",
    "    \n",
    "    if not all(item in allowed_FN_COLUMN_list for item in FN_COLUMN_LIST):\n",
    "        raise Exception(\"Not allowed FN column name. Allowed: \", ', '.join(allowed_FN_COLUMN_list))    \n",
    "\n",
    "    if not OVERRIDE_COLUMN in allowed_RESULT_COLUMN_list:\n",
    "        raise Exception(\"Not allowed RESULT column name. Allowed: \", ', '.join(allowed_RESULT_COLUMN_list))    \n",
    "    \n",
    "    RESULT_TYPE = column_mapping_RESULT_TYPE1.get(OVERRIDE_COLUMN, None)\n",
    "\n",
    "\n",
    "    # Construct the SQL query to fetch the rules\n",
    "    columns_to_select = ', '.join(KEY_COLUMNS_LIST) + ', ' + ', '.join(VALUE_COLUMN_LIST) + ', ' + ', '.join(FN_COLUMN_LIST) + ', ' + OVERRIDE_COLUMN + ', ' + RESULT_TYPE\n",
    "\n",
    "\n",
    "    #If criteria is provided then filter packrule table based on CRITERIA value\n",
    "    filter_CRITERIA_query = f\"\"\"AND CRITERIA = \"{packrule_CRITERIA}\" \"\"\" if packrule_CRITERIA else \"\"\n",
    "\n",
    "    # Fetch the rules\n",
    "    rules_df = spark.sql(f\"\"\"\n",
    "        SELECT {columns_to_select}\n",
    "        FROM {rules_temp_view_name}\n",
    "        WHERE FUNCTION = \"{packrule_FUNCTION}\" {filter_CRITERIA_query}\n",
    "        -- ORDER BY SEQUENCE ASC\n",
    "    \"\"\")\n",
    "\n",
    "    pd_rules_df = rules_df.toPandas()\n",
    "\n",
    "    # Initialize the CASE statement\n",
    "    case_query = \"\\nCASE\"\n",
    "\n",
    "    #End of case statement\n",
    "    case_query_end = \"\\n    ELSE NULL\\nEND\\n\"\n",
    "\n",
    "    # Iterate over the rules and construct the CASE statement\n",
    "    for _, row in pd_rules_df.iterrows():\n",
    "        conditions = []\n",
    "        for i, key in enumerate(KEY_COLUMNS_LIST):\n",
    "            value = row[VALUE_COLUMN_LIST[i]]\n",
    "            fn = row[FN_COLUMN_LIST[i]]\n",
    "            if value is not None:\n",
    "                if fn.strip().upper() == \"IN\":\n",
    "                    # For IN operator, don't put quotes around the value\n",
    "                    conditions.append(f\"\"\"`{row[key]}` {fn} {value} \"\"\")\n",
    "                else:\n",
    "                    conditions.append(f\"\"\"`{row[key]}` {fn} \"{value}\" \"\"\")\n",
    "            else:\n",
    "                conditions.append(f\"\"\"`{row[key]}` IS NULL \"\"\")\n",
    "\n",
    "        if conditions:\n",
    "            if row[RESULT_TYPE] == \"[column]\":\n",
    "                case_query += f\"\"\"\\n    WHEN {' AND '.join(conditions)} THEN `{row[OVERRIDE_COLUMN]}` \"\"\"\n",
    "            elif row[RESULT_TYPE] == \"[value]\":\n",
    "                case_query += f\"\"\"\\n    WHEN {' AND '.join(conditions)} THEN \"{row[OVERRIDE_COLUMN]}\" \"\"\"\n",
    "            else:\n",
    "                case_query += f\"\"\"\\n    WHEN {' AND '.join(conditions)} THEN {row[OVERRIDE_COLUMN]} \"\"\"\n",
    "\n",
    "    # Add the ELSE clause\n",
    "    case_query += case_query_end\n",
    "\n",
    "    return case_query\n",
    "\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# case_statement_segment_override = get_ods_rule(\n",
    "#                                                         rules_temp_view_name=\"ods_rules\",\n",
    "#                                                         packrule_FUNCTION=\"MGIS_SEGMENT_OVERRIDE\",\n",
    "#                                                         packrule_CRITERIA=\"OVERRIDE\",\n",
    "#                                                         KEY_COLUMNS_LIST=[\"KEY1\", \"KEY2\"],\n",
    "#                                                         VALUE_COLUMN_LIST=[\"VALUE1\", \"VALUE2\"],\n",
    "#                                                         FN_COLUMN_LIST=[\"FN1\", \"FN2\"],\n",
    "#                                                         OVERRIDE_COLUMN=\"RESULT1\"\n",
    "#                                                 )\n",
    "\n",
    "# case_statement_division_override = get_ods_rule(\n",
    "#                                                         rules_temp_view_name=\"ods_rules\",\n",
    "#                                                         packrule_FUNCTION=\"RUSSIA_DIVISION_OVERRIDE\",\n",
    "#                                                         packrule_CRITERIA=\"BY_DIVISION\",\n",
    "#                                                         KEY_COLUMNS_LIST=[\"KEY1\", \"KEY2\"],\n",
    "#                                                         VALUE_COLUMN_LIST=[\"VALUE1\", \"VALUE2\"],\n",
    "#                                                         FN_COLUMN_LIST=[\"FN1\", \"FN2\"],\n",
    "#                                                         OVERRIDE_COLUMN=\"RESULT1\"\n",
    "#                                                 )\n",
    "\n",
    "# case_statement_new_country_override = get_ods_rule(\n",
    "#                                                         rules_temp_view_name=\"ods_rules\",\n",
    "#                                                         packrule_FUNCTION=\"COUNTRY_OF_USE_OVERRIDE\",\n",
    "#                                                         packrule_CRITERIA=\"BY_DRIVER\",\n",
    "#                                                         KEY_COLUMNS_LIST=[\"KEY1\", \"KEY2\"],\n",
    "#                                                         VALUE_COLUMN_LIST=[\"VALUE1\", \"VALUE2\"],\n",
    "#                                                         FN_COLUMN_LIST=[\"FN1\", \"FN2\"],\n",
    "#                                                         OVERRIDE_COLUMN=\"RESULT1\"\n",
    "#                                                     )\n",
    "\n",
    "# case_statement_new_country_override_1 = get_ods_rule(\n",
    "#                                                         rules_temp_view_name=\"ods_rules\",\n",
    "#                                                         packrule_FUNCTION=\"COUNTRIES_TO_SUBSEGMENTS_MGIS_ADJ\",\n",
    "#                                                         packrule_CRITERIA=\"BY_COUNTRY_OF_USE\",\n",
    "#                                                         KEY_COLUMNS_LIST=[\"KEY1\"],\n",
    "#                                                         VALUE_COLUMN_LIST=[\"VALUE1\"],\n",
    "#                                                         FN_COLUMN_LIST=[\"FN1\"],\n",
    "#                                                         OVERRIDE_COLUMN=\"RESULT1\"\n",
    "#                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## send_notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def send_notification(message, subject, header, status, color, recipients, env, \n",
    "                     kv_secret_name_with_url=\"ODS-notification-url\", \n",
    "                     lv_linked_service=\"SOLUTION_KEY_VAULT_LS\"):\n",
    "    \"\"\"\n",
    "    Sends a notification using a REST API endpoint with credentials stored in Azure Key Vault.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    message : str\n",
    "        The main body of the notification message\n",
    "    subject : str\n",
    "        The subject line of the notification\n",
    "    header : str\n",
    "        The header text for the notification\n",
    "    status : str\n",
    "        The status indicator for the notification (e.g., 'success', 'error', 'warning', 'info')\n",
    "    color : str\n",
    "        The color theme for the notification (e.g., 'blue', 'red', 'green', 'yellow')\n",
    "    recipients : str\n",
    "        Email address(es) of the notification recipient(s)\n",
    "    env : str\n",
    "        Environment identifier (e.g., 'dev', 'test', 'prod')\n",
    "    kv_secret_name_with_url : str, optional\n",
    "        The name of the secret in Key Vault containing the notification service URL\n",
    "        (default: \"ODS-notification-url\")\n",
    "    lv_linked_service : str, optional\n",
    "        The name of the linked service for Key Vault access\n",
    "        (default: \"SOLUTION_KEY_VAULT_LS\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if the notification was sent successfully (status code 200 or 202)\n",
    "        False if the request failed\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    Exception\n",
    "        If there's an error accessing Key Vault or sending the notification\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> result = send_notification(\n",
    "    ...     message=\"Pipeline completed successfully\",\n",
    "    ...     subject=\"Pipeline Status Update\",\n",
    "    ...     header=\"Pipeline Notification\",\n",
    "    ...     status=\"success\",\n",
    "    ...     color=\"green\",\n",
    "    ...     recipients=\"team@company.com\",\n",
    "    ...     env=\"prod\"\n",
    "    ... )\n",
    "    >>> print(\"Notification sent:\", result)\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - The function uses mssparkutils.credentials to access Key Vault\n",
    "    - Success is indicated by HTTP status codes 200 or 202\n",
    "    - Full response details are logged for debugging purposes\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get URL from Key Vault\n",
    "        url = mssparkutils.credentials.getSecretWithLS(lv_linked_service, kv_secret_name_with_url)\n",
    "        \n",
    "        # Prepare data\n",
    "        data = {\n",
    "            \"message\": message,\n",
    "            \"subject\": subject,\n",
    "            \"header\": header,\n",
    "            \"status\": status,\n",
    "            \"color\": color,\n",
    "            \"recipients\": recipients,\n",
    "            \"env\": env\n",
    "        }\n",
    "        \n",
    "        # Send request\n",
    "        response = requests.post(url, json=data)\n",
    "        \n",
    "        # Print response details for debugging\n",
    "        print(\"Status Code:\", response.status_code)\n",
    "        print(\"Response Content:\", response.text)\n",
    "        print(\"Response Headers:\", dict(response.headers))\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Notification sent successfully\")\n",
    "            return True\n",
    "                \n",
    "        elif response.status_code == 202:\n",
    "            print(\"Request accepted\")\n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## repartition_with_salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def repartition_with_salt(df: DataFrame, target_rows_per_partition: int = 100000) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Rebalances a DataFrame by adding a salt column and repartitioning the data.\n",
    "    \n",
    "    :param df: Input DataFrame\n",
    "    :param target_rows_per_partition: Approximate target number of rows per partition (default: 100,000)\n",
    "    :return: Repartitioned DataFrame with balanced partitions\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "\n",
    "    num_partitions = max(1, total_rows // target_rows_per_partition)\n",
    "\n",
    "    print(f\"Repartition: {num_partitions}\")\n",
    "\n",
    "    df = df.withColumn(\"salt\", (F.rand() * num_partitions).cast(\"int\"))\n",
    "\n",
    "    df = df.repartition(num_partitions, \"salt\")\n",
    "\n",
    "    df = df.drop(\"salt\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## create_table_from_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_table_from_dataframe(env, schema_name, table_name, dataframe, type = 'parquet', partitionBy = []):\n",
    "    ## Formulating the target table name\n",
    "    if env:\n",
    "        schema_name = schema_name +\"_\"+ env\n",
    "    finalTableName = f\"{schema_name}.{table_name}\"\n",
    "\n",
    "    ## Dropping the table\n",
    "    spark.sql(\n",
    "        f\"DROP TABLE IF EXISTS {finalTableName}\"\n",
    "    )  # drop every time for now until everything will be checked (to avoid mergeschema)\n",
    "\n",
    "    print(f\"Create {type} table: {finalTableName}\")\n",
    "    if type == 'parquet':\n",
    "        # Save as parquet\n",
    "        dataframe.write.mode(\"overwrite\").partitionBy(*partitionBy).saveAsTable(finalTableName)\n",
    "    elif type == 'delta':\n",
    "        # Save as delta\n",
    "        dataframe.write.format(\"delta\").mode(\"overwrite\").partitionBy(*partitionBy).saveAsTable(finalTableName)\n",
    "    ## Validating if table is populated\n",
    "    ## This would result to an error; halting the cell\n",
    "    assert spark.sql(f\"SELECT COUNT(*) as total_rows from {finalTableName}\").collect()[0]['total_rows'] >= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## create_table_from_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_table_from_schema(env, schema_name, table_name, schema: StructType(), partitionBy):\n",
    "\n",
    "    if env:\n",
    "        _schema_name = schema_name +\"_\"+ env\n",
    "        table_full_name = f\"{_schema_name}.{table_name}\"\n",
    "    else:\n",
    "        _schema_name = schema_name\n",
    "        table_full_name = f\"{_schema_name}.{table_name}\"\n",
    "        \n",
    "    # Check if result table exists\n",
    "    if not delta_table_exists(table_full_name):\n",
    "        emp_RDD = spark.sparkContext.emptyRDD()\n",
    "        empty_agg_df = spark.createDataFrame(emp_RDD, schema=schema)\n",
    "\n",
    "        # Create the table with the defined schema if it does not exist using an empty RDD\n",
    "        create_table_from_dataframe(env, schema_name, table_name, empty_agg_df, \"delta\", partitionBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## list_files_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of all files in the data folder and its subfolders, including additional file information\n",
    "# Now supports filtering by file extension\n",
    "def list_files_recursive(folder_path, extension='parquet', file_name_like = None):\n",
    "    all_files = []\n",
    "    try:\n",
    "        for file_info in mssparkutils.fs.ls(folder_path):\n",
    "            if file_info.isDir:\n",
    "                # Recursively list files in subfolder, including additional file information\n",
    "                all_files.extend(list_files_recursive(file_info.path, extension, file_name_like))\n",
    "            else:\n",
    "                # Check if file extension matches the specified extension, if any\n",
    "                if (extension is None or file_info.path.endswith(f\".{extension}\")) and (file_name_like is None or file_name_like in file_info.path):\n",
    "                    # Create a dictionary with additional file information\n",
    "                    file_details = {\n",
    "                        'path': file_info.path,\n",
    "                        'name': file_info.name,\n",
    "                        'modifyTime': time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(file_info.modifyTime / 1000))\n",
    "                    }\n",
    "                    all_files.append(file_details)\n",
    "    except:\n",
    "        print(f\" Error at {folder_path}\")\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## export_to_synfs_ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def export_to_synfs_ADLS(metadata_json, current_timestamp, current_yyyymmdd, delete_export_files = False):\n",
    "\n",
    "    # Export data from one ADLS location defined by Linked Service to another location defined by Linked Service and rename files\n",
    "    #Read json metadata\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    #Get evnironment\n",
    "    env = None if metadata[\"env\"]==\"None\" else metadata[\"env\"]\n",
    "    project_name = metadata[\"project_name\"]\n",
    "\n",
    "\n",
    "    logger.info(f\"ENV: {env}\")\n",
    "    print(f\"ENV: {env}\")\n",
    "    logger.info(f\"PROJECT_NAME: {project_name}\")\n",
    "    print(f\"PROJECT_NAME: {project_name}\")\n",
    "\n",
    "    mount_scope = \"workspace\"\n",
    "\n",
    "    export_to_linked_service = metadata[\"export_to\"].get(\"linked_service\",None) \n",
    "    export_to_container = metadata[\"export_to\"].get(\"container\",None)\n",
    "    export_to_path = metadata[\"export_to\"].get(\"path\",None)\n",
    "    export_to_extension = metadata[\"export_to\"].get(\"extension\",\"csv\")\n",
    "    rename_file_as = metadata[\"export_to\"].get(\"rename_file_as\",\"\")\n",
    "    \n",
    "    if not export_to_linked_service or not export_to_container or not export_to_path:\n",
    "        raise ValueError(\"export to: Required parameters path, container, linked_service not declared\")\n",
    "\n",
    "    export_from_linked_service = metadata[\"export_from\"].get(\"linked_service\",None) \n",
    "    export_from_container = metadata[\"export_from\"].get(\"container\",None)\n",
    "    export_from_path = metadata[\"export_from\"].get(\"path\",None)\n",
    "    export_from_extension = metadata[\"export_from\"].get(\"extension\",\"csv\")\n",
    "\n",
    "    backup_export_enabled =  metadata[\"backup_export_to\"].get(\"enabled\",\"False\")\n",
    "    backup_export_path =  metadata[\"backup_export_to\"].get(\"path\",None)\n",
    "    backup_export_container =  metadata[\"backup_export_to\"].get(\"container\",None)\n",
    "    backup_export_linked_service =  metadata[\"backup_export_to\"].get(\"linked_service\",None)\n",
    "    backup_export_folder_timestamp = metadata[\"backup_export_to\"].get(\"folder_timestamp\",\"00000000_000000\")\n",
    "    backup_export_folder = metadata[\"backup_export_to\"].get(\"folder\",\"None\")\n",
    "    \n",
    "    print(f\"\\n    Backup is:  {backup_export_enabled}\")\n",
    "    \n",
    "    if not export_from_linked_service or not export_from_container or not export_from_path:\n",
    "        raise ValueError(\"export from: Required parameters path, container, linked_service not declared\")\n",
    "\n",
    "\n",
    "    #mount_point declared in \"Utils/data_processing_utilis_v2\"\n",
    "    export_to_mount_point_name = get_mount_name(export_to_linked_service, export_to_container, mount_scope, env)\n",
    "\n",
    "    mount_point[export_to_mount_point_name], mount_synfs_path[export_to_mount_point_name] = mount_from_linkedservice(export_to_linked_service, export_to_container, mount_scope, env)\n",
    "\n",
    "    export_from_mount_point_name = get_mount_name(export_from_linked_service, export_from_container, mount_scope, env)\n",
    "\n",
    "    mount_point[export_from_mount_point_name], mount_synfs_path[export_from_mount_point_name] = mount_from_linkedservice(export_from_linked_service, export_from_container, mount_scope, env)\n",
    "\n",
    "    export_to_synfs_path = f\"{mount_synfs_path[export_to_mount_point_name]}{export_to_path}/\"\n",
    "    export_from_synfs_path = f\"{mount_synfs_path[export_from_mount_point_name]}{export_from_path}/\"\n",
    "\n",
    "    print(f\"Export to: {export_to_synfs_path}\")\n",
    "    print(f\"Export from: {export_from_synfs_path}\")\n",
    "    \n",
    "    file_list_to_export = list_files_recursive(export_from_synfs_path, export_from_extension)\n",
    "\n",
    "\n",
    "    if backup_export_enabled == \"True\":\n",
    "        print(f\"\\n    Backup files:  {backup_export_enabled}\")\n",
    "        backup_export_mount_point_name = get_mount_name(backup_export_linked_service, backup_export_container, mount_scope, env)\n",
    "        mount_point[backup_export_mount_point_name], mount_synfs_path[backup_export_mount_point_name] = mount_from_linkedservice(backup_export_linked_service, backup_export_container, mount_scope, env)\n",
    "        backup_export_synfs_path = f\"{mount_synfs_path[backup_export_mount_point_name]}{backup_export_path}\"\n",
    "\n",
    "        print(f\"backup_export_synfs_path: {backup_export_synfs_path}\")\n",
    "\n",
    "        backup_export_synfs_file_path = backup_export_synfs_path+backup_export_folder_timestamp+\"/\"+backup_export_folder+\"/\"\n",
    "        \n",
    "        for file in file_list_to_export:\n",
    "            \n",
    "            match = re.search(r\"=([^/]+)/\", file['path'])\n",
    "\n",
    "            if match:\n",
    "                value = match.group(1)\n",
    "                final_target_path = f\"{backup_export_synfs_file_path}{rename_file_as}_{value}_{current_timestamp}.{export_to_extension}\"\n",
    "            else:\n",
    "                final_target_path = f\"{backup_export_synfs_file_path}{rename_file_as}_{current_timestamp}.{export_to_extension}\"\n",
    "\n",
    "            print(f\"Export: {file['path']} to {final_target_path}\")\n",
    "            \n",
    "            mssparkutils.fs.cp(file['path'], final_target_path, True)\n",
    "\n",
    "\n",
    "    if delete_export_files:\n",
    "        try:\n",
    "            delete_files_in_folder_with_linked_service(export_to_linked_service, export_to_container, export_target_path_lorax, ENV, False)\n",
    "        except:\n",
    "            print(\"\\n    Folder does not exists\")\n",
    "\n",
    "    for file in file_list_to_export:\n",
    "        \n",
    "        match = re.search(r\"=([^/]+)/\", file['path'])\n",
    "\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            final_target_path = f\"{export_to_synfs_path}{rename_file_as}_{value}_{current_timestamp}.{export_to_extension}\"\n",
    "        else:\n",
    "            final_target_path = f\"{export_to_synfs_path}{rename_file_as}_{current_timestamp}.{export_to_extension}\"\n",
    "\n",
    "        print(f\"Export: {file['path']} to {final_target_path}\")\n",
    "        \n",
    "        mssparkutils.fs.cp(file['path'], final_target_path, True)\n",
    "\n",
    "\n",
    "########## USAGE ###########\n",
    "\n",
    "### ------------------------------------------------------------------------------------\n",
    "#export_metadata_json = '''\n",
    "#{\n",
    "#    \"export_to\":   {\"path\" :\"GLOBAL_XSEG_CORPORATE_ODS/EXPORTS/LORAX/LORAX_VENDORS\",  \"rename_file_as\":\"lorax_vendors\",  \"container\": \"output\" , \"linked_service\" : \"MARS_ANALYTICS_EXPORT_ADLS_LS\"  , \"file_extension\": \"csv\"},\n",
    "#    \"backup_export_to\":   { \"enabled\":\"'''+str(publish_success_file)+'''\",  \"path\" :\"'''+str(backup_export_path)+'''\", \"folder\":\"'''+str(output_table_name)+'''\", \"folder_timestamp\": \"'''+str(folder_timestamp)+'''\", \"container\": \"output\" , \"linked_service\" : \"MARS_ANALYTICS_EXPORT_ADLS_LS\"},\n",
    "#    \"export_from\": {\"path\":\"LORAX/DATA_TO_EXPORT/LORAX_VENDORS\",                        \"container\": \"files\" ,  \"linked_service\" : \"SOLUTION_ADLS_LS\"               , \"file_extension\": \"csv\"},\n",
    "#    \"env\": \"'''+str(ENV)+'''\",\n",
    "#    \"project_name\": \"'''+str(PROJECT_NAME)+'''\"\n",
    "#}\n",
    "#'''\n",
    "#\n",
    "#export_to_synfs_ADLS(export_metadata_json, current_timestamp, current_yyyymmdd, True)\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## create_incremental_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    START DELTA LOAD                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  SCHEMA EVOLUTION CHECK                                          â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\n",
    "â”‚  source_columns = get_table_columns(source)     â†’ [A, B, C, NEW] â”‚\n",
    "â”‚  target_columns = get_table_columns(target)     â†’ [A, B, C]      â”‚\n",
    "â”‚  new_columns = source - target                  â†’ [NEW]          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                               â”‚\n",
    "              â–¼                               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ignore=TRUE             â”‚     â”‚ ignore=FALSE                    â”‚\n",
    "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\n",
    "â”‚ 1. ALTER TABLE ADD NEW  â”‚     â”‚ 1. ALTER TABLE ADD NEW          â”‚\n",
    "â”‚ 2. MERGE UPDATE NEW     â”‚     â”‚ 2. (no update - stays NULL)     â”‚\n",
    "â”‚    from source          â”‚     â”‚                                 â”‚\n",
    "â”‚                         â”‚     â”‚                                 â”‚\n",
    "â”‚ columns_for_comparison  â”‚     â”‚ columns_for_comparison          â”‚\n",
    "â”‚   = [A, B, C] (OLD)     â”‚     â”‚   = [A, B, C, NEW] (ALL)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚                               â”‚\n",
    "              â–¼                               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ HASH COMPARISON         â”‚     â”‚ HASH COMPARISON                 â”‚\n",
    "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚\n",
    "â”‚ source: hash(A,B,C)     â”‚     â”‚ source: hash(A,B,C,NEW)         â”‚\n",
    "â”‚ target: hash(A,B,C)     â”‚     â”‚ target: hash(A,B,C,NULL)        â”‚\n",
    "â”‚                         â”‚     â”‚                                 â”‚\n",
    "â”‚ Result: EQUAL           â”‚     â”‚ Result: NOT EQUAL               â”‚\n",
    "â”‚ â†’ No change detected    â”‚     â”‚ â†’ All records = Update          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚                               â”‚\n",
    "              â–¼                               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ OUTPUT                  â”‚     â”‚ OUTPUT                          â”‚\n",
    "â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚\n",
    "â”‚ total_count = 0         â”‚     â”‚ total_count = N (all rows)      â”‚\n",
    "â”‚ \"No updates to process\" â”‚     â”‚ operation_type = 'U'            â”‚\n",
    "â”‚ last_update_dt UNCHANGEDâ”‚     â”‚ last_update_dt = NOW            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, to_date\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List, Dict, Any, Tuple, Set\n",
    "import json\n",
    "\n",
    "# Technical columns added by the incremental load process (not from source)\n",
    "TECHNICAL_COLUMNS: Set[str] = {'operation_type', 'last_update_dt'}\n",
    "\n",
    "\n",
    "def get_table_columns(table_name: str, exclude_technical: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves list of columns from a table, excluding partitioning metadata.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Fully qualified table name\n",
    "        exclude_technical: If True, excludes operation_type and last_update_dt\n",
    "    \n",
    "    Returns:\n",
    "        List of column names\n",
    "    \"\"\"\n",
    "    describe_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "    columns = []\n",
    "    \n",
    "    for row in describe_df.collect():\n",
    "        col_name = row.col_name\n",
    "        \n",
    "        # Stop at partitioning section\n",
    "        if col_name and col_name.startswith(\"#\"):\n",
    "            break\n",
    "        \n",
    "        # Skip empty or invalid entries\n",
    "        if not col_name or col_name.strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Optionally exclude technical columns\n",
    "        if exclude_technical and col_name in TECHNICAL_COLUMNS:\n",
    "            continue\n",
    "        \n",
    "        columns.append(col_name)\n",
    "    \n",
    "    return columns\n",
    "\n",
    "\n",
    "def get_column_types(table_name: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves dictionary of {column_name: data_type} for a table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Fully qualified table name\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping column names to their data types\n",
    "    \"\"\"\n",
    "    describe_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "    column_types = {}\n",
    "    \n",
    "    for row in describe_df.collect():\n",
    "        col_name = row.col_name\n",
    "        data_type = row.data_type\n",
    "        \n",
    "        # Stop at partitioning section\n",
    "        if col_name and col_name.startswith(\"#\"):\n",
    "            break\n",
    "        \n",
    "        if col_name and col_name.strip() != \"\" and data_type:\n",
    "            column_types[col_name] = data_type\n",
    "    \n",
    "    return column_types\n",
    "\n",
    "\n",
    "def detect_new_columns(\n",
    "    source_table: str, \n",
    "    target_table: str\n",
    ") -> Tuple[List[str], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Detects new columns in source that don't exist in target.\n",
    "    Compares only business columns (excludes technical columns from target).\n",
    "    \n",
    "    Args:\n",
    "        source_table: Source table name\n",
    "        target_table: Target table name\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (list of new column names, dict of {column: data_type})\n",
    "    \"\"\"\n",
    "    source_columns = set(get_table_columns(source_table))\n",
    "    # Exclude technical columns when comparing - they are added by this process\n",
    "    target_business_columns = set(get_table_columns(target_table, exclude_technical=True))\n",
    "    \n",
    "    new_columns = list(source_columns - target_business_columns)\n",
    "    \n",
    "    if new_columns:\n",
    "        source_types = get_column_types(source_table)\n",
    "        new_column_types = {c: source_types[c] for c in new_columns if c in source_types}\n",
    "    else:\n",
    "        new_column_types = {}\n",
    "    \n",
    "    return new_columns, new_column_types\n",
    "\n",
    "\n",
    "def add_columns_to_delta_table(\n",
    "    table_name: str, \n",
    "    columns_with_types: Dict[str, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Adds new columns to an existing Delta table using ALTER TABLE.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Fully qualified table name\n",
    "        columns_with_types: Dictionary of {column_name: data_type}\n",
    "    \n",
    "    Returns:\n",
    "        List of successfully added column names\n",
    "    \"\"\"\n",
    "    added_columns = []\n",
    "    \n",
    "    if not columns_with_types:\n",
    "        return added_columns\n",
    "    \n",
    "    for col_name, col_type in columns_with_types.items():\n",
    "        try:\n",
    "            spark.sql(f\"ALTER TABLE {table_name} ADD COLUMNS (`{col_name}` {col_type})\")\n",
    "            print(f\"    [+] Added column '{col_name}' ({col_type})\")\n",
    "            added_columns.append(col_name)\n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"already exists\" in error_msg or \"found duplicate\" in error_msg:\n",
    "                print(f\"    [=] Column '{col_name}' already exists, skipping\")\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    return added_columns\n",
    "\n",
    "\n",
    "def update_new_columns_from_source(\n",
    "    target_table: str,\n",
    "    source_table: str,\n",
    "    id_key_column: str,\n",
    "    new_columns: List[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Updates values of new columns in target table from source using MERGE.\n",
    "    Only updates existing records (matched by id_key_column).\n",
    "    Does NOT update last_update_dt or operation_type.\n",
    "    \n",
    "    Args:\n",
    "        target_table: Target Delta table name\n",
    "        source_table: Source table name\n",
    "        id_key_column: Column name used for joining\n",
    "        new_columns: List of new column names to update\n",
    "    \"\"\"\n",
    "    if not new_columns:\n",
    "        return\n",
    "    \n",
    "    delta_table = DeltaTable.forName(spark, target_table)\n",
    "    source_df = spark.table(source_table)\n",
    "    \n",
    "    # Build SET clause: {target_column: source_column_expression}\n",
    "    # Using col() function for proper column reference\n",
    "    update_set = {c: col(f\"source.{c}\") for c in new_columns}\n",
    "    \n",
    "    print(f\"    Executing MERGE UPDATE for columns: {new_columns}\")\n",
    "    \n",
    "    (\n",
    "        delta_table.alias(\"target\")\n",
    "        .merge(\n",
    "            source_df.alias(\"source\"),\n",
    "            f\"target.`{id_key_column}` = source.`{id_key_column}`\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set=update_set)\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"    [âœ“] Updated {len(new_columns)} column(s) with values from source\")\n",
    "\n",
    "\n",
    "def handle_schema_evolution(\n",
    "    source_table_name: str,\n",
    "    target_table_name: str,\n",
    "    history_table_name: Optional[str],\n",
    "    id_key_column: str,\n",
    "    ignore_new_columns_as_change: bool\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Handles schema evolution by detecting and adding new columns.\n",
    "    \n",
    "    CRITICAL LOGIC:\n",
    "    - source_columns: All business columns from source (for writing)\n",
    "    - columns_for_comparison: Columns used for hash comparison (change detection)\n",
    "    \n",
    "    When ignore_new_columns_as_change=True:\n",
    "        1. Add new columns to target with ALTER TABLE\n",
    "        2. UPDATE target with values from source (MERGE)\n",
    "        3. Hash comparison uses OLD columns only â†’ no change detected\n",
    "    \n",
    "    When ignore_new_columns_as_change=False:\n",
    "        1. Add new columns to target as NULL (ALTER TABLE only)\n",
    "        2. No UPDATE\n",
    "        3. Hash comparison uses ALL columns â†’ change detected (NULL vs value)\n",
    "    \n",
    "    Args:\n",
    "        source_table_name: Source table name\n",
    "        target_table_name: Target table name  \n",
    "        history_table_name: History table name (optional)\n",
    "        id_key_column: Primary key column\n",
    "        ignore_new_columns_as_change: Schema evolution mode\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (source_columns, columns_for_comparison)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCHEMA EVOLUTION CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get business columns (exclude technical columns from target)\n",
    "    source_columns = get_table_columns(source_table_name)\n",
    "    target_business_columns = get_table_columns(target_table_name, exclude_technical=True)\n",
    "    \n",
    "    print(f\"Source columns count: {len(source_columns)}\")\n",
    "    print(f\"Target business columns count: {len(target_business_columns)}\")\n",
    "    \n",
    "    # Detect new columns\n",
    "    new_columns, new_column_types = detect_new_columns(source_table_name, target_table_name)\n",
    "    \n",
    "    if not new_columns:\n",
    "        print(\"\\n[âœ“] No new columns detected - schemas are aligned\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        return source_columns, source_columns\n",
    "    \n",
    "    print(f\"\\n[!] Detected {len(new_columns)} NEW COLUMN(S): {new_columns}\")\n",
    "    print(f\"    Mode: ignore_new_columns_as_change = {ignore_new_columns_as_change}\")\n",
    "    \n",
    "    if ignore_new_columns_as_change:\n",
    "        # ============================================================\n",
    "        # MODE: TRUE - Add columns WITH values, no change detection\n",
    "        # ============================================================\n",
    "        print(\"\\n--- Processing: Add columns with actual values ---\")\n",
    "        \n",
    "        # Step 1: Add columns to target table structure\n",
    "        print(f\"\\n  Step 1: Adding columns to TARGET ({target_table_name})\")\n",
    "        add_columns_to_delta_table(target_table_name, new_column_types)\n",
    "        \n",
    "        # Step 2: Update values from source using MERGE\n",
    "        print(f\"\\n  Step 2: Populating new columns with source values\")\n",
    "        update_new_columns_from_source(\n",
    "            target_table=target_table_name,\n",
    "            source_table=source_table_name,\n",
    "            id_key_column=id_key_column,\n",
    "            new_columns=new_columns\n",
    "        )\n",
    "        \n",
    "        # Step 3: Add columns to history table (values will be NULL for historical records)\n",
    "        if history_table_name and delta_table_exists(history_table_name):\n",
    "            print(f\"\\n  Step 3: Adding columns to HISTORY ({history_table_name})\")\n",
    "            add_columns_to_delta_table(history_table_name, new_column_types)\n",
    "            print(\"    Note: Historical records will have NULL for new columns\")\n",
    "        \n",
    "        # For comparison, use only OLD columns (pre-evolution)\n",
    "        # This ensures no false change detection due to new columns\n",
    "        columns_for_comparison = target_business_columns\n",
    "        \n",
    "        print(f\"\\n  Result:\")\n",
    "        print(f\"    - New columns added and populated: {new_columns}\")\n",
    "        print(f\"    - Hash comparison will use {len(columns_for_comparison)} OLD columns\")\n",
    "        print(f\"    - Changes in existing data WILL be detected\")\n",
    "        print(f\"    - New columns alone will NOT trigger updates\")\n",
    "        \n",
    "    else:\n",
    "        # ============================================================\n",
    "        # MODE: FALSE - Add columns as NULL, detect as change\n",
    "        # ============================================================\n",
    "        print(\"\\n--- Processing: Add columns as NULL (will detect as change) ---\")\n",
    "        \n",
    "        # Step 1: Add columns to target (will be NULL)\n",
    "        print(f\"\\n  Step 1: Adding columns to TARGET as NULL ({target_table_name})\")\n",
    "        add_columns_to_delta_table(target_table_name, new_column_types)\n",
    "        \n",
    "        # Step 2: Add columns to history (will be NULL)\n",
    "        if history_table_name and delta_table_exists(history_table_name):\n",
    "            print(f\"\\n  Step 2: Adding columns to HISTORY as NULL ({history_table_name})\")\n",
    "            add_columns_to_delta_table(history_table_name, new_column_types)\n",
    "        \n",
    "        # For comparison, use ALL columns including new ones\n",
    "        # NULL (target) vs value (source) = change detected\n",
    "        columns_for_comparison = source_columns\n",
    "        \n",
    "        print(f\"\\n  Result:\")\n",
    "        print(f\"    - New columns added as NULL: {new_columns}\")\n",
    "        print(f\"    - Hash comparison will use ALL {len(columns_for_comparison)} columns\")\n",
    "        print(f\"    - All records WILL be marked as 'Update'\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return source_columns, columns_for_comparison\n",
    "\n",
    "\n",
    "def create_incremental_table(\n",
    "    id_key_column: str,\n",
    "    target_table_name: str,\n",
    "    source_table_name: str,\n",
    "    metadata_full_load: str,\n",
    "    metadata_delta_load: str,\n",
    "    create_table_params: Optional[Dict[str, Any]] = None,\n",
    "    included_columns_for_hash: Optional[List[str]] = None,\n",
    "    excluded_columns_for_hash: Optional[List[str]] = None,\n",
    "    log_history: bool = False,\n",
    "    history_table_name: Optional[str] = None,\n",
    "    history_retention_days: Optional[int] = None,\n",
    "    ignore_new_columns_as_change: bool = True\n",
    ") -> Optional[DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates or updates an incremental Delta table with schema evolution support.\n",
    "    \n",
    "    CHANGE DETECTION LOGIC:\n",
    "    - Insertions: Records in source not in target (LEFT ANTI JOIN)\n",
    "    - Deletions: Records in target not in source (marked as 'D')\n",
    "    - Updates (tracked): Changes in specified columns â†’ new last_update_dt\n",
    "    - Updates (untracked): Changes in other columns â†’ preserve last_update_dt\n",
    "    - Reactivations: Previously deleted records reappearing in source\n",
    "    \n",
    "    SCHEMA EVOLUTION:\n",
    "    - New columns in source are detected automatically\n",
    "    - ignore_new_columns_as_change=True: Add with values, no change trigger\n",
    "    - ignore_new_columns_as_change=False: Add as NULL, triggers Update\n",
    "    \n",
    "    Args:\n",
    "        id_key_column: Primary key column name for matching records\n",
    "        target_table_name: Fully qualified target table name\n",
    "        source_table_name: Fully qualified source table name\n",
    "        metadata_full_load: JSON metadata for full load operation\n",
    "        metadata_delta_load: JSON metadata for delta load operation\n",
    "        create_table_params: Additional parameters for create_table()\n",
    "        included_columns_for_hash: Columns to include in change detection hash\n",
    "        excluded_columns_for_hash: Columns to exclude from change detection hash\n",
    "        log_history: Whether to log changes to history table\n",
    "        history_table_name: Name of history/audit table\n",
    "        history_retention_days: Days to retain history records\n",
    "        ignore_new_columns_as_change: Schema evolution mode\n",
    "            True (default): New columns added silently with values\n",
    "            False: New columns treated as changes, all records updated\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with changes applied, or None if no changes detected\n",
    "    \"\"\"\n",
    "    create_table_params = create_table_params or {}\n",
    "    \n",
    "    # ================================================================\n",
    "    # FULL LOAD - Target table does not exist\n",
    "    # ================================================================\n",
    "    if not delta_table_exists(target_table_name):\n",
    "        logger.info(\"Executing full load logic\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FULL LOAD - Creating new target table\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        make_env_tables(metadata_full_load)\n",
    "        \n",
    "        df = spark.sql(\"\"\"\n",
    "            SELECT  *,\n",
    "                    'I' AS operation_type,\n",
    "                    current_timestamp() AS last_update_dt\n",
    "            FROM    source \n",
    "        \"\"\")\n",
    "        \n",
    "        record_count = df.count()\n",
    "        print(f\"Inserting {record_count} records with operation_type='I'\")\n",
    "        \n",
    "        create_table(metadata_full_load, df, skip_data_lineage=True)\n",
    "        \n",
    "        print(f\"[âœ“] Full load completed: {target_table_name}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # ================================================================\n",
    "    # DELTA LOAD - Target table exists, process changes\n",
    "    # ================================================================\n",
    "    logger.info(\"Executing delta logic\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DELTA LOAD - Processing incremental changes\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 1: Handle schema evolution BEFORE creating views\n",
    "    # ----------------------------------------------------------------\n",
    "    # This ensures target table has new columns before we read it\n",
    "    source_columns, columns_for_comparison = handle_schema_evolution(\n",
    "        source_table_name=source_table_name,\n",
    "        target_table_name=target_table_name,\n",
    "        history_table_name=history_table_name if log_history else None,\n",
    "        id_key_column=id_key_column,\n",
    "        ignore_new_columns_as_change=ignore_new_columns_as_change\n",
    "    )\n",
    "    \n",
    "    # Columns to write (all source columns)\n",
    "    table_columns = source_columns\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 2: Determine hash columns for change detection\n",
    "    # ----------------------------------------------------------------\n",
    "    # Start with columns_for_comparison (may exclude new columns)\n",
    "    # Then apply user-specified include/exclude filters\n",
    "    \n",
    "    if included_columns_for_hash and excluded_columns_for_hash:\n",
    "        # Both specified: include only specified, then exclude\n",
    "        table_columns_for_hash = [\n",
    "            c for c in included_columns_for_hash\n",
    "            if c not in excluded_columns_for_hash \n",
    "            and c in columns_for_comparison\n",
    "        ]\n",
    "    elif included_columns_for_hash:\n",
    "        # Only include specified\n",
    "        table_columns_for_hash = [\n",
    "            c for c in included_columns_for_hash\n",
    "            if c in columns_for_comparison\n",
    "        ]\n",
    "    elif excluded_columns_for_hash:\n",
    "        # Exclude specified from comparison set\n",
    "        table_columns_for_hash = [\n",
    "            c for c in columns_for_comparison\n",
    "            if c not in excluded_columns_for_hash\n",
    "        ]\n",
    "    else:\n",
    "        # Default: use all comparison columns\n",
    "        table_columns_for_hash = list(columns_for_comparison)\n",
    "    \n",
    "    print(f\"Hash configuration:\")\n",
    "    print(f\"  - Columns for tracked changes hash: {len(table_columns_for_hash)}\")\n",
    "    print(f\"  - Columns for all-fields hash: {len(columns_for_comparison)}\")\n",
    "    print(f\"  - Total columns to write: {len(table_columns)}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 3: Create temp views (AFTER schema evolution)\n",
    "    # ----------------------------------------------------------------\n",
    "    # Now target view will include new columns with populated values\n",
    "    make_env_tables(metadata_delta_load)\n",
    "    \n",
    "    source = spark.table(\"source\")\n",
    "    target = spark.table(\"target\")\n",
    "    \n",
    "    now = datetime.now()\n",
    "    \n",
    "    print(f\"\\nProcessing changes at: {now}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 4: Detect INSERTIONS (new records in source)\n",
    "    # ----------------------------------------------------------------\n",
    "    df_new_insertions = (\n",
    "        source.alias(\"s\")\n",
    "        .join(\n",
    "            target.alias(\"t\"),\n",
    "            on=col(f\"s.{id_key_column}\") == col(f\"t.{id_key_column}\"),\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        .select(*[col(c) for c in table_columns])\n",
    "        .withColumn(\"operation_type\", lit(\"I\"))\n",
    "        .withColumn(\"last_update_dt\", lit(now))\n",
    "        .withColumn(\"update_type\", lit(\"Insert\"))\n",
    "    )\n",
    "    insertions_count = df_new_insertions.count()\n",
    "    print(f\"  Insertions (new records):           {insertions_count:>8}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 5: Detect DELETIONS (records removed from source)\n",
    "    # ----------------------------------------------------------------\n",
    "    # Note: We select from target, so columns exist\n",
    "    # Only mark active records (I or U) as deleted\n",
    "    df_new_deletions = (\n",
    "        target.alias(\"t\")\n",
    "        .join(\n",
    "            source.alias(\"s\"),\n",
    "            on=col(f\"s.{id_key_column}\") == col(f\"t.{id_key_column}\"),\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "        .filter(col(\"t.operation_type\").isin(\"I\", \"U\"))\n",
    "        .select(*[col(f\"t.{c}\") for c in table_columns])\n",
    "        .withColumn(\"operation_type\", lit(\"D\"))\n",
    "        .withColumn(\"last_update_dt\", lit(now))\n",
    "        .withColumn(\"update_type\", lit(\"Delete\"))\n",
    "    )\n",
    "    deletions_count = df_new_deletions.count()\n",
    "    print(f\"  Deletions (removed from source):    {deletions_count:>8}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 6: Create hashed DataFrames for change detection\n",
    "    # ----------------------------------------------------------------\n",
    "    # CRITICAL: Hash only on columns_for_comparison (excludes new columns if ignore=True)\n",
    "    \n",
    "    df_hashed_source = (\n",
    "        source\n",
    "        .withColumn(\"_hash_tracked\", spark_hash(*table_columns_for_hash))\n",
    "        .withColumn(\"_hash_all\", spark_hash(*columns_for_comparison))\n",
    "    )\n",
    "    \n",
    "    df_hashed_target = (\n",
    "        target\n",
    "        .withColumn(\"_hash_tracked\", spark_hash(*table_columns_for_hash))\n",
    "        .withColumn(\"_hash_all\", spark_hash(*columns_for_comparison))\n",
    "    )\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 7: Detect UPDATES in tracked columns\n",
    "    # ----------------------------------------------------------------\n",
    "    # These get new last_update_dt and operation_type='U'\n",
    "    df_new_updates_tracked = (\n",
    "        df_hashed_source.alias(\"s\")\n",
    "        .join(\n",
    "            df_hashed_target.alias(\"t\"),\n",
    "            on=col(f\"s.{id_key_column}\") == col(f\"t.{id_key_column}\"),\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .filter(\n",
    "            (col(\"s._hash_tracked\") != col(\"t._hash_tracked\")) &\n",
    "            (col(\"t.operation_type\") != \"D\")\n",
    "        )\n",
    "        .select(\n",
    "            *[col(f\"s.{c}\") for c in table_columns],\n",
    "            lit(\"U\").alias(\"operation_type\"),\n",
    "            lit(now).alias(\"last_update_dt\"),\n",
    "            lit(\"Update-tracked\").alias(\"update_type\")\n",
    "        )\n",
    "    )\n",
    "    updates_tracked_count = df_new_updates_tracked.count()\n",
    "    print(f\"  Updates (tracked columns):          {updates_tracked_count:>8}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 8: Detect UPDATES in untracked columns\n",
    "    # ----------------------------------------------------------------\n",
    "    # These preserve original last_update_dt and operation_type\n",
    "    df_new_updates_untracked = (\n",
    "        df_hashed_source.alias(\"s\")\n",
    "        .join(\n",
    "            df_hashed_target.alias(\"t\"),\n",
    "            on=col(f\"s.{id_key_column}\") == col(f\"t.{id_key_column}\"),\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .filter(\n",
    "            (col(\"s._hash_tracked\") == col(\"t._hash_tracked\")) &\n",
    "            (col(\"s._hash_all\") != col(\"t._hash_all\")) &\n",
    "            (col(\"t.operation_type\") != \"D\")\n",
    "        )\n",
    "        .select(\n",
    "            *[col(f\"s.{c}\") for c in table_columns],\n",
    "            col(\"t.operation_type\"),\n",
    "            col(\"t.last_update_dt\"),\n",
    "            lit(\"Update-untracked\").alias(\"update_type\")\n",
    "        )\n",
    "    )\n",
    "    updates_untracked_count = df_new_updates_untracked.count()\n",
    "    print(f\"  Updates (untracked columns):        {updates_untracked_count:>8}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 9: Detect REACTIVATIONS (previously deleted, now back)\n",
    "    # ----------------------------------------------------------------\n",
    "    df_reactivations = (\n",
    "        df_hashed_source.alias(\"s\")\n",
    "        .join(\n",
    "            df_hashed_target.alias(\"t\"),\n",
    "            on=col(f\"s.{id_key_column}\") == col(f\"t.{id_key_column}\"),\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .filter(col(\"t.operation_type\") == \"D\")\n",
    "        .select(\n",
    "            *[col(f\"s.{c}\") for c in table_columns],\n",
    "            lit(\"U\").alias(\"operation_type\"),\n",
    "            lit(now).alias(\"last_update_dt\"),\n",
    "            lit(\"Reactivate\").alias(\"update_type\")\n",
    "        )\n",
    "    )\n",
    "    reactivations_count = df_reactivations.count()\n",
    "    print(f\"  Reactivations (un-deleted):         {reactivations_count:>8}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 10: Calculate total and process\n",
    "    # ----------------------------------------------------------------\n",
    "    total_count = (\n",
    "        insertions_count + \n",
    "        deletions_count + \n",
    "        updates_tracked_count + \n",
    "        updates_untracked_count + \n",
    "        reactivations_count\n",
    "    )\n",
    "    print(\"-\"*40)\n",
    "    print(f\"  TOTAL CHANGES:                      {total_count:>8}\")\n",
    "    \n",
    "    if total_count == 0:\n",
    "        print(\"\\n[âœ“] Source and Target are aligned. No updates to process!\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        return None\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 11: Union all changes\n",
    "    # ----------------------------------------------------------------\n",
    "    df_all_changes = (\n",
    "        df_new_insertions\n",
    "        .union(df_new_deletions)\n",
    "        .union(df_new_updates_tracked)\n",
    "        .union(df_new_updates_untracked)\n",
    "        .union(df_reactivations)\n",
    "    )\n",
    "    \n",
    "    # Remove internal update_type column for target table\n",
    "    df_for_target = df_all_changes.drop(\"update_type\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 12: Log to history table (optional)\n",
    "    # ----------------------------------------------------------------\n",
    "    if log_history and history_table_name:\n",
    "        print(f\"\\nHistory logging enabled â†’ {history_table_name}\")\n",
    "        \n",
    "        df_for_history = df_all_changes.withColumn(\"log_datetime\", lit(now))\n",
    "        \n",
    "        if delta_table_exists(history_table_name):\n",
    "            print(f\"  Appending {total_count} records to existing history table\")\n",
    "            (\n",
    "                df_for_history.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"mergeSchema\", \"true\")  # Handle new columns\n",
    "                .saveAsTable(history_table_name)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  Creating new history table with {total_count} records\")\n",
    "            (\n",
    "                df_for_history.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"overwrite\")\n",
    "                .option(\"overwriteSchema\", \"true\")\n",
    "                .saveAsTable(history_table_name)\n",
    "            )\n",
    "        \n",
    "        # Apply retention policy\n",
    "        if history_retention_days and history_retention_days > 0:\n",
    "            cutoff_date = (datetime.now() - timedelta(days=history_retention_days)).date()\n",
    "            cutoff_str = cutoff_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            delta_history = DeltaTable.forName(spark, history_table_name)\n",
    "            old_records = (\n",
    "                delta_history.toDF()\n",
    "                .filter(to_date(\"log_datetime\") < lit(cutoff_str))\n",
    "                .count()\n",
    "            )\n",
    "            \n",
    "            if old_records > 0:\n",
    "                print(f\"  Retention: Deleting {old_records} records older than {cutoff_str}\")\n",
    "                delta_history.delete(f\"to_date(log_datetime) < '{cutoff_str}'\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # STEP 13: Write changes to target table\n",
    "    # ----------------------------------------------------------------\n",
    "    print(f\"\\nWriting {total_count} changes to target table...\")\n",
    "    create_table(metadata_delta_load, df_for_target, **create_table_params)\n",
    "    \n",
    "    print(f\"\\n[âœ“] Delta load completed successfully\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return df_all_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Declare variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Mount points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Declare 'mount_point' variable (dict) for synfs paths to ADLS\")\n",
    "\n",
    "from collections import defaultdict\n",
    "# GLOBAL VARIABLES\n",
    "\n",
    "#Job id\n",
    "job_id = mssparkutils.env.getJobId()\n",
    "\n",
    "#Mount points\n",
    "mount_point = defaultdict(dict)\n",
    "mount_synfs_path = defaultdict(dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Process date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from datetime import  timedelta\n",
    "if not process_date:\n",
    "    today = date.today()\n",
    "    process_date = today - timedelta(days=2)\n",
    "    process_date = process_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Process date has not been declared, new process date is: {process_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Workspace env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "workspace_name = mssparkutils.env.getWorkspaceName()\n",
    "workspace_env = re.findall(r\"(dev|prod|uat)(?=syn)\", workspace_name)\n",
    "\n",
    "# Get total cores\n",
    "executor_count = spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size() - 1\n",
    "cores_per_executor = int(spark.conf.get(\"spark.executor.cores\"))\n",
    "total_cores = executor_count * cores_per_executor\n",
    "\n",
    "# Partitioning and repartitioning optiomization\n",
    "optimal_parallelism = total_cores * 2\n",
    "#spark.conf.set(\"spark.default.parallelism\", str(optimal_parallelism))\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", str(optimal_parallelism))\n",
    "BASE_PARTITION_COUNT = optimal_parallelism\n",
    "\n",
    "\n",
    "#print(f\"BASE_PARTITION_COUNT (for table partitioning and spark.default.parallelism / spark.sql.shuffle.partitions): {BASE_PARTITION_COUNT}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Alter delta sharing protocol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    import delta_sharing\n",
    "    import json\n",
    "    from delta_sharing.rest_client import DeltaSharingProfile\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_read_from_file(profile: str) -> \"DeltaSharingProfile\":\n",
    "        # Splitting the profile parameter into key_vault_ls and secret_name\n",
    "        try:\n",
    "            key_vault_ls, secret_name = profile.split(\";\")\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Invalid format for profile. Expected format: 'key_vault_ls;secret_name'\")\n",
    "\n",
    "        print(profile)\n",
    "\n",
    "        # Retrieving the secret from Azure Key Vault\n",
    "        secret_content = mssparkutils.credentials.getSecretWithLS(key_vault_ls, secret_name)\n",
    "\n",
    "        # Parsing the secret content into a DeltaSharingProfile object\n",
    "        try:\n",
    "            return DeltaSharingProfile.from_json(secret_content)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse DeltaSharingProfile: {e}\")\n",
    "\n",
    "    DeltaSharingProfile.read_from_file = custom_read_from_file \n",
    "    print(f\"    Delta sharing imported, version: {delta_sharing.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"    No delta sharing module\")\n",
    "\n",
    "\n",
    "# *** Usage ***\n",
    "# table_url = \"SOLUTION_KEY_VAULT_LS;DeltaSharingProfile#corporate_finance_analytics_sustainability.finsight_core_model.dimensions_item_taxonomy\"\n",
    "## SOLUTION_KEY_VAULT_LS - key vault linked service name\n",
    "## DeltaSharingProfile secret with delta sharing profile (token, endpoint)\n",
    "# df = delta_sharing.load_as_pandas(table_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## delete_folder_with_linked_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder_with_linked_service(Linked_Service_Name, container, path, ENV, recursive = True):\n",
    "\n",
    "    # Define the mounting scope and retrieve the mount point name\n",
    "    mount_scope = \"workspace\"  # Scope of the mount operation\n",
    "    mount_point_name = get_mount_name(Linked_Service_Name, container, mount_scope, ENV)\n",
    "\n",
    "    # Mount the linked service storage container and retrieve paths\n",
    "    mount_point[mount_point_name], mount_synfs_path[mount_point_name] = mount_from_linkedservice(\n",
    "        Linked_Service_Name, container, mount_scope, ENV\n",
    "    )\n",
    "\n",
    "    # Define the full path to the success file\n",
    "    file_synfs_path = f\"{mount_synfs_path[mount_point_name]}{path}\"\n",
    "\n",
    "    # Delete any existing success file to ensure a clean state\n",
    "    print(f\"Deleting data from: {file_synfs_path}\")\n",
    "    try:\n",
    "        mssparkutils.fs.rm(file_synfs_path, recursive)  # Remove the file if it exists\n",
    "    except:\n",
    "        print(\"Folder does not exist\")  # Handle case where the file is not found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### delete_files_in_folder_with_linked_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_in_folder_with_linked_service(Linked_Service_Name, container, path, ENV, recursive = False):\n",
    "\n",
    "    # Define the mounting scope and retrieve the mount point name\n",
    "    mount_scope = \"workspace\"  # Scope of the mount operation\n",
    "    mount_point_name = get_mount_name(Linked_Service_Name, container, mount_scope, ENV)\n",
    "\n",
    "    # Mount the linked service storage container and retrieve paths\n",
    "    mount_point[mount_point_name], mount_synfs_path[mount_point_name] = mount_from_linkedservice(\n",
    "        Linked_Service_Name, container, mount_scope, ENV\n",
    "    )\n",
    "\n",
    "    # Define the full path to the success file\n",
    "    file_synfs_path = f\"{mount_synfs_path[mount_point_name]}{path}\"\n",
    "    files = list_files_recursive(file_synfs_path, extension = None, file_name_like = None)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = file['path']\n",
    "        # Delete any existing success file to ensure a clean state\n",
    "        print(f\"Deleting data from: {file_path}\")\n",
    "        try:\n",
    "            mssparkutils.fs.rm(file_path, recursive)  # Remove the file if it exists\n",
    "        except:\n",
    "            print(f\"File {file_path} does not exist\")  # Handle case where the file is not found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### audit_whitespace_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOOGLPE1 20250625 : audit report + 'normalize' and/or 'trim' function to replace all whitespace-like but not-space chars with space ' ' \\u0020, and trim leading/trailing blanks if requested\n",
    "\n",
    "from pyspark.sql.functions import col, trim, concat_ws, lit, udf\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# === Default whitespace-like characters ===\n",
    "DEFAULT_WHITESPACE_MAP = {\n",
    "    '\\u0009': '<TAB>',\n",
    "    '\\u000A': '<LF>',\n",
    "    '\\u000B': '<VT>',\n",
    "    '\\u000C': '<FF>',\n",
    "    '\\u000D': '<CR>',\n",
    "    '\\u0020': '<SPACE>',\n",
    "    '\\u00A0': '<NBSP>',\n",
    "    '\\u1680': '<OGHAM_SPACE>',\n",
    "    '\\u180E': '<MONGOLIAN_VOWEL_SEP>',\n",
    "    '\\u2000': '<EN_QUAD>',\n",
    "    '\\u2001': '<EM_QUAD>',\n",
    "    '\\u2002': '<EN_SPACE>',\n",
    "    '\\u2003': '<EM_SPACE>',\n",
    "    '\\u2004': '<THREE_PER_EM_SPACE>',\n",
    "    '\\u2005': '<FOUR_PER_EM_SPACE>',\n",
    "    '\\u2006': '<SIX_PER_EM_SPACE>',\n",
    "    '\\u2007': '<FIGURE_SPACE>',\n",
    "    '\\u2008': '<PUNCTUATION_SPACE>',\n",
    "    '\\u2009': '<THIN_SPACE>',\n",
    "    '\\u200A': '<HAIR_SPACE>',\n",
    "    '\\u2028': '<LINE_SEPARATOR>',\n",
    "    '\\u2029': '<PARAGRAPH_SEPARATOR>',\n",
    "    '\\u202F': '<NARROW_NO_BREAK_SPACE>',\n",
    "    '\\u205F': '<MEDIUM_MATH_SPACE>',\n",
    "    '\\u3000': '<IDEOGRAPHIC_SPACE>',\n",
    "    '\\uFEFF': '<ZERO_WIDTH_NBSP>'\n",
    "}\n",
    "\n",
    "# === Audit Function ===\n",
    "def audit_whitespace_usage(df, key_columns, columns_to_process=None, whitespace_map=None):\n",
    "    if not isinstance(key_columns, list):\n",
    "        raise ValueError(\"key_columns must be a list\")\n",
    "\n",
    "    # Always start from the full known list\n",
    "    full_map = DEFAULT_WHITESPACE_MAP\n",
    "\n",
    "    # Use the passed map to filter out chars the user has already handled\n",
    "    excluded_chars = set(whitespace_map.keys()) if whitespace_map else set()\n",
    "    audit_chars = {k: v for k, v in full_map.items() if k not in excluded_chars}\n",
    "\n",
    "    broadcasted_whitespace_set = set(audit_chars.keys())\n",
    "    broadcasted_visual_map = dict(audit_chars)\n",
    "\n",
    "    @udf(returnType=StringType())\n",
    "    def visualize_whitespace(val):\n",
    "        if val is None:\n",
    "            return None\n",
    "        return ''.join([broadcasted_visual_map.get(ch, ch) for ch in val])\n",
    "\n",
    "    @udf(returnType=StringType())\n",
    "    def needs_trim(val):\n",
    "        if val is None or not isinstance(val, str):\n",
    "            return 'N'\n",
    "        return 'Y' if val != val.strip() else 'N'\n",
    "\n",
    "    @udf(returnType=StringType())\n",
    "    def contains_nonspace_whitespace(val):\n",
    "        if val is None or not isinstance(val, str):\n",
    "            return 'N'\n",
    "        for ch in val:\n",
    "            if ch in broadcasted_whitespace_set and ch != '\\u0020':\n",
    "                return 'Y'\n",
    "        return 'N'\n",
    "\n",
    "    if not columns_to_process:\n",
    "        columns_to_process = [c for c in df.columns if c not in key_columns]\n",
    "\n",
    "    audit_rows = []\n",
    "    for colname in columns_to_process:\n",
    "        temp = df.withColumn(\"original_value\", col(colname)) \\\n",
    "                 .withColumn(\"visualized_value\", visualize_whitespace(col(colname))) \\\n",
    "                 .withColumn(\"needs_trimming\", needs_trim(col(colname))) \\\n",
    "                 .withColumn(\"has_nonspace_whitespace\", contains_nonspace_whitespace(col(colname))) \\\n",
    "                 .withColumn(\"column_name\", lit(colname))\n",
    "\n",
    "        for key in key_columns:\n",
    "            temp = temp.withColumn(key, col(key).cast(\"string\"))\n",
    "\n",
    "        temp = temp.withColumn(\"keyfield_id\", concat_ws(\"|\", *[col(k) for k in key_columns]))\n",
    "\n",
    "        result = temp.select(\n",
    "            \"keyfield_id\", \"column_name\", \"original_value\", \"visualized_value\",\n",
    "            \"needs_trimming\", \"has_nonspace_whitespace\"\n",
    "        ).filter((col(\"needs_trimming\") == \"Y\") | (col(\"has_nonspace_whitespace\") == \"Y\"))\n",
    "\n",
    "        audit_rows.append(result)\n",
    "\n",
    "    if audit_rows:\n",
    "        from functools import reduce\n",
    "        return reduce(lambda a, b: a.unionByName(b), audit_rows)\n",
    "    else:\n",
    "        return spark.createDataFrame([], schema=StructType([\n",
    "            StructField(\"keyfield_id\", StringType()),\n",
    "            StructField(\"column_name\", StringType()),\n",
    "            StructField(\"original_value\", StringType()),\n",
    "            StructField(\"visualized_value\", StringType()),\n",
    "            StructField(\"needs_trimming\", StringType()),\n",
    "            StructField(\"has_nonspace_whitespace\", StringType()),\n",
    "        ]))\n",
    "\n",
    "def normalize_whitespace_fields(\n",
    "    df,\n",
    "    columns_to_process=None,\n",
    "    trim=True,\n",
    "    normalize=True,\n",
    "    whitespace_map=None,\n",
    "):\n",
    "    if not whitespace_map:\n",
    "        whitespace_map = DEFAULT_WHITESPACE_MAP\n",
    "\n",
    "    # Validate and determine columns to process\n",
    "    if columns_to_process is None or columns_to_process == []:\n",
    "        columns_to_process = [\n",
    "            f.name for f in df.schema.fields\n",
    "            if isinstance(f.dataType, StringType)\n",
    "        ]\n",
    "    elif not isinstance(columns_to_process, list):\n",
    "        raise TypeError(\"columns_to_process must be a list of strings or None.\")\n",
    "    elif not all(isinstance(c, str) and c.strip() for c in columns_to_process):\n",
    "        raise ValueError(\"columns_to_process must be a list of non-empty strings.\")\n",
    "\n",
    "    whitespace_chars = list(whitespace_map.keys())\n",
    "    universal_space = \"\\u0020\"\n",
    "\n",
    "    def clean_value(val):\n",
    "        if val is None or not isinstance(val, str):\n",
    "            return val\n",
    "        result = val\n",
    "        if normalize:\n",
    "            for ch in whitespace_chars:\n",
    "                result = result.replace(ch, universal_space)\n",
    "        if trim:\n",
    "            result = result.strip()\n",
    "        return result\n",
    "\n",
    "    clean_value_udf = udf(clean_value, StringType())\n",
    "\n",
    "    cleaned_df = df\n",
    "    for colname in columns_to_process:\n",
    "        if colname not in df.columns:\n",
    "            continue\n",
    "        cleaned_df = cleaned_df.withColumn(colname, clean_value_udf(colname))\n",
    "\n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### full_compare_2_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, concat_ws\n",
    "\n",
    "def full_compare_2_dfs(\n",
    "    df1,\n",
    "    df2,\n",
    "    key_fields,\n",
    "    id_label1=\"df1\",\n",
    "    id_label2=\"df2\"\n",
    "):\n",
    "\n",
    "    def _schema_diffs(a, b):\n",
    "        a_cols, b_cols = set(a.columns), set(b.columns)\n",
    "        only_in_a = sorted(a_cols - b_cols)\n",
    "        only_in_b = sorted(b_cols - a_cols)\n",
    "        # check type mismatches for common columns\n",
    "        a_types = {f.name: f.dataType.simpleString() for f in a.schema.fields}\n",
    "        b_types = {f.name: f.dataType.simpleString() for f in b.schema.fields}\n",
    "        common = a_cols & b_cols\n",
    "        type_mismatch = sorted(\n",
    "            [(c, a_types[c], b_types[c]) for c in common if a_types[c] != b_types[c]]\n",
    "        )\n",
    "        return only_in_a, only_in_b, type_mismatch\n",
    "\n",
    "    if isinstance(key_fields, str):\n",
    "        key_fields = [key_fields]\n",
    "\n",
    "    # Validate that schemas match (names & types). Give a precise diff if not.\n",
    "    only_in_df1, only_in_df2, type_mismatch = _schema_diffs(df1, df2)\n",
    "    if only_in_df1 or only_in_df2 or type_mismatch:\n",
    "        msgs = []\n",
    "        if only_in_df1:\n",
    "            msgs.append(f\"Only in {id_label1}: {only_in_df1}\")\n",
    "        if only_in_df2:\n",
    "            msgs.append(f\"Only in {id_label2}: {only_in_df2}\")\n",
    "        if type_mismatch:\n",
    "            # e.g. [('amount','bigint','string'), ...]\n",
    "            pretty = [f\"{c}: {t1} vs {t2}\" for c, t1, t2 in type_mismatch]\n",
    "            msgs.append(\"Type mismatches (col: df1 vs df2): \" + \", \".join(pretty))\n",
    "        raise ValueError(\"Schema mismatch. \" + \" | \".join(msgs))\n",
    "\n",
    "    # Use df1 column order as canonical\n",
    "    table_cols = df1.columns\n",
    "    df2 = df2.select(*df1.columns)\n",
    "\n",
    "    only_in_df1 = df1.subtract(df2).withColumn(\"_source\", lit(id_label1)).withColumn(\"keyfield_id\", concat_ws(\"|\", *[col(k) for k in key_fields]))\n",
    "    only_in_df2 = df2.subtract(df1).withColumn(\"_source\", lit(id_label2)).withColumn(\"keyfield_id\", concat_ws(\"|\", *[col(k) for k in key_fields]))\n",
    "\n",
    "    # Reorder and output\n",
    "    cols_order = [\"keyfield_id\", \"_source\"] + table_cols\n",
    "    combined = only_in_df1.select(*cols_order).unionByName(only_in_df2.select(*cols_order)).orderBy(\"keyfield_id\", \"_source\")\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### convert_columns_to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "print(\"Function: convert_columns_to_date\")\n",
    "\n",
    "def convert_columns_to_date(\n",
    "    df: DataFrame, \n",
    "    column_list: List[str], \n",
    "    date_format: str = \"yyyyMMdd\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts text columns to date type with proper null handling.\n",
    "    Ensures dates are >= 1900-01-01. Dates before 1900-01-01 are set to 1900-01-01.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        column_list: List of column names to convert\n",
    "        date_format: Date format (default 'yyyyMMdd')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns converted to date type\n",
    "    \"\"\"\n",
    "    min_allowed_date = F.to_date(F.lit(\"1900-01-01\"))\n",
    "    \n",
    "    for col_name in column_list:\n",
    "        converted_date = F.to_date(F.col(col_name).cast(\"string\"), date_format)\n",
    "        \n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.when(\n",
    "                (F.col(col_name).isNull()) | \n",
    "                (F.trim(F.col(col_name)) == \"\") |\n",
    "                (F.trim(F.col(col_name)) == \"0\"),\n",
    "                F.lit(None).cast(\"date\")\n",
    "            ).otherwise(\n",
    "                # Use F.greatest to ensure date >= 1900-01-01\n",
    "                F.greatest(converted_date, min_allowed_date)\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### trim_leading_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import List\n",
    "\n",
    "print(\"Function: trim_leading_zeros\")\n",
    "\n",
    "def trim_leading_zeros(df: DataFrame, column_list: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Removes leading zeros from specified columns, preserving '0' for '0000' values.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        column_list: List of column names to process\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with processed columns\n",
    "        \n",
    "    Examples:\n",
    "        '0001' -> '1'\n",
    "        '0000' -> '0'\n",
    "        '0' -> '0'\n",
    "    \"\"\"\n",
    "    for col_name in column_list:\n",
    "        df = df.withColumn(\n",
    "            col_name, \n",
    "            F.regexp_replace(F.col(col_name), '^0+(?=.)', '')\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### rename_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, column_mapping):\n",
    "    \"\"\"\n",
    "    Renames columns in DataFrame according to the provided mapping.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with original column names\n",
    "        column_mapping: Dictionary mapping {old_name: new_name}\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with renamed columns\n",
    "    \"\"\"\n",
    "    df_renamed = df\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns:\n",
    "            df_renamed = df_renamed.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    return df_renamed\n",
    "\n",
    "# Example\n",
    "#\n",
    "## Column mapping\n",
    "#column_mapping = {\n",
    "#    'grdcode': 'GRD_code',\n",
    "#    'packagingSpecNo': 'PackagingSpecNo',\n",
    "#    'quantity': 'Quantity'\n",
    "#}\n",
    "#\n",
    "## Function call\n",
    "#df_transformed = rename_columns(df_source, column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
