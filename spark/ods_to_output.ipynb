{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ODS to output\n",
        "process and output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "| Modified by |  Last modification | Task Number | Description |\n",
        "| ----------- | ----------- | ----------- |----------- |\n",
        "| Luca Barcella | 2025-04-08 |000000|Create notebook|\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "ENV = None\n",
        "PROJECT_NAME = \"ods\"\n",
        "process_date = None\n",
        "\n",
        "#Metadata store\n",
        "admin_schema_name = \"admin\"\n",
        "data_lineage_table_name = \"data_lineage\"\n",
        "admin_table_name_to_output = \"ods_metadata_to_output_v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false
        }
      },
      "source": [
        "%run \"Utils/data_processing_utilis_v2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "if not process_date:\n",
        "    process_date = date.today().strftime('%Y-%m-%d')\n",
        "    print(f\"Process date has not been declared, new process date is: {process_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def cast_dataframe_to_schema(source_df, target_df):\n",
        "    \"\"\"\n",
        "    Cast all columns from source_df to match the exact data types in target_df.\n",
        "    This ensures schema compatibility and prevents type mismatch errors during append operations.\n",
        "    \n",
        "    Args:\n",
        "        source_df: Source DataFrame whose columns will be cast\n",
        "        target_df: Target DataFrame whose schema will be used as reference\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with columns from source_df cast to target_df data types\n",
        "    \"\"\"\n",
        "    # Find common columns between both tables\n",
        "    common_columns = sorted(list(set(source_df.columns) & set(target_df.columns)))\n",
        "    \n",
        "    # Create a mapping of column names to their target data types\n",
        "    target_schema = {field.name: field.dataType for field in target_df.schema.fields}\n",
        "    \n",
        "    # Cast each column to the target type\n",
        "    result_df = source_df.select([\n",
        "        F.col(col_name).cast(target_schema[col_name]) \n",
        "        for col_name in common_columns\n",
        "    ])\n",
        "    \n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# <mark>Code</mark>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Source system ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"ignore\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"table\":\"dbo.GlobalOptionsetMetadata\", \"view\":\"GlobalOptionsetMetadata\", \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "# Get Core source system ID  - 1\n",
        "core_source_system_id = spark.sql(\"\"\"\n",
        "    SELECT      DISTINCT Option \n",
        "    FROM        GlobalOptionsetMetadata\n",
        "    WHERE       OptionSetName = 'lk_sourcesystemunit'\n",
        "    AND         lower(LocalizedLabel) = lower('Mars Core systems (ATLAS/MDG/Veritas)')\n",
        "    LIMIT 1\n",
        "\"\"\").collect()[0][0]\n",
        "print(f\"core_source_system_id: {core_source_system_id}\")\n",
        "\n",
        "# Get UK Kind source system ID - 2\n",
        "uk_kind_source_system_id = spark.sql(\"\"\"\n",
        "    SELECT      DISTINCT Option \n",
        "    FROM        GlobalOptionsetMetadata\n",
        "    WHERE       OptionSetName = 'lk_sourcesystemunit'\n",
        "    AND         lower(LocalizedLabel) = lower('UK Kind')\n",
        "    LIMIT 1\n",
        "\"\"\").collect()[0][0]\n",
        "print(f\"uk_kind_source_system_id: {uk_kind_source_system_id}\")\n",
        "\n",
        "# Get Fusion source system ID -3\n",
        "fusion_source_system_id = spark.sql(\"\"\"\n",
        "    SELECT      DISTINCT Option \n",
        "    FROM        GlobalOptionsetMetadata\n",
        "    WHERE       OptionSetName = 'lk_sourcesystemunit'\n",
        "    AND         lower(LocalizedLabel) = lower('Fusion')\n",
        "    LIMIT 1\n",
        "\"\"\").collect()[0][0]\n",
        "print(f\"fusion_source_system_id: {fusion_source_system_id}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_plant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_plant\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"view_name\":\"dimentitieshierarchy\", \"path\":\"environments/s4o/prod/s4o_process/dimentitieshierarchy\", \"file_format\":\"parquet\", \"container\":\"synapse\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}},\n",
        "        {\"view_name\":\"plant_site_code_xref\", \"path\":\"SKUPRINT/plant_site_code_xref.csv\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}},\n",
        "        {\"view_name\":\"0plant_attr_aep\",  \"path\":\"ATLAS/AEP/MASTER_DATA/0PLANT_ATTR/DATA\", \"container\":\"output\" ,\"linked_service_name\":\"MARS_ANALYTICS_ADLS_LS\", \"opertation_type\":\"Linked Service\"},\n",
        "        {\"view_name\":\"0plant_attr_aap\",  \"path\":\"ATLAS/AAP/MASTER_DATA/0PLANT_ATTR/DATA\", \"container\":\"output\" ,\"linked_service_name\":\"MARS_ANALYTICS_ADLS_LS\", \"opertation_type\":\"Linked Service\"},\n",
        "        {\"view_name\":\"0plant_attr_app\",  \"path\":\"ATLAS/APP/MASTER_DATA/0PLANT_ATTR/DATA\", \"container\":\"output\" ,\"linked_service_name\":\"MARS_ANALYTICS_ADLS_LS\", \"opertation_type\":\"Linked Service\"},\n",
        "\t\t{\"table\":\"dbo.v_odsrules\",      \"view\":\"v_odsrules\",    \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df_atlas = spark.sql(\"\"\"\n",
        "    WITH Enablon AS (\n",
        "        SELECT  DISTINCT\n",
        "                Country                                         AS enablon_country,\n",
        "                Organization                                    AS enablon_organization,\n",
        "                Segment                                         AS enablon_segment,\n",
        "                Division                                        AS enablon_division,\n",
        "                Business_Unit                                   AS enablon_business_unit,\n",
        "                Site                                            AS enablon_site\n",
        "        FROM    dimentitieshierarchy\n",
        "    ),\n",
        "    Xref AS (\n",
        "        SELECT  plant,\n",
        "                site_code\n",
        "        FROM    plant_site_code_xref\n",
        "    ),\n",
        "    0plant_attr_union AS (\n",
        "        SELECT  WERKS_PK                                        AS plant_code,\n",
        "                NAME1                                           AS plant_name,\n",
        "                VKORG                                           AS sales_organisation,\n",
        "                LAND1                                           AS country_code,\n",
        "                BUKRS                                           AS company_code,\n",
        "                replace(ltrim(replace(KUNNR,'0',' ')),' ','0')  AS customer_number,\n",
        "                replace(ltrim(replace(LIFNR,'0',' ')),' ','0')  AS vendor_number,\n",
        "                _MARS_SITECODE                                  AS site_code_atlas,\n",
        "                LOAD_DATE\n",
        "        FROM    0plant_attr_aep\n",
        "\n",
        "        UNION \n",
        "\n",
        "        SELECT  WERKS_PK                                        AS plant_code,\n",
        "                NAME1                                           AS plant_name,\n",
        "                VKORG                                           AS sales_organisation,\n",
        "                LAND1                                           AS country_code,\n",
        "                BUKRS                                           AS company_code,\n",
        "                replace(ltrim(replace(KUNNR,'0',' ')),' ','0')  AS customer_number,\n",
        "                replace(ltrim(replace(LIFNR,'0',' ')),' ','0')  AS vendor_number,\n",
        "                _MARS_SITECODE                                  AS site_code_atlas,\n",
        "                LOAD_DATE\n",
        "        FROM    0plant_attr_aap\n",
        "\n",
        "        UNION\n",
        "\n",
        "        SELECT  WERKS_PK                                        AS plant_code,\n",
        "                NAME1                                           AS plant_name,\n",
        "                VKORG                                           AS sales_organisation,\n",
        "                LAND1                                           AS country_code,\n",
        "                BUKRS                                           AS company_code,\n",
        "                replace(ltrim(replace(KUNNR,'0',' ')),' ','0')  AS customer_number,\n",
        "                replace(ltrim(replace(LIFNR,'0',' ')),' ','0')  AS vendor_number,\n",
        "                _MARS_SITECODE                                  AS site_code_atlas,\n",
        "                LOAD_DATE\n",
        "        FROM    0plant_attr_app\n",
        "    ),\n",
        "    Atlas AS (\n",
        "        SELECT  DISTINCT \n",
        "                *\n",
        "        FROM (\n",
        "            SELECT  plant_code,\n",
        "                    LAST_VALUE(plant_name, TRUE) OVER (PARTITION BY plant_code ORDER BY LOAD_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS plant_name,\n",
        "                    LAST_VALUE(sales_organisation, TRUE) OVER (PARTITION BY plant_code ORDER BY LOAD_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS sales_organisation,\n",
        "                    LAST_VALUE(country_code, TRUE) OVER (PARTITION BY plant_code ORDER BY LOAD_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS country_code,\n",
        "                    LAST_VALUE(company_code, TRUE) OVER (PARTITION BY plant_code ORDER BY LOAD_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS company_code,\n",
        "                    LAST_VALUE(customer_number, TRUE) OVER (PARTITION BY plant_code ORDER BY LOAD_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS customer_number,\n",
        "                    LAST_VALUE(vendor_number, TRUE) OVER (PARTITION BY plant_code ORDER BY LOAD_DATE ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS vendor_number\n",
        "            FROM    0plant_attr_union\n",
        "        )\n",
        "    )\n",
        "\n",
        "    SELECT      a.*,\n",
        "                x.*,\n",
        "                e.*,\n",
        "                '1'                             AS SourceSystemUnit,\n",
        "                a.plant_code                    AS src_agnostic_unique_ID    \n",
        "    FROM        Atlas a \n",
        "    LEFT JOIN   Xref x \n",
        "        ON      a.plant_code = x.plant\n",
        "    LEFT JOIN   Enablon e \n",
        "        ON      x.site_code = e.enablon_site\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "df_system_agnostic = spark.sql(\"\"\"\n",
        "    SELECT  value2                              AS plant_code,\n",
        "            value3                              AS plant_name,\n",
        "            value4                              AS country_code,\n",
        "            CONCAT(value1, ':', value5)         AS company_code,\n",
        "            CONCAT(value1, ':', value6)         AS sales_organisation,\n",
        "            value1                              AS SourceSystemUnit,\n",
        "            CONCAT(value1, ':', value2)         AS src_agnostic_unique_ID\n",
        "    FROM    v_odsrules\n",
        "    WHERE   function = 'SYSTEM_AGNOSTIC_SETUP'\n",
        "        AND criteria = 'PLANT'\n",
        "\"\"\")\n",
        "\n",
        "df_joined = df_atlas.unionByName(df_system_agnostic, allowMissingColumns=True).withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_material"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### dim_material <u>CORE</u> \n",
        "Atlas/Veritas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"dim_material\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\", \"table\":\"purchase_order_line_item\",             \"view\":\"purchase_order_line_item\",          \"project_name\":\"atlas\",     \"params\":{\"ignore_env\": true}},\n",
        "        {\"schema\":\"normalized\", \"table\":\"process_order\",                        \"view\":\"process_order\",                     \"project_name\":\"atlas\",     \"params\":{\"ignore_env\": true}},\n",
        "        {\"schema\":\"normalized\", \"table\":\"mdg_classification_zzglobal01\",        \"view\":\"mdg_classification_zzglobal01_decoded\"},\n",
        "        {\"schema\":\"normalized\", \"table\":\"mdg_classification_zzpack01\",          \"view\":\"mdg_classification_zzpack01_decoded\"},\n",
        "        {\"schema\":\"normalized\", \"table\":\"mdg_classification_zzraw01\",           \"view\":\"mdg_classification_zzraw01_decoded\"},\n",
        "        {\"schema\":\"normalized\", \"table\":\"mdg_classification_zzudc\",             \"view\":\"mdg_classification_zzudc_decoded\"},\n",
        "        {\"schema\":\"output\",     \"table\":\"dim_plant\",                            \"view\":\"dim_plant\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_regional_codes\",                   \"view\":\"mdg_regional_codes\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_material_group\",                   \"view\":\"mdg_material_group\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"atlas_sievo_categories\",               \"view\":\"atlas_sievo_categories\"},\n",
        "\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_finished_products_fert\",           \"view\":\"mdg_finished_products_fert\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_packaging_and_pack_verp\",          \"view\":\"mdg_packaging_and_pack_verp\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_hierarchy_node_materials_zhie\",    \"view\":\"mdg_hierarchy_node_materials_zhie\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_raw_material_roh\",                 \"view\":\"mdg_raw_material_roh\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_food_contact_zqfc\",                \"view\":\"mdg_food_contact_zqfc\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_unval_material_rent_pal_unbw\",     \"view\":\"mdg_unval_material_rent_pal_unbw\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"mdg_representative_item_zrep\",         \"view\":\"mdg_representative_item_zrep\"},\n",
        "\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_item_taxonomy\",                 \"view\":\"helios_item_taxonomy\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_it_copack_group\",               \"view\":\"helios_it_copack_group\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_it_pack_size\",                  \"view\":\"helios_it_pack_size\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_it_tech\",                       \"view\":\"helios_it_tech\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_it_intext_mfg_group\",           \"view\":\"helios_it_intext_mfg_group\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_it_ec_group\",                   \"view\":\"helios_it_ec_group\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"helios_it_financial_product_segment\",  \"view\":\"helios_it_financial_product_segment\"},\n",
        "        {                       \"table\":\"dbo.v_odsrules\",                       \"view\":\"v_odsrules\",    \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "print(f\"core_source_system_id: {core_source_system_id}\")\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                        1. Combine material data by material type from RAW                                  =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "fields = [\n",
        "    \"Material\", \"Description\", \"Representative_Item\", \"Material_Type\", \"Created_By\", \"Created_On\", \"Gross_Weight\", \"Net_Weight\", \"Weight_Unit\", \n",
        "        \"X_Plant_Status\", \"Length\", \"Width\", \"Height\", \"EAN\", \"EAN_Category\", \"Unit_of_Dimension\", \"Material_Group\", \"Base_Unit_of_Measure\", \"Document\",\n",
        "        \"Total_shelf_life\", \"Min_Rem_Shelf_Life\", \"Merchandising_Unit\", \"Retail_Sales_Unit\", \"Traded_Unit\", \"Intermediate_Product_Component\", \"Semi_Finished_Product\"\n",
        "]\n",
        "\n",
        "# list of source tables\n",
        "tables = [\n",
        "    \"mdg_finished_products_fert\",                                   # FERT\n",
        "    \"mdg_packaging_and_pack_verp\",                                  # VERP\n",
        "    \"mdg_food_contact_zqfc\",                                        # ZGFC\n",
        "    \"mdg_hierarchy_node_materials_zhie\",                            # ZHIE\n",
        "    \"mdg_raw_material_roh\",                                         # ROH\n",
        "    \"mdg_unval_material_rent_pal_unbw\",                             # UNBW\n",
        "    \"mdg_representative_item_zrep\"                                  # ZREP\n",
        "]\n",
        "\n",
        "dfs = [spark.table(t).select(fields) for t in tables]               # build list of DataFrames\n",
        "df = reduce(DataFrame.unionByName, dfs)                             # union them all\n",
        "df.createOrReplaceTempView(\"df_material\")                           # create view\n",
        "\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                        2. Establish list of Sourcing Plants by material                                    =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "df = spark.sql(f\"\"\"\n",
        "    WITH Orders AS (\n",
        "        SELECT  Material_Number,\n",
        "                Base_Unit_of_Measure, \n",
        "                Plant, \n",
        "                CAST(Quantity_of_goods_received as FLOAT) as Quantity, \n",
        "                \"Order\" as OrderType\n",
        "        FROM    process_order \n",
        "        UNION ALL\n",
        "        SELECT  Material_Number,\n",
        "                Base_Unit_of_Measure, \n",
        "                Plant , \n",
        "                CAST(Purchase_Order_Quantity as FLOAT)  as Quantity, \n",
        "                \"Purchase\" as OrderType\n",
        "        FROM    purchase_order_line_item \n",
        "    )\n",
        "\n",
        "    SELECT      o.Material_Number, \n",
        "                o.Plant, \n",
        "                o.Quantity, \n",
        "                o.OrderType, \n",
        "                d.plant_name, \n",
        "                o.Base_Unit_of_Measure\n",
        "    FROM        Orders o \n",
        "    LEFT JOIN   dim_plant d \n",
        "        ON      o.Plant = d.plant_code\n",
        "\"\"\")\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import sum as spark_sum, round as spark_round, to_date, concat_ws, collect_list, sort_array\n",
        "\n",
        "df = df.withColumn(\"Quantity_numeric\", col(\"Quantity\").cast(\"double\"))\n",
        "\n",
        "# Data aggregation\n",
        "aggregated_df = df.groupBy(\"Material_Number\", \"Plant\", \"plant_name\", \"Base_Unit_of_Measure\").agg(spark_sum(\"Quantity_numeric\").alias(\"Total_Quantity\"))\n",
        "\n",
        "# Calculate total quantity for each Material_Number\n",
        "total_quantity_df = df.groupBy(\"Material_Number\").agg(spark_sum(\"Quantity_numeric\").alias(\"Total_Quantity_All_Plants\"))\n",
        "\n",
        "# Join data\n",
        "joined_df = aggregated_df.join(total_quantity_df, [\"Material_Number\"])\n",
        "\n",
        "# Calculate percentage share\n",
        "result_df = joined_df.withColumn(\n",
        "    \"Percentage\",\n",
        "    spark_round((col(\"Total_Quantity\") / col(\"Total_Quantity_All_Plants\")) * 100, 2)\n",
        ")\n",
        "\n",
        "# Creating a summary column\n",
        "result_df = result_df.withColumn(\n",
        "    \"Plant_Details\",\n",
        "    concat_ws(\": \",  concat_ws(\" - \", col(\"Plant\") , col(\"plant_name\"), col(\"Base_Unit_of_Measure\") ), spark_round(col(\"Total_Quantity\")).cast(IntegerType()), \n",
        "              concat_ws(\"\", lit(\" (\"), col(\"Percentage\"), lit(\"%)\")))\n",
        ")\n",
        "\n",
        "# Creating a summary column\n",
        "df = result_df.groupBy(\"Material_Number\").agg(\n",
        "    concat_ws(\"; \", sort_array(collect_list(col(\"Plant_Details\")))).alias(\"Sourcing_Plant\")\n",
        ")\n",
        "\n",
        "# Aggregation into the final formatSave result to target table\n",
        "df.createOrReplaceTempView(\"df_sourcing_plant_per_material\")\n",
        "\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                                     3. Establish Manufacturing Type                                        =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT      Material_Number,\n",
        "                MAX(IsComan) as IsComan,\n",
        "                MAX(IsCopack) as IsCopack,\n",
        "                MAX(IsInhouse) as IsInhouse\n",
        "    FROM (\n",
        "        SELECT      d.Material AS Material_Number, \n",
        "                    CAST(1 AS BOOLEAN) as IsComan,\n",
        "                    CAST(0 AS BOOLEAN) as IsCopack,\n",
        "                    CAST(0 AS BOOLEAN) as IsInhouse\n",
        "        FROM        df_material d\n",
        "        SEMI JOIN   purchase_order_line_item p\n",
        "            ON      p.Material_Number = d.Material\n",
        "            AND     p.Purchasing_Document_Type = 'NB'\n",
        "            AND     p.Item_category_in_purchasing_document IN (0)\n",
        "        WHERE       d.Material_Type = 'FERT'\n",
        "\n",
        "        UNION\n",
        "\n",
        "        SELECT      d.Material AS Material_Number,\n",
        "                    CAST(0 AS BOOLEAN) as IsComan,\n",
        "                    CAST(1 AS BOOLEAN) as IsCopack,\n",
        "                    CAST(0 AS BOOLEAN) as IsInhouse\n",
        "        FROM        df_material d\n",
        "        SEMI JOIN   purchase_order_line_item p\n",
        "            ON      p.Material_Number = d.Material\n",
        "            AND     p.Purchasing_Document_Type = 'NB'\n",
        "            AND     p.Item_category_in_purchasing_document IN (3)\n",
        "        WHERE       d.Material_Type = 'FERT'\n",
        "\n",
        "        UNION\n",
        "\n",
        "        SELECT  Material_Number,\n",
        "                CAST(0 AS BOOLEAN) as IsComan,\n",
        "                CAST(0 AS BOOLEAN) as IsCopack,\n",
        "                CAST(1 AS BOOLEAN) as IsInhouse\n",
        "        FROM    process_order\n",
        "    )\n",
        "    GROUP BY    Material_Number\n",
        "\"\"\").createOrReplaceTempView(\"df_manufacturing_type_per_material\")\n",
        "\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                                 4. Prepare HELIOS taxonomy input                                           =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT      DISTINCT\n",
        "                t.Material,\n",
        "                t.IT_CoPack_Group_ID,\n",
        "                cg.Description Copack_Group,\n",
        "                t.IT_Pack_Size_ID,\n",
        "                ps.Description Pack_Size,\n",
        "                t.IT_Tech_ID,\n",
        "                it.Description Tech,\n",
        "                t.IT_IntExt_Mfg_Group_ID,\n",
        "                int.Description IntExt_Mfg_Group,\n",
        "                t.IT_EC_Group_ID,\n",
        "                eg.Description EC_Group,\n",
        "                t.IT_Financial_Product_Segment_ID,\n",
        "                fps.Description AS Financial_Product_Segment\n",
        "    FROM        helios_item_taxonomy t\n",
        "    LEFT JOIN   helios_it_copack_group cg\n",
        "        ON      t.IT_CoPack_Group_ID = cg.IT_CoPack_Group_ID\n",
        "    LEFT JOIN   helios_it_pack_size ps \n",
        "        ON      t.IT_Pack_Size_ID = ps.IT_Pack_Size_ID\n",
        "    LEFT JOIN   helios_it_tech it \n",
        "        ON      t.IT_Tech_ID = it.IT_Tech_ID\n",
        "    LEFT JOIN   helios_it_intext_mfg_group int\n",
        "        ON      t.IT_IntExt_Mfg_Group_ID = int.IT_IntExt_Mfg_Group_ID\n",
        "    LEFT JOIN   helios_it_ec_group eg \n",
        "        ON      t.IT_EC_Group_ID = eg.IT_EC_Group_ID\n",
        "    LEFT JOIN   helios_it_financial_product_segment fps\n",
        "        ON      t.IT_Financial_Product_Segment_ID = fps.IT_Financial_Product_Segment_ID\n",
        "\"\"\").createOrReplaceTempView(\"df_helios_material_taxonomy\")\n",
        "\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                                     5. Prepare final output                                                =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT      m.Material,\n",
        "                m.Description,\n",
        "                m.Representative_Item,\n",
        "                regio_codes.Veritas_Spec_Number,\n",
        "                m.Material_Type,\n",
        "                mft.IsComan,\n",
        "                mft.IsCopack,\n",
        "                mft.IsInhouse,\n",
        "                sp.Sourcing_Plant,\n",
        "                mg.Material_Group_Long_Description,\n",
        "                m.Created_By,\n",
        "                m.Created_On,\n",
        "                m.Gross_Weight,\n",
        "                CASE    WHEN m.Weight_Unit='KG' THEN m.Net_Weight * 1000.00\n",
        "                        WHEN m.Weight_Unit='G'  THEN m.Net_Weight\n",
        "                        WHEN m.Weight_Unit='LB' THEN m.Net_Weight * 453.59237\n",
        "                        WHEN m.Weight_Unit='OZ' THEN m.Net_Weight * 28.3495231\n",
        "                        WHEN m.Weight_Unit='MG' THEN m.Net_Weight / 1000.00\n",
        "                        WHEN m.Weight_Unit='TO' THEN m.Net_Weight * 1000000.00\n",
        "                        ELSE m.Weight_Unit END                                          AS Net_Weight_Grams,\n",
        "                m.Net_Weight,\n",
        "                m.Weight_Unit,\n",
        "                m.X_Plant_Status,\n",
        "                CASE    WHEN m.X_Plant_Status=10 THEN 'Active'\n",
        "                        WHEN m.X_Plant_Status=40 THEN 'Development' \n",
        "                        WHEN m.X_Plant_Status=50 THEN 'Creation'\n",
        "                        WHEN m.X_Plant_Status=90 THEN 'Retired' END                     AS X_Plant_Status_Desc,\n",
        "                CASE    WHEN m.Weight_Unit IS NOT NULL THEN 1*1000000 / (\n",
        "                            CASE    WHEN m.Weight_Unit='KG' THEN m.Net_Weight * 1000.00\n",
        "                                    WHEN m.Weight_Unit='G'  THEN m.Net_Weight\n",
        "                                    WHEN m.Weight_Unit='LB' THEN m.Net_Weight * 453.59237\n",
        "                                    WHEN m.Weight_Unit='OZ' THEN m.Net_Weight * 28.3495231\n",
        "                                    WHEN m.Weight_Unit='MG' THEN m.Net_Weight / 1000.00\n",
        "                                    WHEN m.Weight_Unit='TO' THEN m.Net_Weight * 1000000.00\n",
        "                                    ELSE m.Weight_Unit END)\n",
        "                        ELSE m.Weight_Unit END                                          AS Quantity_of_1,\n",
        "                m.Length,\n",
        "                m.Width,\n",
        "                m.Height,\n",
        "                m.EAN,\n",
        "                m.EAN_Category,\n",
        "                m.Unit_of_Dimension,\n",
        "                m.Material_Group,\n",
        "                m.Base_Unit_of_Measure,\n",
        "                m.Document,\n",
        "                m.Total_shelf_life,\n",
        "                m.Min_Rem_Shelf_Life,\n",
        "                m.Merchandising_Unit,\n",
        "                m.Retail_Sales_Unit,\n",
        "                m.Traded_Unit,\n",
        "                m.Intermediate_Product_Component,\n",
        "                m.Semi_Finished_Product,\n",
        "                zzglobal.Animal_Life_Stage,\n",
        "                zzglobal.Animal_Breed,\n",
        "                zzraw.Animal_Parts,\n",
        "                zzglobal.Animal_Size,\n",
        "                zzglobal.Animal_Species,\n",
        "                zzglobal.Brand_Essence,\n",
        "                zzglobal.CoManufact_Packing_Activity,\n",
        "                zzglobal.CoPacking_Packing_Activity,\n",
        "                zzglobal.CoPacking_Product_Format,\n",
        "                zzglobal.CoPacking_Technology,\n",
        "                zzglobal.Compliant_for_Countries,\n",
        "                zzglobal.Cuisine,\n",
        "                zzglobal.Diet_Claims,\n",
        "                zzglobal.Display_Storage_Condition,\n",
        "                zzglobal.Lifestyle,\n",
        "                zzglobal.On_pack_Consumer_Offer,\n",
        "                zzglobal.On_pack_Consumer_Value,\n",
        "                zzglobal.On_pack_Trade_Offer,\n",
        "                zzglobal.Product_Pack_Size_Group,\n",
        "                zzglobal.Sustainability,\n",
        "                zzglobal.Traded_Unit_Configuration,\n",
        "                zzglobal.Traded_Unit_Format,\n",
        "                COALESCE(zzglobal.Business_Segment,         zzraw.Business_Segment)                                     AS Business_Segment,\n",
        "                COALESCE(zzglobal.Market_Segment,           zzraw.Market_Segment)                                       AS Market_Segment,\n",
        "                COALESCE(zzglobal.Brand_Flag,               zzpack.Brand_Flag,              zzraw.Brand_Flag)           AS Brand_Flag,\n",
        "                COALESCE(zzglobal.Brand_Sub_Flag,           zzpack.Brand_Sub_Flag,          zzraw.Brand_Sub_Flag)       AS Brand_Sub_Flag,\n",
        "                COALESCE(zzglobal.Supply_Segment,\t\t    zzpack.Product_Pack_Size)                                   AS Supply_Segment,\n",
        "                COALESCE(zzglobal.Ingredient_Variety,\t    zzraw.Ingredient_Variety)                                   AS Ingredient_Variety,\n",
        "                COALESCE(zzglobal.Functional_Variety,\t    zzraw.Functional_variety)                                   AS Functional_Variety,\n",
        "                COALESCE(zzglobal.Trade_Sector,\t\t\t    zzraw.Trade_Sector)                                         AS Trade_Sector,\n",
        "                COALESCE(zzglobal.Marketing_Concept,\t    zzraw.Marketing_Concept)                                    AS Marketing_Concept,\n",
        "                COALESCE(zzglobal.Multipack_Quantity,\t    zzraw.Multipack_Quantity)                                   AS Multipack_Quantity,\n",
        "                COALESCE(zzglobal.Occasion,\t\t\t\t    zzraw.Occasion)                                             AS Occasion,\n",
        "                COALESCE(zzglobal.Product_Category,\t\t    zzraw.Product_Category)                                     AS Product_Category,\n",
        "                COALESCE(zzglobal.Product_Type,\t\t\t    zzraw.Product_Type)                                         AS Product_Type,\n",
        "                COALESCE(zzglobal.Product_Pack_Size,\t    zzpack.Product_Pack_Size,       zzraw.Product_Pack_Size)    AS Product_Pack_Size,\n",
        "                COALESCE(zzglobal.Consumer_Pack_Type,\t    zzraw.Consumer_Pack_Type)                                   AS Consumer_Pack_Type,\n",
        "                COALESCE(zzglobal.Consumer_Pack_Format,\t    zzraw.Consumer_Pack_Format)                                 AS Consumer_Pack_Format,\n",
        "                COALESCE(zzglobal.FPPS_Minor_Pack,\t\t    zzraw.FPPS_Minor_Pack)                                      AS FPPS_Minor_Pack,\n",
        "                zzpack.US_Value_Chain_Segment,\n",
        "                zzudc.MW_Economic_Cell_EU,\n",
        "                zzudc.MW_Primary_Packaging,\n",
        "                zzudc.MW_Product_Form,\n",
        "                zzudc.MW_Secondary_Packaging,\n",
        "                zzpack.Pack_Family,\n",
        "                zzpack.Pack_Sub_Family,\n",
        "                zzraw.Physical_Condition,\n",
        "                zzraw.Raw_Family,\n",
        "                zzraw.Raw_Group,\n",
        "                zzraw.Raws_Sub_Family,\n",
        "                zzglobal.ID_Brand_Flag,\n",
        "                zzglobal.ID_Business_Segment,\n",
        "                zzglobal.ID_Market_Segment,\n",
        "                zzglobal.ID_Product_Category,\n",
        "                zzglobal.ID_Product_Type,\n",
        "                tax.IT_CoPack_Group_ID,\n",
        "                tax.Copack_Group,\n",
        "                tax.IT_Pack_Size_ID,\n",
        "                tax.Pack_Size,\n",
        "                tax.IT_Tech_ID,\n",
        "                tax.Tech,\n",
        "                tax.IT_IntExt_Mfg_Group_ID,\n",
        "                tax.IntExt_Mfg_Group,\n",
        "                tax.IT_EC_Group_ID,\n",
        "                tax.EC_Group,\n",
        "                tax.IT_Financial_Product_Segment_ID,\n",
        "                tax.Financial_Product_Segment,\n",
        "                CASE WHEN LOWER(TRIM(sc.SievoCategoryL1)) IN ('', 'null', 'na', 'n/a', 'blank/s' ) OR sc.SievoCategoryL1 IS NULL THEN 'Missing' ELSE sc.SievoCategoryL1 END as SievoCategoryL1,\n",
        "                CASE WHEN LOWER(TRIM(sc.SievoCategoryL2)) IN ('', 'null', 'na', 'n/a', 'blank/s' ) OR sc.SievoCategoryL2 IS NULL THEN 'Missing' ELSE sc.SievoCategoryL2 END as SievoCategoryL2,\n",
        "                CASE WHEN LOWER(TRIM(sc.SievoCategoryL3)) IN ('', 'null', 'na', 'n/a', 'blank/s' ) OR sc.SievoCategoryL3 IS NULL THEN 'Missing' ELSE sc.SievoCategoryL3 END as SievoCategoryL3,\n",
        "                CASE WHEN LOWER(TRIM(sc.SievoCategoryL4)) IN ('', 'null', 'na', 'n/a', 'blank/s' ) OR sc.SievoCategoryL4 IS NULL THEN 'Missing' ELSE sc.SievoCategoryL4 END as SievoCategoryL4,\n",
        "                CASE WHEN LOWER(TRIM(sc.SievoCategoryL5)) IN ('', 'null', 'na', 'n/a', 'blank/s' ) OR sc.SievoCategoryL5 IS NULL THEN 'Missing' ELSE sc.SievoCategoryL5 END as SievoCategoryL5,\n",
        "                CASE WHEN LOWER(TRIM(sc.SievoCategoryL6)) IN ('', 'null', 'na', 'n/a', 'blank/s' ) OR sc.SievoCategoryL6 IS NULL THEN 'Missing' ELSE sc.SievoCategoryL6 END as SievoCategoryL6,\n",
        "                CAST(NULL as STRING) as Box_Qty\n",
        "    FROM        df_material m\n",
        "    LEFT JOIN   mdg_classification_zzglobal01_decoded zzglobal\n",
        "        ON      m.Material = zzglobal.matnr_zzglobal\n",
        "    LEFT JOIN   mdg_classification_zzpack01_decoded zzpack\n",
        "        ON      m.Material = zzpack.matnr_zzpack\n",
        "    LEFT JOIN   mdg_classification_zzraw01_decoded zzraw\n",
        "        ON      m.Material = zzraw.matnr_zzraw\n",
        "    LEFT JOIN   mdg_classification_zzudc_decoded zzudc\n",
        "        ON      m.Material = zzudc.matnr_zzudc\n",
        "    LEFT JOIN   mdg_regional_codes regio_codes\n",
        "        ON      m.Material = regio_codes.Material\n",
        "    LEFT JOIN   df_helios_material_taxonomy tax\n",
        "        ON      m.Material = tax.Material\n",
        "    LEFT JOIN   mdg_material_group mg\n",
        "        ON      m.Material_Group = mg.Material_Group\n",
        "    LEFT JOIN   df_manufacturing_type_per_material mft\n",
        "        ON      mft.Material_Number = m.Material\n",
        "    LEFT JOIN   df_sourcing_plant_per_material sp \n",
        "        ON      m.Material = sp.Material_Number\n",
        "    LEFT JOIN   atlas_sievo_categories sc \n",
        "        ON      m.Material = sc.MaterialNumber\n",
        "\"\"\").createOrReplaceTempView(\"int_material\")\n",
        "\n",
        "df=spark.sql(\"\"\"\n",
        "    WITH MARS_SEGMENT_DIVISION AS (\n",
        "        SELECT  VALUE1 as Business_Segment,\n",
        "                VALUE2 as Brand_Flag,\n",
        "                RESULT1 as Mars_Segment,\n",
        "                RESULT2 as Mars_Division\n",
        "        FROM    v_odsrules\n",
        "        WHERE   Function = \"MDG_MARS_SEGMENT_AND_DIVISION\"\n",
        "            AND Criteria = \"MARS_SEGMENT_DIVISION\"\n",
        "            AND COALESCE(VALUE2,\"[Any/Other]\") <> \"[Any/Other]\"\n",
        "    ),\n",
        "    MARS_SEGMENT_DIVISION_NO_BRAND AS (\n",
        "        SELECT  VALUE1 as Business_Segment,\n",
        "                RESULT1 as Mars_Segment,\n",
        "                RESULT2 as Mars_Division\n",
        "        FROM    v_odsrules\n",
        "        WHERE   Function = \"MDG_MARS_SEGMENT_AND_DIVISION\"\n",
        "            AND Criteria = \"MARS_SEGMENT_DIVISION\"\n",
        "            AND COALESCE(VALUE2,\"[Any/Other]\") = \"[Any/Other]\"\n",
        "    ),\n",
        "    MARS_NET_WEIGHT_GRAMS AS (\n",
        "        SELECT  VALUE1 as Material,\n",
        "                RESULT1 as Net_Weight_override,\n",
        "                CAST(RESULT2 AS STRING) as RuleM1,\n",
        "                CAST(RESULT3 AS STRING) as override_flag\n",
        "        FROM    v_odsrules\n",
        "        WHERE   Function = \"MDG_MARS_NET_WEIGHT_GRAMS\"\n",
        "            AND Criteria = \"MARS_NET_WEIGHT_GRAMS\"\n",
        "    ),\n",
        "    MDG_MARKET_SEGMENT AS (\n",
        "        SELECT  DISTINCT\n",
        "                VALUE1 as Material,\n",
        "                RESULT1 as Market_Segment,\n",
        "                CAST(RESULT2 AS STRING) as RuleM2,\n",
        "                CAST(RESULT3 AS STRING) as override_flag\n",
        "        FROM    v_odsrules\n",
        "        WHERE   Function = \"MDG_MARKET_SEGMENT\"\n",
        "            AND Criteria = \"BY_MATERIAL\"\n",
        "    )\n",
        "\n",
        "    SELECT      m.*,\n",
        "                IF(w.RuleM1 IS NULL, m.Net_Weight_Grams, w.Net_Weight_override)       AS Net_Weight_Grams_Final,\n",
        "                IF(w.RuleM1 IS NULL, m.Net_Weight, w.Net_Weight_override)             AS Net_Weight_Final,\n",
        "                COALESCE(ms.Market_Segment, m.Market_Segment) AS Market_Segment_Final,\n",
        "                COALESCE(sd.Mars_Segment,sdb.Mars_Segment) as Mars_Segment,\n",
        "                COALESCE(sd.Mars_Division,sdb.Mars_Division) as Mars_Division,\n",
        "                COALESCE(ms.override_flag,w.override_flag) as override_flag,\n",
        "                CONCAT_WS(',',    \n",
        "                    IF(w.RuleM1 IS NULL, NULL,\n",
        "                        CONCAT(w.RuleM1, ':(Net_Weight = ', COALESCE(CAST( ROUND(m.Net_Weight, 2) AS STRING),'') ,'->', COALESCE(CAST( ROUND(w.Net_Weight_override, 2) AS STRING),''), ')')),\n",
        "                    IF(ms.RuleM2 IS NULL, NULL,\n",
        "                        CONCAT(ms.RuleM2, ':(Market_Segment = ', COALESCE(CAST(m.Market_Segment AS STRING),'') ,'->', COALESCE(CAST(ms.Market_Segment AS STRING),''), ')'))\n",
        "                ) AS override_rule\n",
        "    FROM        int_material m\n",
        "    LEFT JOIN   MDG_MARKET_SEGMENT ms \n",
        "        ON      ms.Material = m.Material\n",
        "    LEFT JOIN   MARS_SEGMENT_DIVISION sd \n",
        "        ON      sd.Business_Segment = m.Business_Segment\n",
        "        AND     sd.Brand_Flag = m.Brand_Flag\n",
        "    LEFT JOIN   MARS_SEGMENT_DIVISION_NO_BRAND sdb\n",
        "        ON      sdb.Business_Segment = m.Business_Segment\n",
        "    LEFT JOIN   MARS_NET_WEIGHT_GRAMS w \n",
        "        ON      w.Material = m.Material\n",
        "\"\"\").withColumns({\n",
        "        \"Net_Weight_Grams\" :            F.col(\"Net_Weight_Grams_Final\"),\n",
        "        \"Net_Weight\" :                  F.col(\"Net_Weight_Final\"),\n",
        "        \"Market_Segment\" :              F.col(\"Market_Segment_Final\"),\n",
        "        \"SourceSystemUnit\" :            F.lit(core_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\" :      F.col(\"Material\").cast(\"string\"),\n",
        "        \"_run_id\":                      F.lit(job_id),\n",
        "        \"_run_timestamp\" :              F.current_timestamp()\n",
        "    }) \\\n",
        "    .drop(\"Net_Weight_Grams_Final\", \"Net_Weight_Final\", \"Market_Segment_Final\")\n",
        "\n",
        "\n",
        "# Reorder columns\n",
        "priority_cols = ['SourceSystemUnit', 'src_agnostic_unique_ID']\n",
        "other_cols = [col for col in df.columns if col not in priority_cols]\n",
        "df = df.select(priority_cols + other_cols)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### dim_material <u>Source Agnostic</u> - UK Kind (Source System/Unit : 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"dim_material\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type - Append\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",     \"view\":\"ukkind_item\",           \"table\":\"ukkind_item\"},\n",
        "        {\"schema\":\"raw\",            \"view\":\"ukkind_salesdetail\",    \"table\":\"ukkind_salesdetail\"},\n",
        "        {\"schema\":\"output\",         \"view\":\"dim_material\",          \"table\":\"dim_material\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "print(f\"uk_kind_source_system_id: {uk_kind_source_system_id}\")\n",
        "\n",
        "uk_kind_dim_material_df = spark.sql(\"\"\"\n",
        "    SELECT DISTINCT\n",
        "        a.ItemNumber                                                AS Material,\n",
        "        CONCAT(a.FlavorDescription, ' - ', a.ItemDescription2)      AS Description,\n",
        "        'FERT'                                                      AS Material_Type,\n",
        "        NULL                                                        AS IsComan,\n",
        "        NULL                                                        AS IsCopack,\n",
        "        False                                                       AS IsInhouse,\n",
        "        CAST(0 AS Double)                                           AS Gross_Weight,\n",
        "        '0'                                                         AS Net_Weight,\n",
        "        'G'                                                         AS Weight_Unit,\n",
        "        '1'                                                         AS Quantity_of_1,\n",
        "        CAST(a.ItemUPC AS LONG)                                     AS EAN,\n",
        "        'EA'                                                        AS Base_Unit_of_Measure,\n",
        "        'X'                                                         AS Traded_Unit,\n",
        "        CONCAT('KIND', ' - ', a.Brand)                              AS Brand_Essence,\n",
        "        a.ItemConfiguration                                         AS Traded_Unit_Configuration,\n",
        "        'Chocolate'                                                 AS Business_Segment,                      \n",
        "        CONCAT('KIND', ' - ', a.Brand)                              AS Brand_Flag,\n",
        "        'Snacking'                                                  AS Mars_Segment,\n",
        "        'Health & Wellness'                                         AS Mars_Division,\n",
        "        '2'                                                         AS SourceSystemUnit,\n",
        "        CONCAT('2', ':', a.ItemNumber)                              AS src_agnostic_unique_ID\n",
        "    FROM        ukkind_item a\n",
        "    INNER JOIN  ukkind_salesdetail b                                                                    -- filter the data only to materials that appear in sales data\n",
        "        ON      a.ItemNumber                                        =  b.ItemNumber\n",
        "    WHERE       a.ProductType                                       IN ('Assembly', 'Product')          -- only take finished goods\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": F.lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp(),\n",
        "        \"SourceSystemUnit\": F.lit(uk_kind_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.concat(F.lit(uk_kind_source_system_id), F.lit(\":\"), F.col(\"Material\")).cast(\"string\")\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "# Cast all columns from uk_kind_dim_material_df to match the exact data types in dim_material\n",
        "# This ensures schema compatibility and prevents type mismatch errors during the append operation\n",
        "dim_material_df = spark.table(\"dim_material\")\n",
        "df = cast_dataframe_to_schema(uk_kind_dim_material_df, dim_material_df)\n",
        "\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### dim_material <u>Source Agnostic</u> - Fusion (Source System/Unit : 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"dim_material\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type - Append\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"raw\",            \"view\":\"fusion_dim_material\",       \"table\":\"fusion_dim_material\"},\n",
        "        {\"schema\":\"output\",         \"view\":\"dim_material\",              \"table\":\"dim_material\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "print(f\"fusion_source_system_id: {fusion_source_system_id}\")\n",
        "\n",
        "\n",
        "fusion_dim_material_df = spark.sql(\"\"\"\n",
        "    SELECT      material_number as Material ,\n",
        "                material_description as Description ,\n",
        "                material_internal_code as Representative_Item ,\n",
        "\n",
        "                -- as Veritas_Spec_Number ,\n",
        "\n",
        "                material_type as Material_Type ,\n",
        "                CAST(NULL as BOOLEAN) as IsComan ,\n",
        "                CAST(NULL as BOOLEAN) as IsCopack ,\n",
        "                False as IsInhouse ,\n",
        "                cross_plant as Sourcing_Plant ,\n",
        "                external_material_group as Material_Group_Long_Description ,\n",
        "\n",
        "                --as Created_By,\n",
        "                 \n",
        "                source_record_modify_date as Created_On,\n",
        "                gross_weight as Gross_Weight ,\n",
        "                CASE    WHEN weight_unit='KG' THEN net_weight * 1000.00\n",
        "                        WHEN weight_unit='G'  THEN net_weight\n",
        "                        WHEN weight_unit='LB' THEN net_weight * 453.59237\n",
        "                        WHEN weight_unit='OZ' THEN net_weight * 28.3495231\n",
        "                        WHEN weight_unit='MG' THEN net_weight / 1000.00\n",
        "                        WHEN weight_unit='TO' THEN net_weight * 1000000.00\n",
        "                        ELSE weight_unit END                                          AS Net_Weight_Grams,\n",
        "                net_weight as Net_Weight ,\n",
        "                weight_unit as Weight_Unit ,\n",
        "                plant_bulk_status_code as X_Plant_Status ,\n",
        "\n",
        "                -- as X_Plant_Status_Desc ,\n",
        "\n",
        "                1 as Quantity_of_1,\n",
        "                length as Length ,\n",
        "                width as Width ,\n",
        "                height as Height ,\n",
        "                ean as EAN ,\n",
        "                ean_category as EAN_Category,\n",
        "                unit_of_dimension as Unit_of_Dimension,\n",
        "                package_material_group as Material_Group,\n",
        "                unit_of_measure as Base_Unit_of_Measure,\n",
        "\n",
        "                -- as Document ,\n",
        "\n",
        "                total_shelf_life as Total_shelf_life ,\n",
        "                min_remng_shelf_life as Min_Rem_Shelf_Life ,\n",
        "                inner_pack_quantity as Merchandising_Unit ,\n",
        "\n",
        "                --as Retail_Sales_Unit ,\n",
        "\n",
        "                po_unit_of_measure as Traded_Unit ,\n",
        "\n",
        "                -- as Animal_Life_Stage ,\n",
        "                -- as Animal_Breed ,\n",
        "                -- as Animal_Parts ,\n",
        "                -- as Animal_Size ,\n",
        "                -- as Animal_Species ,\n",
        "\n",
        "                story as Brand_Essence ,\n",
        "\n",
        "                -- as CoManufact_Packing_Activity ,\n",
        "                -- as CoPacking_Packing_Activity ,\n",
        "                -- as CoPacking_Product_Format ,\n",
        "                -- as CoPacking_Technology ,\n",
        "                -- as Compliant_for_Countries ,\n",
        "                -- as Cuisine ,\n",
        "                -- as Diet_Claims ,\n",
        "                -- as Display_Storage_Condition ,\n",
        "                -- as Lifestyle ,\n",
        "                -- as On_pack_Consumer_Offer ,\n",
        "                -- as On_pack_Consumer_Value ,\n",
        "                -- as On_pack_Trade_Offer ,\n",
        "                -- as Product_Pack_Size_Group ,\n",
        "                -- as Sustainability ,\n",
        "                casepack_quantity as Traded_Unit_Configuration ,\n",
        "                -- as Traded_Unit_Format ,\n",
        "                division  as Business_Segment ,\n",
        "                -- as Market_Segment ,\n",
        "                -- as Brand_Flag ,\n",
        "                -- as Brand_Sub_Flag ,\n",
        "                -- as Supply_Segment ,\n",
        "                -- as Ingredient_Variety ,\n",
        "                -- as Functional_Variety ,\n",
        "                indust_sectr as Trade_Sector ,\n",
        "                -- as Marketing_Concept ,\n",
        "                -- as Multipack_Quantity ,\n",
        "                -- as Occasion ,\n",
        "                -- as Product_Category ,\n",
        "                -- as Product_Type ,\n",
        "                size as Product_Pack_Size ,\n",
        "                -- as Consumer_Pack_Type ,\n",
        "                -- as Consumer_Pack_Format ,\n",
        "                -- as FPPS_Minor_Pack ,\n",
        "                -- as US_Value_Chain_Segment ,\n",
        "                -- as MW_Economic_Cell_EU ,\n",
        "                -- as MW_Primary_Packaging ,\n",
        "                -- as MW_Product_Form ,\n",
        "                -- as MW_Secondary_Packaging ,\n",
        "                -- as Pack_Family ,\n",
        "                -- as Pack_Sub_Family ,\n",
        "                -- as Physical_Condition ,\n",
        "                -- as Raw_Family ,\n",
        "                basic_material as Raw_Group ,\n",
        "                -- as Raws_Sub_Family ,\n",
        "                -- as ID_Brand_Flag ,\n",
        "                -- as ID_Business_Segment ,\n",
        "                -- as ID_Market_Segment ,\n",
        "                -- as ID_Product_Category ,\n",
        "                -- as ID_Product_Type ,\n",
        "                -- as IT_CoPack_Group_ID ,\n",
        "                -- as Copack_Group ,\n",
        "                -- as IT_Pack_Size_ID ,\n",
        "                -- as Pack_Size ,\n",
        "                -- as IT_Tech_ID ,\n",
        "                -- as Tech ,\n",
        "                -- as IT_IntExt_Mfg_Group_ID ,\n",
        "                -- as IntExt_Mfg_Group ,\n",
        "                -- as IT_EC_Group_ID ,\n",
        "                -- as EC_Group ,\n",
        "                -- as IT_Financial_Product_Segment_ID ,\n",
        "                -- as Financial_Product_Segment ,\n",
        "\n",
        "                'N/A' as SievoCategoryL1 ,\n",
        "                'N/A' as SievoCategoryL2 ,\n",
        "\n",
        "                division_name as SievoCategoryL3 ,\n",
        "                department_name as SievoCategoryL4 ,\n",
        "                class_name as SievoCategoryL5 ,\n",
        "                sub_class_name as SievoCategoryL6 \n",
        "\n",
        "\n",
        "                -- as Box_Qty ,\n",
        "                -- as Mars_Segment ,\n",
        "                -- as Mars_Division ,\n",
        "                -- as override_flag ,\n",
        "                -- as override_rule ,\n",
        "                -- as SourceSystemUnit ,\n",
        "                -- as src_agnostic_unique_ID\n",
        "    FROM        fusion_dim_material\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": F.lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp(),\n",
        "        \"SourceSystemUnit\": F.lit(fusion_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.concat(F.lit(fusion_source_system_id), F.lit(\":\"), F.col(\"Material\")).cast(\"string\")\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "# Cast all columns from fusion_dim_material to match the exact data types in dim_material\n",
        "# This ensures schema compatibility and prevents type mismatch errors during the append operation\n",
        "dim_material_df = spark.table(\"dim_material\")\n",
        "df = cast_dataframe_to_schema(fusion_dim_material_df, dim_material_df)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_vendor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_vendor\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",         \"table\":\"vendor_text\",                  \"view\":\"vendor_text\",                   \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"normalized\",         \"table\":\"vendor_attributes\",            \"view\":\"vendor_attributes\",             \"project_name\":\"atlas\"},\n",
        "        \n",
        "        {\"schema\":\"raw\",                \"view\":\"fusion_dim_vendor\",     \"table\":\"fusion_dim_vendor\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "#   CORE\n",
        "core_vendor_df = spark.sql(\"\"\"\n",
        "    SELECT      a.Account_Number                                AS supplier_number,\n",
        "                a.Account_Name                                  AS supplier_name,\n",
        "                b.Country_Key                                   AS supplier_country,\n",
        "                b.Supplier_Account_Group                        AS supplier_account_group,\n",
        "                b.City                                          AS city,\n",
        "                b.VAT_Registration_Number                       AS vat_registration_number,\n",
        "                b.Central_Deletion_Flag_for_Master_Record       AS deletion_flag\n",
        "    FROM        vendor_text a\n",
        "    LEFT JOIN   vendor_attributes b\n",
        "        ON      a.Account_Number = b.Account_Number\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(core_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.col(\"supplier_number\").cast(\"string\")\n",
        "    })\n",
        " \n",
        "\n",
        "#   FUSION\n",
        "fusion_vendor_df = spark.sql(\"\"\"\n",
        "    SELECT      vendor_code                         AS supplier_number,\n",
        "                vendor_name                         AS supplier_name,\n",
        "                country_key                         AS supplier_country,\n",
        "                city                                AS city,\n",
        "                tax_number                          AS vat_registration_number\n",
        "    FROM        fusion_dim_vendor a\n",
        "\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(fusion_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.concat(F.lit(fusion_source_system_id), F.lit(\":\"), F.col(\"supplier_number\")).cast(\"string\")\n",
        "    })\n",
        " \n",
        "\n",
        "df = core_vendor_df.unionByName(fusion_vendor_df, allowMissingColumns=True).withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "# Reorder columns\n",
        "priority_cols = ['SourceSystemUnit', 'src_agnostic_unique_ID']\n",
        "other_cols = [col for col in df.columns if col not in priority_cols]\n",
        "df = df.select(priority_cols + other_cols)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_customer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\",     \"target_table\": \"dim_customer\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",     \"table\":\"customer_attributes\",          \"view\":\"customer_attributes\",       \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"raw\",            \"table\":\"ukkind_customer\",              \"view\":\"ukkind_customer\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "#   CORE\n",
        "df_cust_atlas = spark.sql(\"\"\"\n",
        "    SELECT  a.Customer_Number,\n",
        "            a.Name_1,\n",
        "            a.Address,\n",
        "            a.Central_Deletion_Flag_for_Master_Record,\n",
        "            a.City as City,\n",
        "            a.City_Code,\n",
        "            a.Country_Key,\n",
        "            a.Customer_Account_Group,\n",
        "            a.Date_on_which_the_Record_Was_Created,\n",
        "            a.District,\n",
        "            a.Name_2,\n",
        "            a.Postal_Code,\n",
        "            a.Region\n",
        "    FROM    customer_attributes a\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(core_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.col(\"Customer_Number\").cast(\"string\")\n",
        "    })\n",
        "\n",
        "#   UK KIND\n",
        "df_cust_ukkind = spark.sql(\"\"\"\n",
        "    SELECT  a.TempCustomerId                                AS Customer_Number,\n",
        "            a.Name                                          AS Name_1,\n",
        "            a.CustomerType                                  AS Customer_Account_Group\n",
        "    FROM    ukkind_customer a\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(uk_kind_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.concat(F.lit(uk_kind_source_system_id), F.lit(\":\"), F.col(\"Customer_Number\")).cast(\"string\")\n",
        "    })\n",
        "\n",
        "\n",
        "# Union data\n",
        "df = df_cust_atlas.unionByName(df_cust_ukkind, allowMissingColumns=True) \\\n",
        "                  .withColumns({\n",
        "                          \"_run_id\": lit(job_id),\n",
        "                          \"_run_timestamp\" :  F.current_timestamp()\n",
        "                      })\n",
        "\n",
        "# Reorder columns\n",
        "priority_cols = ['SourceSystemUnit', 'src_agnostic_unique_ID']\n",
        "other_cols = [col for col in df.columns if col not in priority_cols]\n",
        "df = df.select(priority_cols + other_cols)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_company_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_company_code\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",         \"table\":\"company_code_text\",            \"view\":\"company_code_text\",                     \"project_name\":\"atlas\"},\n",
        "        {\"table\":\"dbo.v_odsrules\",      \"view\":\"v_odsrules\",                    \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"schema\":\"raw\",                \"table\":\"fusion_fact_purchase_order_header\",            \"view\":\"fusion_fact_purchase_order_header\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "#   CORE\n",
        "df_company_atlas = spark.sql(\"\"\"\n",
        "    SELECT          a.*\n",
        "    FROM            company_code_text a\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(core_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.col(\"Company_Code\").cast(\"string\")\n",
        "    })\n",
        "\n",
        "#   UK KIND\n",
        "df_company_ukkind = spark.sql(\"\"\"\n",
        "    SELECT  value2                                      AS Company_Code,\n",
        "            value3                                      AS Company_Name,\n",
        "            value1                                      AS SourceSystemUnit,\n",
        "            CONCAT(value1, ':', value2)                 AS src_agnostic_unique_ID\n",
        "    FROM    v_odsrules\n",
        "    WHERE   function = 'SYSTEM_AGNOSTIC_SETUP'\n",
        "        AND criteria = 'COMPANY_CODE'\n",
        "\"\"\")\n",
        "\n",
        "#   FUSION\n",
        "df_company_fusion = spark.sql(\"\"\"\n",
        "    SELECT  DISTINCT\n",
        "            company_code                                AS Company_Code,\n",
        "            company_code                                AS Company_Name\n",
        "    FROM    fusion_fact_purchase_order_header\n",
        "\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(fusion_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.concat(F.lit(fusion_source_system_id), F.lit(\":\"), F.col(\"Company_Code\")).cast(\"string\")\n",
        "    })\n",
        "\n",
        "\n",
        "# Union data\n",
        "df = df_company_atlas.unionByName(df_company_ukkind, allowMissingColumns=True) \\\n",
        "                     .unionByName(df_company_fusion, allowMissingColumns=True) \\\n",
        "                     .withColumns({\n",
        "                             \"_run_id\": lit(job_id),\n",
        "                             \"_run_timestamp\" :  F.current_timestamp()\n",
        "                         })\n",
        "\n",
        "# Reorder columns\n",
        "priority_cols = ['SourceSystemUnit', 'src_agnostic_unique_ID']\n",
        "other_cols = [col for col in df.columns if col not in priority_cols]\n",
        "df = df.select(priority_cols + other_cols)\n",
        "\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_sales_organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_sales_organization\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",    \"table\":\"sales_organization\",        \"view\":\"sales_organization\",                    \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"normalized\",    \"table\":\"sales_organization_dict\",   \"view\":\"sales_organization_dict\",               \"project_name\":\"atlas\"},\n",
        "        {\"table\":\"dbo.v_odsrules\", \"view\":\"v_odsrules\",                 \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "#   CORE\n",
        "df_atlas = spark.sql(\"\"\"\n",
        "\n",
        "    SELECT      s.sales_organization,\n",
        "                d.description,\n",
        "                s.company_code,\n",
        "                s.country_code,\n",
        "                s.currency,\n",
        "                s.customer_number                             \n",
        "    FROM        sales_organization s\n",
        "    LEFT JOIN   sales_organization_dict d \n",
        "        ON      s.sales_organization = d.sales_organization\n",
        "\"\"\").withColumns({\n",
        "        \"SourceSystemUnit\": F.lit(core_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\": F.col(\"sales_organization\").cast(\"string\")\n",
        "    })\n",
        "\n",
        "\n",
        "#   UK KIND\n",
        "df_system_agnostic = spark.sql(\"\"\"\n",
        "    SELECT  value2                                      AS sales_organization,\n",
        "            value3                                      AS description,\n",
        "            value4                                      AS country_code,\n",
        "            value5                                      AS company_code,\n",
        "            value1                                      AS SourceSystemUnit,\n",
        "            CONCAT(value1, ':', value2)                 AS src_agnostic_unique_ID\n",
        "    FROM    v_odsrules\n",
        "    WHERE   function = 'SYSTEM_AGNOSTIC_SETUP'\n",
        "        AND criteria = 'SALES_ORGANIZATION'\n",
        "\"\"\")\n",
        "\n",
        "df_joined = df_atlas.unionByName(df_system_agnostic, allowMissingColumns=True).withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_calendar_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_calendar_day\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "            {\"schema\":\"raw\", \"table\": \"mars_calendar\", \"view\": \"mars_calendar\"}\n",
        "        ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT  DISTINCT\n",
        "            md5(GCALDAY)                                                AS id_calendar,\n",
        "            DATE_FORMAT(TO_DATE(GCALDAY, 'yyyyMMdd'), 'yyyy-MM-dd')     AS day,\n",
        "            CAST(GCALYEAR AS INT)                                       AS year,\n",
        "            GMARSPINY___T                                               AS period,\n",
        "            GMARSQINY___T                                               AS quarter,\n",
        "            GMARSWINP___T                                               AS week,\n",
        "            GMARSYP                                                     AS year_period,\n",
        "            GMARSYPW                                                    AS year_period_week\n",
        "    FROM    mars_calendar\n",
        "    WHERE   DATE_FORMAT(TO_DATE(GCALDAY, 'yyyyMMdd'), 'yyyy-MM-dd')     IS NOT NULL\n",
        "\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_calendar_period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_calendar_period\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "            {\"schema\":\"raw\", \"table\": \"mars_calendar\", \"view\": \"mars_calendar\"}\n",
        "        ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "                md5(CONCAT_WS(\"\",GCALYEAR,GMARSPINY___T)) as id_calendar,\n",
        "                cast(GCALYEAR as int) as year,\n",
        "                GMARSPINY___T as period,\n",
        "                GMARSQINY___T as quarter,\n",
        "                MIN(TO_DATE(GCALDAY, 'yyyyMMdd')) as period_first_day,\n",
        "                MAX(TO_DATE(GCALDAY, 'yyyyMMdd')) as period_last_day,\n",
        "                CONCAT_WS(\"\",GCALYEAR,GMARSPINY___T) as year_period\n",
        "    FROM        mars_calendar\n",
        "    GROUP BY    GCALYEAR,\n",
        "                GMARSPINY___T,\n",
        "                GMARSQINY___T,\n",
        "                GCALYEAR\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_calendar_quarter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"dim_calendar_quarter\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "            {\"schema\":\"raw\", \"table\": \"mars_calendar\", \"view\": \"mars_calendar\"}\n",
        "        ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "                md5(CONCAT_WS(\"\",GCALYEAR,GMARSQINY___T)) as id_calendar,\n",
        "                cast(GCALYEAR as int) as year,\n",
        "                GMARSQINY___T as quarter,\n",
        "                MIN(TO_DATE(GCALDAY, 'yyyyMMdd')) as quarter_first_day,\n",
        "                MAX(TO_DATE(GCALDAY, 'yyyyMMdd')) as quarter_last_day,\n",
        "                CONCAT_WS(\"\",GCALYEAR,GMARSQINY___T) as year_quarter\n",
        "    FROM        mars_calendar\n",
        "    GROUP BY    GCALYEAR,\n",
        "                GMARSQINY___T,\n",
        "                GCALYEAR\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\",  \"target_table\":\"dim_component\",     \"format\":\"delta\",   \"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\",        \"table\":\"dim_material\",     \"view\":\"dim_material\"},\n",
        "        {\"table\":\"dbo.cr579_loraxpackfamilytopackgroup\",        \"view\":\"cr579_loraxpackfamilytopackgroup\",  \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"table\":\"dbo.GlobalOptionsetMetadata\",                 \"view\":\"GlobalOptionsetMetadata\",           \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}        \n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "component_df = spark.sql(f\"\"\"\n",
        "    SELECT      Material,\n",
        "                Material_Type,\n",
        "                Base_Unit_of_Measure,\n",
        "                Brand_Flag,\n",
        "                Brand_Sub_Flag,\n",
        "                Created_On,\n",
        "                Description,\n",
        "                Net_Weight,\n",
        "                Net_Weight_Grams,\n",
        "                Pack_Family,\n",
        "                Pack_Sub_Family,\n",
        "                Quantity_of_1,\n",
        "                Weight_Unit,\n",
        "                X_Plant_Status,\n",
        "                X_Plant_Status_Desc,\n",
        "                Created_By,\n",
        "                Document,\n",
        "                CAST(EAN AS string) as EAN,\n",
        "                EAN_Category,\n",
        "                Material_Group,\n",
        "                Unit_of_Dimension,\n",
        "                Length,\n",
        "                Width,\n",
        "                Height,\n",
        "                Veritas_Spec_Number as Regional_Code_Number,\n",
        "                Raw_Family,\n",
        "                Raw_Group,\n",
        "                Raws_Sub_Family,\n",
        "                Length * Width as Area,\n",
        "                Length * Width * Height as Volume,\n",
        "                g_usage.LocalizedLabel as Usage,\n",
        "                g_packgroup.LocalizedLabel as Pack_Group,\n",
        "                g_main.LocalizedLabel as Main,\n",
        "                g_flexrigid.LocalizedLabel as Flex_Rigid,\n",
        "                g_disposalcodegeneric.LocalizedLabel as Disposal_Code_Generic,\n",
        "                '1'                                         AS SourceSystemUnit,\n",
        "                m.Material                                  AS src_agnostic_unique_ID\n",
        "    FROM        dim_material m\n",
        "    LEFT JOIN   cr579_loraxpackfamilytopackgroup l\n",
        "        ON      UPPER(m.Pack_Family)                        = UPPER(l.cr579_packfamilyname)\n",
        "        AND     UPPER(m.Pack_Sub_Family)                    = UPPER(l.cr579_packsubfamilyname)\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_flexrigid \n",
        "        ON      UPPER(l.cr579_flexrigid)                    = UPPER(g_flexrigid.Option) \n",
        "        AND     UPPER(g_flexrigid.OptionSetName)            = UPPER('cr579_flexrigid')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_main \n",
        "        ON      UPPER(l.cr579_main)                         = UPPER(g_main.Option)\n",
        "        AND     UPPER(g_main.OptionSetName)                 = UPPER('cr579_main')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_packgroup\n",
        "        ON      UPPER(l.cr579_packgroup)                    = UPPER(g_packgroup.Option) \n",
        "        AND     UPPER(g_packgroup.OptionSetName)            = UPPER('cr579_packgroup')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_usage\n",
        "        ON      UPPER(l.cr579_usage)                        = UPPER(g_usage.Option)\n",
        "        AND     UPPER(g_usage.OptionSetName)                = UPPER('cr579_usage')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_disposalcodegeneric\n",
        "        ON      UPPER(l.lk_disposalcodegeneric)             = UPPER(g_disposalcodegeneric.Option)\n",
        "        AND     UPPER(g_disposalcodegeneric.OptionSetName)  = UPPER('lk_disposalcodegeneric')\n",
        "    WHERE       Material_Type IN ('VERP', 'UNBW', 'ZQFC')\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, component_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_component_hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"dim_component_hybrid\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\", \"params\":{\"drop_table\":\"True\"}},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\",    \"table\":\"dim_component\",     \"view\":\"dim_component\"},\n",
        "        {\"view_name\":\"material_spec_finder\",        \"path\":\"environments/veripack/prod/vrp_process/material_spec_finder\",    \"container\":\"synapse\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\"},\n",
        "        {\"view_name\":\"shared_dim_pms_veripack\",     \"path\":\"environments/veripack/prod/vrp_process/shared_dim_pms_veripack\", \"container\":\"synapse\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\"},\n",
        "        {\"view_name\":\"packaging_layers\",            \"path\":\"environments/veripack/prod/vrp_output/packaging_layers\",         \"container\":\"synapse\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\"},\n",
        "\n",
        "        {\"table\":\"dbo.lk_componentoverride\",            \"view\":\"lk_componentoverride\",              \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"table\":\"dbo.GlobalOptionsetMetadata\",         \"view\":\"GlobalOptionsetMetadata\",           \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"table\":\"dbo.cr579_loraxpackfamilytopackgroup\",\"view\":\"cr579_loraxpackfamilytopackgroup\",  \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"table\":\"dbo.cr579_pacinnermaterial\",          \"view\":\"cr579_pacinnermaterial\",            \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                                     1. Core ODS process                                                    =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT      DISTINCT\n",
        "                p.Category,\n",
        "                p.SubCategory,\n",
        "                CASE WHEN p.TARE_WEIGHT_GRAMS = 0.0 THEN NULL ELSE p.TARE_WEIGHT_GRAMS END as TARE_WEIGHT_GRAMS,\n",
        "                p.Usage_final,\n",
        "                p.Packaging_Group,\n",
        "                p.Disposal_Code_Final,\n",
        "                p.Main_Final,\n",
        "                p.Height_value,\n",
        "                p.Width_value,\n",
        "                p.Depth_value,\n",
        "                p.BASISWEIGHTTOTAL                  AS basisweighttotal,\n",
        "                p.Bio_Based                         AS bio_based,\n",
        "                p.Branded                           AS branded,\n",
        "                p.BUSINESSUNITS                     AS businessunits,\n",
        "                p.Compostability_Certificate        AS compostability_certificate,\n",
        "                p.Consumer_ReUse                    AS consumer_reuse,\n",
        "                p.D4C_Category                      AS d4c_category,\n",
        "                p.D4C_Design                        AS d4c_design,\n",
        "                p.DIAMETER                          AS diameter,\n",
        "                p.DiameterUOM                       AS diameteruom,\n",
        "                p.DIRECTFOODCONTACT                 AS directfoodcontact,\n",
        "                p.ENVIRONMENTALPACKAGINGTYPE        AS environmentalpackagingtype,\n",
        "                p.EXTERNALVOLUMEMETRICWIDTHVALUE    AS external_volume,\n",
        "                p.Fillers_Increase_Density          AS fillers_increase_density,\n",
        "                p.GROUP                             AS group,\n",
        "                p.Is_Paper_Cert_Ver_Recy            AS is_paper_cert_ver_recy,\n",
        "                p.LabelWaterSol                     AS labelwatersol,\n",
        "                p.LACQUERVARNISH                    AS lacquervarnish,\n",
        "                p.LAMINATE                          AS laminate,\n",
        "                p.LAMINATIONSYSTEM                  AS laminationsystem,\n",
        "                p.MANUFACTURINGPROCESS              AS manufacturingprocess,\n",
        "                p.Material_Color                    AS material_color,\n",
        "                p.Material_Thickness                AS thickness,\n",
        "                p.MATERIALTHICKNESSAVERAGEMETRIC    AS thickness_avg,\n",
        "                p.Metalized                         AS metalized,\n",
        "                p.Mono_Multi                        AS mono_multi,\n",
        "                p.PACKAGINGITEMTYPE                 AS packagingitemtype,\n",
        "                p.PackReason_Change                 AS packreason_change,\n",
        "                p.PackSpec_Current_Owner            AS packspec_current_owner,\n",
        "                p.PackSpec_Originator               AS packspec_originator,\n",
        "                p.PLASTICMASTERBATCHCOLOURREF       AS plasticmasterbatchcolourref,\n",
        "                p.Problematic_Materials             AS problematic_materials,\n",
        "                p.Recycled_P                        AS percentrecyclable,\n",
        "                p.Reinforcement                     AS reinforcement,\n",
        "                p.SEALTYPE                          AS sealtype,\n",
        "                p.Sleeve_Design                     AS sleeve_design,\n",
        "                p.SPECNAME                          AS specname,\n",
        "                p.SPECNUMBER                        AS specnumber,\n",
        "                p.SPECSTATUS                        AS specstatus,\n",
        "                p.SURFACEAREAUNITMETRIC             AS surfaceareaunitmetric,\n",
        "                p.SurfaceUOM                        AS surfaceuom,\n",
        "                p.Sustainable_Source_Certificate    AS sustainable_source_certificate,\n",
        "                p.ThicknessAvgUOM                   AS thickness_avg_uom,\n",
        "                p.ThicknessUOM                      AS thickness_uom,\n",
        "                p.TOTALSHELFLIFE                    AS totalshelflife,\n",
        "                p.Type_of_Recycled_Content          AS type_of_recycled_content,\n",
        "                p.VolumeUOM                         AS external_volume_uom,\n",
        "                p.Water_Based_Adhesive              AS water_based_adhesive,\n",
        "                p.DimensionUOM,\n",
        "                p.Flex_Rigid,\n",
        "                p.Packaging_Group,\n",
        "                pl.Layer_Category_1                 AS layer_category_1, \n",
        "                pl.Layer_Category_2                 AS layer_category_2, \n",
        "                pl.Layer_Category_3                 AS layer_category_3, \n",
        "                pl.Layer_Category_4                 AS layer_category_4, \n",
        "                pl.Layer_Category_5                 AS layer_category_5, \n",
        "                pl.Layer_Category_6                 AS layer_category_6, \n",
        "                pl.Layer_Category_7                 AS layer_category_7, \n",
        "                pl.Layer_Category_8                 AS layer_category_8, \n",
        "                pl.Layer_Category_9                 AS layer_category_9, \n",
        "                pl.`FLEXIBLECOMP1%`                 AS percentflexiblecomp1, \n",
        "                pl.`FLEXIBLECOMP2%`                 AS percentflexiblecomp2, \n",
        "                pl.`FLEXIBLECOMP3%`                 AS percentflexiblecomp3, \n",
        "                pl.`FLEXIBLECOMP4%`                 AS percentflexiblecomp4, \n",
        "                pl.`FLEXIBLECOMP5%`                 AS percentflexiblecomp5, \n",
        "                pl.`FLEXIBLECOMP6%`                 AS percentflexiblecomp6, \n",
        "                pl.`FLEXIBLECOMP7%`                 AS percentflexiblecomp7, \n",
        "                pl.`FLEXIBLECOMP8%`                 AS percentflexiblecomp8, \n",
        "                pl.`FLEXIBLECOMP9%`                 AS percentflexiblecomp9,\n",
        "                pl.layer_indicator,\n",
        "                p.INNERLINERMATERIAL                AS innerlinermaterial          \n",
        "    FROM        shared_dim_pms_veripack p \n",
        "    LEFT JOIN   packaging_layers pl\n",
        "        ON      p.SPECNUMBER = pl.SPECNUMBER \n",
        "        AND     pl.Total_Percentage_Weight_Tracking = 'Equals 100'\n",
        "\"\"\").createOrReplaceTempView(\"shared_pms\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT      lk_componentname        AS material,\n",
        "                lk_componentspecname    AS specnumber,\n",
        "                lk_componentweightg     AS component_weight_g,\n",
        "                lk_usagename            AS component_usage, \n",
        "                lk_categoryname         AS component_category, \n",
        "                lk_disposalcodename     AS component_disposalcode,\n",
        "                lk_subcategoryname      AS component_subcategory,\n",
        "                cr579_packgroupname     AS component_pack_group,\n",
        "                ARRAY(\n",
        "                    IF(lk_componentspecname  IS NOT NULL,'R1',NULL),\n",
        "                    IF(lk_componentweightg   IS NOT NULL,'R2',NULL),\n",
        "                    IF(lk_usagename          IS NOT NULL,'R3',NULL),\n",
        "                    IF(lk_categoryname       IS NOT NULL,'R4',NULL),\n",
        "                    IF(lk_disposalcodename   IS NOT NULL,'R5',NULL),\n",
        "                    IF(lk_subcategoryname    IS NOT NULL,'R6',NULL),\n",
        "                    IF(cr579_packgroupname   IS NOT NULL,'R7',NULL)\n",
        "                ) AS rules_arr\n",
        "    FROM        lk_componentoverride\n",
        "    WHERE       statuscode = 1\n",
        "\"\"\").createOrReplaceTempView(\"component_override\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT      c.Material                          AS material,\n",
        "                c.Description                       AS component_description,\n",
        "                c.Material_Type                     AS material_type,\n",
        "                c.X_Plant_Status_Desc               AS material_status,\n",
        "                c.EAN                               AS component_ean_upc_code,\n",
        "                c.EAN_Category                      AS component_ean_category,\n",
        "                COALESCE(p.Category, m.Category, c.Pack_Family)                                                         AS component_category,\n",
        "                co.component_category                                                                                   AS component_category_override,\n",
        "                COALESCE(P.SubCategory, m.SubCategory, c.Pack_Sub_Family)                                               AS component_subcategory,\n",
        "                co.component_subcategory                                                                                AS component_subcategory_override,\n",
        "                COALESCE(p.TARE_WEIGHT_GRAMS, c.Net_Weight_Grams)                                                       AS component_weight_g,\n",
        "                co.component_weight_g                                                                                   AS component_weight_g_override,\n",
        "                COALESCE(p.Height_value, m.Height_value, c.Height)                                                      AS component_height,\n",
        "                COALESCE(p.Width_value, m.Width_value, c.Width)                                                         AS component_width,\n",
        "                COALESCE(p.Depth_value, m.Depth_value, c.Length)                                                        AS component_depth, \n",
        "                COALESCE(p.DimensionUOM,c.Unit_of_Dimension)                                                            AS component_dimension_uom, \n",
        "                CASE WHEN p.Depth_value * p.Width_value IS NULL THEN c.Area ELSE p.Depth_value * p.Width_value END      AS component_area,\n",
        "                CASE WHEN p.Depth_value * p.Width_value * p.Height_value IS NULL THEN c.Volume \n",
        "                    ELSE p.Depth_value * p.Width_value * p.Height_value END                                             AS component_volume,\n",
        "                COALESCE(p.Flex_Rigid,c.Flex_Rigid)                                                                     AS component_flex_rigid, \n",
        "                COALESCE(p.Usage_final,m.Usage_final,c.Usage)                                                           AS component_usage,\n",
        "                co.component_usage                                                                                      AS component_usage_override,\n",
        "                COALESCE(p.Packaging_Group,m.Packaging_Group,c.Pack_Group)                                              AS component_pack_group,\n",
        "                co.component_pack_group                                                                                 AS component_pack_group_override,\n",
        "                COALESCE(p.Disposal_Code_Final,m.Disposal_Code_Final,c.Disposal_Code_Generic )                          AS component_disposalcode,\n",
        "                co.component_disposalcode                                                                               AS component_disposalcode_override,\n",
        "                COALESCE(p.Main_Final,m.Main_Final,c.Main)                                                              AS component_is_main,\n",
        "                p.basisweighttotal,\n",
        "                p.bio_based,\n",
        "                p.branded,\n",
        "                p.businessunits,\n",
        "                p.compostability_certificate,\n",
        "                p.consumer_reuse,\n",
        "                p.d4c_category,\n",
        "                p.d4c_design,\n",
        "                p.diameter,\n",
        "                p.diameteruom,\n",
        "                p.directfoodcontact,\n",
        "                p.environmentalpackagingtype,\n",
        "                p.external_volume,\n",
        "                p.fillers_increase_density,\n",
        "                p.group,\n",
        "                p.is_paper_cert_ver_recy,\n",
        "                p.labelwatersol,\n",
        "                p.lacquervarnish,\n",
        "                p.laminate,\n",
        "                p.laminationsystem,\n",
        "                p.manufacturingprocess,\n",
        "                p.material_color,\n",
        "                p.thickness,\n",
        "                p.thickness_avg,\n",
        "                p.metalized,\n",
        "                p.mono_multi,\n",
        "                p.packagingitemtype,\n",
        "                p.packreason_change,\n",
        "                p.packspec_current_owner,\n",
        "                p.packspec_originator,\n",
        "                p.plasticmasterbatchcolourref,\n",
        "                p.problematic_materials,\n",
        "                p.percentrecyclable,\n",
        "                p.reinforcement,\n",
        "                p.sealtype,\n",
        "                p.sleeve_design,\n",
        "                p.specname,\n",
        "                COALESCE(co.specnumber, p.SPECNUMBER) as specnumber,\n",
        "                p.SPECNUMBER as Source_specnumber,\n",
        "                p.specstatus,\n",
        "                p.surfaceareaunitmetric,\n",
        "                p.surfaceuom,\n",
        "                p.sustainable_source_certificate,\n",
        "                p.thickness_avg_uom,\n",
        "                p.thickness_uom,\n",
        "                p.totalshelflife,\n",
        "                p.type_of_recycled_content,\n",
        "                p.external_volume_uom,\n",
        "                p.water_based_adhesive,\n",
        "                p.layer_category_1, \n",
        "                p.layer_category_2, \n",
        "                p.layer_category_3, \n",
        "                p.layer_category_4, \n",
        "                p.layer_category_5, \n",
        "                p.layer_category_6, \n",
        "                p.layer_category_7, \n",
        "                p.layer_category_8, \n",
        "                p.layer_category_9, \n",
        "                p.percentflexiblecomp1, \n",
        "                p.percentflexiblecomp2, \n",
        "                p.percentflexiblecomp3, \n",
        "                p.percentflexiblecomp4, \n",
        "                p.percentflexiblecomp5, \n",
        "                p.percentflexiblecomp6, \n",
        "                p.percentflexiblecomp7, \n",
        "                p.percentflexiblecomp8, \n",
        "                p.percentflexiblecomp9,\n",
        "                p.layer_indicator,\n",
        "                m.More_Than_1_Spec                  AS more_than_1_spec,\n",
        "                m.Matching_Method                   AS matching_method,\n",
        "                p.innerlinermaterial,\n",
        "                CASE WHEN co.material IS NOT NULL THEN 'X' ELSE CAST(NULL AS STRING) END AS override_flag,\n",
        "                co.rules_arr\n",
        "    FROM        dim_component c\n",
        "    LEFT JOIN   component_override co \n",
        "        ON      co.material = c.material\n",
        "    LEFT JOIN   material_spec_finder m \n",
        "        ON      c.Material = m.GRD_Code\n",
        "    LEFT JOIN   shared_pms p\n",
        "        ON      p.SPECNUMBER = COALESCE(co.specnumber,m.FINAL_SPECNUMBER,c.regional_code_number)  -- if override specnumber exists\n",
        "\"\"\").createOrReplaceTempView(\"base\")\n",
        "\n",
        "df=spark.sql(\"\"\"\n",
        "    WITH base_ranked AS (\n",
        "        SELECT\n",
        "            base.*,\n",
        "            ROW_NUMBER() OVER (\n",
        "                PARTITION BY base.material \n",
        "                ORDER BY base.component_weight_g DESC, specnumber DESC\n",
        "            ) AS rn,\n",
        "            SUM(base.component_weight_g) OVER (PARTITION BY base.material) AS total_weight\n",
        "        FROM base\n",
        "    ), \n",
        "    component AS (\n",
        "        SELECT  b.material,\n",
        "                b.component_description,\n",
        "                b.material_type,\n",
        "                b.material_status,\n",
        "                b.component_ean_upc_code,\n",
        "                b.component_ean_category,\n",
        "                COALESCE(b.component_category_override, b.component_category) AS component_category,\n",
        "                COALESCE(b.component_subcategory_override, b.component_subcategory) AS component_subcategory,\n",
        "                b.component_height,\n",
        "                b.component_width,\n",
        "                b.component_depth,\n",
        "                b.component_dimension_uom,\n",
        "                b.component_area,\n",
        "                b.component_volume,\n",
        "                b.component_flex_rigid,\n",
        "                COALESCE(b.component_usage_override, b.component_usage) as component_usage,\n",
        "                COALESCE(b.component_pack_group_override, b.component_pack_group) AS component_pack_group,\n",
        "                COALESCE(b.component_disposalcode_override, b.component_disposalcode) AS component_disposalcode,\n",
        "                b.component_is_main,\n",
        "                b.basisweighttotal,\n",
        "                b.bio_based,\n",
        "                b.branded,\n",
        "                b.businessunits,\n",
        "                b.compostability_certificate,\n",
        "                b.consumer_reuse,\n",
        "                b.d4c_category,\n",
        "                b.d4c_design,\n",
        "                b.diameter,\n",
        "                b.diameteruom,\n",
        "                b.directfoodcontact,\n",
        "                b.environmentalpackagingtype,\n",
        "                b.external_volume,\n",
        "                b.fillers_increase_density,\n",
        "                b.group,\n",
        "                b.is_paper_cert_ver_recy,\n",
        "                b.labelwatersol,\n",
        "                b.lacquervarnish,\n",
        "                b.laminate,\n",
        "                b.laminationsystem,\n",
        "                b.manufacturingprocess,\n",
        "                b.material_color,\n",
        "                b.thickness,\n",
        "                b.thickness_avg,\n",
        "                b.metalized,\n",
        "                b.mono_multi,\n",
        "                b.packagingitemtype,\n",
        "                b.packreason_change,\n",
        "                b.packspec_current_owner,\n",
        "                b.packspec_originator,\n",
        "                b.plasticmasterbatchcolourref,\n",
        "                b.problematic_materials,\n",
        "                b.percentrecyclable,\n",
        "                b.reinforcement,\n",
        "                b.sealtype,\n",
        "                b.sleeve_design,\n",
        "                b.specname,\n",
        "                b.specnumber,\n",
        "                b.specstatus,\n",
        "                b.surfaceareaunitmetric,\n",
        "                b.surfaceuom,\n",
        "                b.sustainable_source_certificate,\n",
        "                b.thickness_avg_uom,\n",
        "                b.thickness_uom,\n",
        "                b.totalshelflife,\n",
        "                b.type_of_recycled_content,\n",
        "                b.external_volume_uom,\n",
        "                b.water_based_adhesive,\n",
        "                b.layer_category_1, \n",
        "                b.layer_category_2, \n",
        "                b.layer_category_3, \n",
        "                b.layer_category_4, \n",
        "                b.layer_category_5, \n",
        "                b.layer_category_6, \n",
        "                b.layer_category_7, \n",
        "                b.layer_category_8, \n",
        "                b.layer_category_9, \n",
        "                b.percentflexiblecomp1, \n",
        "                b.percentflexiblecomp2, \n",
        "                b.percentflexiblecomp3, \n",
        "                b.percentflexiblecomp4, \n",
        "                b.percentflexiblecomp5, \n",
        "                b.percentflexiblecomp6, \n",
        "                b.percentflexiblecomp7, \n",
        "                b.percentflexiblecomp8, \n",
        "                b.percentflexiblecomp9,\n",
        "                b.layer_indicator,\n",
        "                b.more_than_1_spec,\n",
        "                b.matching_method,\n",
        "                b.innerlinermaterial,\n",
        "                CAST(ROUND(COALESCE(b.component_weight_g_override,b.total_weight) , 4) AS  DECIMAL(10, 4)) AS component_weight_g,\n",
        "                b.override_flag,\n",
        "                CONCAT_WS(',', \n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R1'), \n",
        "                        CONCAT('R1:(specnumber = ',             COALESCE(CAST(b.Source_specnumber AS STRING),''),       ' -> ', COALESCE(CAST(b.specnumber AS STRING),''), ')'), NULL),\n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R2'),\n",
        "                        CONCAT('R2:(component_weight_g = ',     COALESCE(CAST(ROUND(b.total_weight, 2) AS STRING),''),  ' -> ', COALESCE(CAST(ROUND(b.component_weight_g_override, 2) AS STRING),''), ')'), NULL),\n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R3'),\n",
        "                        CONCAT('R3:(component_usage = ',        COALESCE(CAST(b.component_usage AS STRING),''),         ' -> ', COALESCE(CAST(b.component_usage_override AS STRING),''), ')'), NULL),\n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R4'),\n",
        "                        CONCAT('R4:(component_category = ',     COALESCE(CAST(b.component_category AS STRING),''),      ' -> ', COALESCE(CAST(b.component_category_override AS STRING),''), ')'), NULL),\n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R5'),\n",
        "                        CONCAT('R5:(component_disposalcode = ', COALESCE(CAST(b.component_disposalcode AS STRING),''),  ' -> ', COALESCE(CAST(b.component_disposalcode_override AS STRING),''), ')'), NULL),\n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R6'),\n",
        "                        CONCAT('R6:(component_subcategory = ',  COALESCE(CAST(b.component_subcategory AS STRING),''),   ' -> ', COALESCE(CAST(b.component_subcategory_override AS STRING),''), ')'), NULL),\n",
        "                    IF(ARRAY_CONTAINS(b.rules_arr,'R7'),\n",
        "                        CONCAT('R7:(component_pack_group = ',   COALESCE(CAST(b.component_pack_group AS STRING),''),    ' -> ', COALESCE(CAST(b.component_pack_group_override AS STRING),''), ')'), NULL)\n",
        "                ) AS override_rule\n",
        "        FROM    base_ranked b\n",
        "        WHERE   b.rn = 1  -- choose material/specnumber by component_weight_g + highest specnumber if multiple specs with same weight\n",
        "    )\n",
        "\n",
        "    SELECT      cte.*,\n",
        "                COALESCE(cte.component_flex_rigid,g_flexrigid.LocalizedLabel)               AS component_flex_rigid_final,\n",
        "                COALESCE(cte.component_usage,g_usage.LocalizedLabel)                        AS component_usage_final,\n",
        "                COALESCE(cte.component_pack_group,g_packgroup.LocalizedLabel)               AS component_pack_group_final,\n",
        "                COALESCE(cte.component_disposalcode,g_disposalcodegeneric.LocalizedLabel)   AS component_disposalcode_final,\n",
        "                COALESCE(cte.component_is_main,g_main.LocalizedLabel)                       AS component_is_main_final,\n",
        "                CASE WHEN cte.layer_category_1 IS NOT NULL THEN cte.layer_category_1\n",
        "                        WHEN im.lk_categoryname IS NOT NULL\n",
        "                            AND cte.basisweighttotal IS NOT NULL THEN im.lk_categoryname \n",
        "                END                                                                         AS layer_category_1_final, \n",
        "                CASE    WHEN cte.layer_category_2 IS NOT NULL THEN cte.layer_category_2 \n",
        "                        WHEN im.lk_categoryname IS NOT NULL \n",
        "                            AND cte.basisweighttotal IS NOT NULL THEN \"Paper\" \n",
        "                END                                                                         AS layer_category_2_final,\n",
        "                CASE    WHEN cte.percentflexiblecomp1 IS NOT NULL THEN cte.percentflexiblecomp1 \n",
        "                        WHEN im.lk_categoryname IS NOT NULL \n",
        "                            AND cte.basisweighttotal IS NOT NULL THEN \n",
        "                                im.lk_weight/SUBSTRING(cte.basisweighttotal, 0,\n",
        "                                LENGTH(cte.basisweighttotal)-5)\n",
        "                END                                                                         AS percentflexiblecomp1_final,\n",
        "                CASE    WHEN cte.percentflexiblecomp2 IS NOT NULL THEN cte.percentflexiblecomp2 \n",
        "                        WHEN im.lk_categoryname IS NOT NULL \n",
        "                            AND cte.basisweighttotal IS NOT NULL THEN \n",
        "                                (SUBSTRING(cte.basisweighttotal, 0, \n",
        "                                LENGTH(cte.basisweighttotal)-5) - im.lk_weight) / \n",
        "                                SUBSTRING(cte.basisweighttotal, 0, \n",
        "                                LENGTH(cte.basisweighttotal)-5)\n",
        "                END                                                                         AS percentflexiblecomp2_final,\n",
        "                CASE    WHEN cte.layer_category_1 IS NOT NULL THEN cte.layer_indicator  \n",
        "                        WHEN im.lk_categoryname IS NOT NULL\n",
        "                            AND cte.basisweighttotal IS NOT NULL THEN \"Carton\"\n",
        "                        ELSE cte.layer_indicator\n",
        "                END                                                                         AS layer_indicator_final\n",
        "    FROM        component cte\n",
        "    LEFT JOIN   cr579_loraxpackfamilytopackgroup l\n",
        "        ON      UPPER(cte.component_category)               = UPPER(l.cr579_packfamilyname)\n",
        "        AND     UPPER(cte.component_subcategory)            = UPPER(l.cr579_packsubfamilyname)\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_flexrigid \n",
        "        ON      UPPER(l.cr579_flexrigid)                    = UPPER(g_flexrigid.Option) \n",
        "        AND     UPPER(g_flexrigid.OptionSetName)            = UPPER('cr579_flexrigid')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_main \n",
        "        ON      UPPER(l.cr579_main)                         = UPPER(g_main.Option)\n",
        "        AND     UPPER(g_main.OptionSetName)                 = UPPER('cr579_main')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_packgroup\n",
        "        ON      UPPER(l.cr579_packgroup)                    = UPPER(g_packgroup.Option) \n",
        "        AND     UPPER(g_packgroup.OptionSetName)            = UPPER('cr579_packgroup')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_usage\n",
        "        ON      UPPER(l.cr579_usage)                        = UPPER(g_usage.Option)\n",
        "        AND     UPPER(g_usage.OptionSetName)                = UPPER('cr579_usage')\n",
        "    LEFT JOIN   GlobalOptionsetMetadata g_disposalcodegeneric\n",
        "        ON      UPPER(l.lk_disposalcodegeneric)             = UPPER(g_disposalcodegeneric.Option)\n",
        "        AND     UPPER(g_disposalcodegeneric.OptionSetName)  = UPPER('lk_disposalcodegeneric')\n",
        "    LEFT JOIN   cr579_pacinnermaterial im\n",
        "        ON      TRIM(cte.innerlinermaterial) = TRIM(im.cr579_innerlinermaterial)\n",
        "        AND     im.lk_weight IS NOT NULL\n",
        "\"\"\").drop(\"layer_indicator\") \\\n",
        "    .withColumns({\n",
        "        \"component_flex_rigid\":     F.col(\"component_flex_rigid_final\"),\n",
        "        \"component_usage\":          F.col(\"component_usage_final\"),\n",
        "        \"component_pack_group\":     F.col(\"component_pack_group_final\"),\n",
        "        \"component_disposalcode\":   F.col(\"component_disposalcode_final\"),\n",
        "        \"component_is_main\":        F.col(\"component_is_main_final\"),\n",
        "        \"layer_category_1\":         F.col(\"layer_category_1_final\"),\n",
        "        \"layer_category_2\":         F.col(\"layer_category_2_final\"),\n",
        "        \"percentflexiblecomp1\":     F.col(\"percentflexiblecomp1_final\"),\n",
        "        \"percentflexiblecomp2\":     F.col(\"percentflexiblecomp2_final\"),\n",
        "        \"Source\":                   F.lit(\"ODS\"),\n",
        "        \"SourceSystemUnit\":         F.lit(core_source_system_id).cast(\"string\"),\n",
        "        \"src_agnostic_unique_ID\":   F.col(\"Material\"),\n",
        "        \"_run_id\":                  F.lit(job_id),\n",
        "        \"_run_timestamp\" :          F.current_timestamp(),\n",
        "        \"specname\":                 F.regexp_replace(F.regexp_replace(\"specname\", r'\\\\\\\\+', r'\\\\'), r'\\\\\"', '\"')\n",
        "    }) \\\n",
        "    .drop(\"component_flex_rigid_final\", \"component_usage_final\", \"component_pack_group_final\", \"component_disposalcode_final\", \"component_is_main_final\", \\\n",
        "        \"layer_category_1_final\", \"layer_category_2_final\", \"percentflexiblecomp1_final\", \"percentflexiblecomp2_final\") \\\n",
        "    .withColumnRenamed(\"layer_indicator_final\", \"layer_indicator\")\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_component_hybrid - <u>manual/source agnostic</u>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"dim_component_hybrid\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type - Append\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\",     \"table\":\"dim_component_hybrid\",                 \"view\":\"dim_component_hybrid\"},\n",
        "        {                       \"table\":\"dbo.lk_component\",                     \"view\":\"lk_component\",                      \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {                       \"table\":\"dbo.GlobalOptionsetMetadata\",          \"view\":\"GlobalOptionsetMetadata\",           \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {                       \"table\":\"dbo.cr579_loraxpackfamilytopackgroup\", \"view\":\"cr579_loraxpackfamilytopackgroup\",  \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\",    \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "from pyspark.sql import types as T, functions as F\n",
        "\n",
        "# Get data from Manual Components PowerApp table ...\n",
        "df = spark.sql(\"\"\"\n",
        "    SELECT      COALESCE(lk_localcomponentid, lk_id)                            AS material,\n",
        "                lk_description                                                  AS component_description,\n",
        "                'VERP'                                                          AS material_type,\n",
        "                'Active'                                                        AS material_status,\n",
        "                lk_categoryname                                                 AS component_category,\n",
        "                lk_subcategoryname                                              AS component_subcategory,\n",
        "                lk_weight                                                       AS component_weight_g,\n",
        "                lk_height                                                       AS component_height,\n",
        "                lk_width                                                        AS component_width,\n",
        "                lk_depth                                                        AS component_depth,\n",
        "                cr579_dimensionuom                                              AS component_dimension_uom,\n",
        "                lk_depth * lk_width                                             AS component_area,\n",
        "                lk_depth * lk_width * lk_height                                 AS component_volume,\n",
        "                COALESCE(lk_usagename, gm_usage.LocalizedLabel)                 AS component_usage,\n",
        "                COALESCE(lk_packgroupname, gm_pack_group.LocalizedLabel)        AS component_pack_group,\n",
        "                COALESCE(lk_disposalcodename, gm_disposalcode.LocalizedLabel)   AS component_disposalcode,\n",
        "                CASE WHEN lk_isrigid = 'True'   THEN 'Rigid'\n",
        "                     WHEN lk_isrigid = 'False'  THEN 'Flexible'\n",
        "                     ELSE gm_flex_rigid.LocalizedLabel\n",
        "                END                                                             AS component_flex_rigid,\n",
        "                gm_main.LocalizedLabel                                          AS component_is_main,\n",
        "                    CONCAT(CASE WHEN lk_sourcesystemunit = 1 THEN '' ELSE CONCAT(lk_sourcesystemunit,':') END, COALESCE(lk_localcomponentid, lk_id), '-001')\n",
        "                                                                                AS specnumber,\n",
        "                CAST(lk_sourcesystemunit AS STRING)                             AS SourceSystemUnit,\n",
        "                    CONCAT(CASE WHEN lk_sourcesystemunit = 1 THEN '' ELSE CONCAT(lk_sourcesystemunit,':') END, COALESCE(lk_localcomponentid, lk_id))\n",
        "                                                                                AS src_agnostic_unique_ID\n",
        "    FROM        lk_component comp\n",
        "    LEFT JOIN   cr579_loraxpackfamilytopackgroup l\n",
        "        ON      comp.lk_categoryname = l.cr579_packfamilyname\n",
        "        AND     comp.lk_subcategoryname = l.cr579_packsubfamilyname\n",
        "    LEFT JOIN   GlobalOptionsetMetadata gm_flex_rigid\n",
        "        ON      l.cr579_flexrigid = gm_flex_rigid.Option\n",
        "        AND     gm_flex_rigid.OptionSetName = 'cr579_flexrigid'\n",
        "    LEFT JOIN   GlobalOptionsetMetadata gm_pack_group\n",
        "        ON      l.cr579_packgroup = gm_pack_group.Option\n",
        "        AND     gm_pack_group.OptionSetName = 'cr579_packgroup'\n",
        "    LEFT JOIN   GlobalOptionsetMetadata gm_usage\n",
        "        ON      l.cr579_usage = gm_usage.Option\n",
        "        AND     gm_usage.OptionSetName = 'cr579_usage'\n",
        "    LEFT JOIN   GlobalOptionsetMetadata gm_disposalcode\n",
        "        ON      l.lk_disposalcodegeneric = gm_disposalcode.Option\n",
        "        AND     gm_disposalcode.OptionSetName = 'lk_disposalcodegeneric'\n",
        "    LEFT JOIN   GlobalOptionsetMetadata gm_main\n",
        "        ON      l.cr579_main = gm_main.Option\n",
        "        AND     gm_main.OptionSetName = 'cr579_main' \n",
        "\"\"\").createOrReplaceTempView(\"df_manual\")\n",
        "\n",
        "# Filter out any components already found in dim_component_hybrid (shouldn't be the case / possible anyway, but better safe than sorry), align table schema.\n",
        "df=spark.sql(\"\"\"\n",
        "    SELECT      man.*\n",
        "    FROM        df_manual man\n",
        "    ANTI JOIN   dim_component_hybrid ch\n",
        "        ON      ch.src_agnostic_unique_id = man.src_agnostic_unique_id\n",
        "\"\"\").withColumns({\n",
        "        \"component_area\":           F.col(\"component_area\").cast(T.DoubleType()),\n",
        "        \"component_depth\":          F.col(\"component_depth\").cast(T.StringType()),\n",
        "        \"component_width\":          F.col(\"component_width\").cast(T.StringType()),\n",
        "        \"component_height\":         F.col(\"component_height\").cast(T.StringType()),\n",
        "        \"component_weight_g\":       F.col(\"component_weight_g\").cast(T.DecimalType(10,4)),\n",
        "        \"component_volume\":         F.col(\"component_volume\").cast(T.DoubleType()),\n",
        "        \"component_dimension_uom\":  F.col(\"component_dimension_uom\").cast(T.StringType()),\n",
        "        \"Source\":                   F.lit(\"Manual\"),\n",
        "        \"_run_id\":                  F.lit(job_id),\n",
        "        \"_run_timestamp\" :          F.current_timestamp()\n",
        "    }).cache()\n",
        "\n",
        "print(\"Appending\", df.count(), \"manual components ...\")\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)\n",
        "df.unpersist();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## dim_component_hybrid - orphaned components referred to in BOM Versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"dim_component_hybrid\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type - Append\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\",     \"table\":\"dim_component_hybrid\",                         \"view\":\"dim_component_hybrid\"},\n",
        "        {\"schema\":\"output\",     \"table\":\"lorax_dim_bill_of_material_persist_detail\",    \"view\":\"lorax_dim_bill_of_material_persist_detail\", \"project_name\":\"lorax\", \"params\":{\"optional\":\"True\"}},\n",
        "        {\"schema\":\"output\",     \"table\":\"lorax_dim_component_hybrid\",                   \"view\":\"lorax_dim_component_hybrid\",                \"project_name\":\"lorax\", \"params\":{\"optional\":\"True\"}}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "from pyspark.sql import types as T, functions as F\n",
        "\n",
        "# Find components that are used in BOM Versions but no longer present in dim_component_hybrid (this can happen for Manual Components and Veritas 'unmatched spec' components)\n",
        "# Copy the data back over from lorax_output.lorax_dim_component_hybrid (which is a 'history' table so it has deleted records too).\n",
        "# The fact of copying them back in to dim_component_hybrid will then change the operation_type from D to U in lorax_output.lorax_dim_component_hybrid when the lorax_process/\n",
        "#    lorax_output update runs.\n",
        "\n",
        "# If either of the 2 lorax_output tables doesn't exist, skip (next pipeline run will then pick up again, as these tables should be there after a complete end-to-end run)\n",
        "chk_tbl_1 = delta_table_exists(\"lorax_dim_component_hybrid\")\n",
        "chk_tbl_2 = delta_table_exists(\"lorax_dim_bill_of_material_persist_detail\")\n",
        "\n",
        "if chk_tbl_1 and chk_tbl_2:\n",
        "\n",
        "    df = spark.sql(\"\"\"\n",
        "        SELECT      *\n",
        "        FROM        lorax_dim_component_hybrid lx_comp\n",
        "        SEMI JOIN   lorax_dim_bill_of_material_persist_detail bom\n",
        "            ON      lx_comp.src_agnostic_unique_id = bom.component\n",
        "        ANTI JOIN   dim_component_hybrid comp\n",
        "            ON      comp.src_agnostic_unique_id = lx_comp.src_agnostic_unique_id\n",
        "    \"\"\") \\\n",
        "        .withColumns({\n",
        "            \"Source\":           F.lit(\"BOM Version orphaned components\"),\n",
        "            \"_run_id\":          F.lit(job_id),\n",
        "            \"_run_timestamp\" :  F.current_timestamp()\n",
        "        }) \\\n",
        "        .drop(\"operation_type\", \"last_update_dt\") \\\n",
        "        .cache()\n",
        "\n",
        "    print(\"Appending\", df.count(), \"orphaned components (present in BOM Versions but no longer in dim_component_hybrid) ...\")\n",
        "\n",
        "    ## ------------------------------------------------------------------------------------\n",
        "    create_table(metadata_json, df)\n",
        "    df.unpersist();\n",
        "\n",
        "else:\n",
        "    print(\"Skipping orphaned components: required tables missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## fact_helios_sales_skuprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"fact_helios_sales_skuprint\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"raw\"     , \"table\": \"helios_sold_goods\"                      , \"view\": \"helios_sold_goods\"},\n",
        "        {\"schema\":\"raw\"     , \"table\": \"helios_unit_hierarchy\"                  , \"view\": \"helios_unit_hierarchy\"},\n",
        "        {\"schema\":\"raw\"     , \"table\": \"helios_unit\"                            , \"view\": \"helios_unit\"},\n",
        "        {\"schema\":\"output\"  , \"table\": \"dim_material\"                           , \"view\": \"dim_material\"},\n",
        "        {\"table\":\"dbo.v_odsrules\", \"view\":\"v_odsrules\", \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\", \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"table\":\"dbo.cr579_countries_to_subsegments\",\"view\":\"cr579_countries_to_subsegments\",  \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\", \"opertation_type\":\"serverless jdbc ls\"},\n",
        "        {\"table\":\"dbo.GlobalOptionsetMetadata\", \"view\":\"GlobalOptionsetMetadata\",   \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\", \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                              1. Prepare helios_unit_mapping_skuprint_unpivot                               =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "from pyspark.sql.functions import array, col, explode, lit, arrays_zip\n",
        "\n",
        "SubSegment_to_Division_df = spark.sql(\"\"\"\n",
        "    SELECT      Value1 as SubSegment_Code,\n",
        "                Value2 as SubSegment_Name,\n",
        "                Value3 as Mars_Division\n",
        "    FROM        v_odsrules\n",
        "    WHERE       Function = 'HELIOS_UNIT_MAPPING_SKUPRINT'\n",
        "    AND         Criteria = 'MARS_DIVISION'\n",
        "\"\"\")\n",
        "\n",
        "#Read table\n",
        "df = spark.table(\"cr579_countries_to_subsegments\")\n",
        "\n",
        "#Metadata table fo EntityName = \"cr579_countries_to_subsegments\"\n",
        "GlobalOptionsetMetadata_df = spark.sql(\"\"\"SELECT * FROM GlobalOptionsetMetadata WHERE EntityName = \"cr579_countries_to_subsegments\" \"\"\")\n",
        "\n",
        "# Get all SubSegment columns based on name\n",
        "columns_to_unpivot = [c for c in df.columns if c.lower().endswith(\"subsegment\") ]\n",
        "columns_to_unpivot = [c for c in columns_to_unpivot if c.lower().startswith(\"cr579\") ]\n",
        "\n",
        "print(f\"Columns to unpivot: {columns_to_unpivot}\")\n",
        "\n",
        "#unpivot helios_unit_mapping_skuprint by columns_to_unpivot\n",
        "df_unpivot = df.withColumn(\n",
        "    \"Unpivoted\",\n",
        "    explode(\n",
        "        arrays_zip(\n",
        "            array([lit(c) for c in columns_to_unpivot]).alias(\"SubSegment_Code\"),\n",
        "            array([col(c) for c in columns_to_unpivot]).alias(\"SubSegment_ID\")\n",
        "        )\n",
        "    )\n",
        ").select(\n",
        "    \"cr579_country\", \n",
        "    \"cr579_country_code\", \n",
        "    \"cr579_country_code_helios\",\n",
        "    col(\"Unpivoted.SubSegment_Code\").alias(\"SubSegment_Code\"),\n",
        "    col(\"Unpivoted.SubSegment_ID\").alias(\"SubSegment_ID\")\n",
        ").distinct()\n",
        "\n",
        "\n",
        "#Get SubSegment labels from GlobalOptionsetMetadata table\n",
        "df_unpivot = df_unpivot.alias('um').join(GlobalOptionsetMetadata_df.alias(\"g\"), (F.col(\"um.SubSegment_Code\") == F.col(\"g.OptionSetName\")) & (F.col(\"um.SubSegment_ID\") == F.col(\"g.Option\")), how = 'left')\\\n",
        "                    .selectExpr(\"um.*\",\"g.LocalizedLabel AS SubSegment\")\n",
        "\n",
        "#Join to SubSegment to Mars_Division mapping\n",
        "df_unpivot = df_unpivot.alias('um').join(SubSegment_to_Division_df.alias(\"s\"), F.col(\"um.SubSegment_Code\") == F.col(\"s.SubSegment_Code\"), how = 'left')\\\n",
        "                    .select(\"um.*\",\"s.Mars_Division\", \"s.SubSegment_Name\")\n",
        "\n",
        "df_unpivot.createOrReplaceTempView(\"helios_unit_mapping_skuprint_unpivot\")\n",
        "\n",
        "# ======================================================================================================================================================\n",
        "# =====================                                                 2. Prepare output                                          =====================\n",
        "# ======================================================================================================================================================\n",
        "\n",
        "case_statement_Segment_Final = get_packrule_by_columns(\"v_odsrules\", \"HELIOS_MARS_SEGMENT_AND_DIVISION\", \"FACT_SALES\", ['KEY1','KEY2'], ['VALUE1','VALUE2'], ['FN1', 'FN2'], 'RESULT1')\n",
        "print(case_statement_Segment_Final)\n",
        "\n",
        "case_statement_Segment_Division = get_packrule_by_columns(\"v_odsrules\", \"HELIOS_MARS_SEGMENT_AND_DIVISION\", \"FACT_SALES\", ['KEY1','KEY2'], ['VALUE1','VALUE2'], ['FN1', 'FN2'], 'RESULT2')\n",
        "print(case_statement_Segment_Division)\n",
        "\n",
        "join_statement_SubSegment_Division = get_packrule_by_columns(\"v_odsrules\", \"HELIOS_MARS_SEGMENT_AND_DIVISION\", \"FACT_SALES\", ['KEY1','KEY2'], ['VALUE1','VALUE2'], ['FN1', 'FN2'], 'RESULT3')\n",
        "print(join_statement_SubSegment_Division)\n",
        "\n",
        "### OE (Operation_Entity) Mapping\n",
        "case_statement_OE_Field_Level4 = get_packrule_by_columns(\"v_odsrules\", \"OE_FIELD_MAPPING\", \"BY_COLUMN\", ['KEY1'], ['VALUE1'], ['FN1'], 'RESULT1')\n",
        "case_statement_OE_Field_Level5 = get_packrule_by_columns(\"v_odsrules\", \"OE_FIELD_MAPPING\", \"BY_COLUMN\", ['KEY2'], ['VALUE1'], ['FN2'], 'RESULT1')\n",
        "case_statement_OE_Field_Level6 = get_packrule_by_columns(\"v_odsrules\", \"OE_FIELD_MAPPING\", \"BY_COLUMN\", ['KEY3'], ['VALUE1'], ['FN3'], 'RESULT1')\n",
        "case_statement_OE_Field_Level4_Unit = get_packrule_by_columns(\"v_odsrules\", \"OE_FIELD_MAPPING\", \"BY_COLUMN_NAME\", ['KEY4'], ['VALUE1'], ['FN4'], 'RESULT1')\n",
        "\n",
        "df = spark.sql(f\"\"\"\n",
        "    WITH helios_unit_table as (\n",
        "        SELECT      Unit_ID,\n",
        "                    Country_Code,\n",
        "                    Country_Name,\n",
        "                    Description\n",
        "        FROM        helios_unit\n",
        "    ),\n",
        "    hierarchy as (\n",
        "        SELECT      coalesce(h.Level12_Unit_ID,h.Level11_Unit_ID,h.Level10_Unit_ID,h.Level9_Unit_ID,h.Level8_Unit_ID,h.Level7_Unit_ID,h.Level6_Unit_ID,h.Level5_Unit_ID,h.Level4_Unit_ID,h.Level3_Unit_ID,h.Level2_Unit_ID,h.Level1_Unit_ID) as Unit_ID,\n",
        "                    CASE    WHEN h.Level12_Unit_ID IS NOT NULL THEN 12\n",
        "                            WHEN h.Level11_Unit_ID IS NOT NULL THEN 11\n",
        "                            WHEN h.Level10_Unit_ID IS NOT NULL THEN 10\n",
        "                            WHEN h.Level9_Unit_ID IS NOT NULL THEN 9\n",
        "                            WHEN h.Level8_Unit_ID IS NOT NULL THEN 8\n",
        "                            WHEN h.Level7_Unit_ID IS NOT NULL THEN 7\n",
        "                            WHEN h.Level6_Unit_ID IS NOT NULL THEN 6\n",
        "                            WHEN h.Level5_Unit_ID IS NOT NULL THEN 5\n",
        "                            WHEN h.Level4_Unit_ID IS NOT NULL THEN 4\n",
        "                            WHEN h.Level3_Unit_ID IS NOT NULL THEN 3\n",
        "                            WHEN h.Level2_Unit_ID IS NOT NULL THEN 2\n",
        "                            WHEN h.Level1_Unit_ID IS NOT NULL THEN 1\n",
        "                            ELSE 0 \n",
        "                    END as Unit_Level,\n",
        "                    h.Level12_Unit_ID,\n",
        "                    u12.Description as Level12_Description,\n",
        "                    concat(h.Level12_Unit_ID,'_', u12.Description) as Full_Level12_Description,\n",
        "                    h.Level11_Unit_ID,\n",
        "                    u11.Description as Level11_Description,\n",
        "                    concat(h.Level11_Unit_ID,'_', u11.Description) as Full_Level11_Description,\n",
        "                    h.Level10_Unit_ID,\n",
        "                    u10.Description as Level10_Description,\n",
        "                    concat(h.Level10_Unit_ID,'_', u10.Description) as Full_Level10_Description,\n",
        "                    h.Level9_Unit_ID,\n",
        "                    u9.Description as Level9_Description,\n",
        "                    concat(h.Level9_Unit_ID,'_', u9.Description) as Full_Level9_Description,\n",
        "                    h.Level8_Unit_ID,\n",
        "                    u8.Description as Level8_Description,\n",
        "                    concat(h.Level8_Unit_ID,'_', u8.Description) as Full_Level8_Description,\n",
        "                    h.Level7_Unit_ID,\n",
        "                    u7.Description as Level7_Description,\n",
        "                    concat(h.Level7_Unit_ID,'_', u7.Description) as Full_Level7_Description,\n",
        "                    h.Level6_Unit_ID,\n",
        "                    u6.Description as Level6_Description,\n",
        "                    concat(h.Level6_Unit_ID,'_', u6.Description) as Full_Level6_Description,\n",
        "                    h.Level5_Unit_ID,\n",
        "                    u5.Description as Level5_Description,\n",
        "                    concat(h.Level5_Unit_ID,'_', u5.Description) as Full_Level5_Description,\n",
        "                    h.Level4_Unit_ID,\n",
        "                    u4.Description as Level4_Description,\n",
        "                    concat(h.Level4_Unit_ID,'_', u4.Description) as Full_Level4_Description,\n",
        "                    h.Level3_Unit_ID,\n",
        "                    u3.Description as Level3_Description,\n",
        "                    concat(h.Level3_Unit_ID,'_', u3.Description) as Full_Level3_Description,\n",
        "                    h.Level2_Unit_ID,\n",
        "                    u2.Description as Level2_Description,\n",
        "                    concat(h.Level2_Unit_ID,'_', u2.Description) as Full_Level2_Description,\n",
        "                    h.Level1_Unit_ID,\n",
        "                    u1.Description as Level1_Description,\n",
        "                    concat(h.Level1_Unit_ID,'_', u1.Description) as Full_Level1_Description\n",
        "        FROM        helios_unit_hierarchy h\n",
        "        LEFT JOIN   helios_unit_table u1\n",
        "            ON      h.Level1_Unit_ID = u1.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u2\n",
        "            ON      h.Level2_Unit_ID = u2.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u3\n",
        "            ON      h.Level3_Unit_ID = u3.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u4\n",
        "            ON      h.Level4_Unit_ID = u4.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u5\n",
        "            ON      h.Level5_Unit_ID = u5.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u6\n",
        "            ON      h.Level6_Unit_ID = u6.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u7\n",
        "            ON      h.Level7_Unit_ID = u7.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u8\n",
        "            ON      h.Level8_Unit_ID = u8.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u9\n",
        "            ON      h.Level9_Unit_ID = u9.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u10\n",
        "            ON      h.Level10_Unit_ID = u10.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u11\n",
        "            ON      h.Level11_Unit_ID = u11.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u12\n",
        "            ON      h.Level12_Unit_ID = u12.Unit_ID\n",
        "        WHERE       h.Level1_Unit_ID = \"12000\" \n",
        "\n",
        "        UNION \n",
        "        \n",
        "        SELECT      Unit_ID,\n",
        "                    1 as Unit_Level,\n",
        "                    null as Level12_Unit_ID,\n",
        "                    null as Level12_Description,\n",
        "                    null as Full_Level12_Description,\n",
        "                    null as Level11_Unit_ID,\n",
        "                    null as Level11_Description,\n",
        "                    null as Full_Level11_Description,\n",
        "                    null as Level10_Unit_ID,\n",
        "                    null as Level10_Description,\n",
        "                    null as Full_Level10_Description,\n",
        "                    null as Level9_Unit_ID,\n",
        "                    null as Level9_Description,\n",
        "                    null as Full_Level9_Description,\n",
        "                    null as Level8_Unit_ID,\n",
        "                    null as Level8_Description,\n",
        "                    null as Full_Level8_Description,\n",
        "                    null as Level7_Unit_ID,\n",
        "                    null as Level7_Description,\n",
        "                    null as Full_Level7_Description,\n",
        "                    null as Level6_Unit_ID,\n",
        "                    null as Level6_Description,\n",
        "                    null as Full_Level6_Description,\n",
        "                    null as Level5_Unit_ID,\n",
        "                    null as Level5_Description,\n",
        "                    null as  Full_Level5_Description,\n",
        "                    null as Level4_Unit_ID,\n",
        "                    null as  Level4_Description,\n",
        "                    null as  Full_Level4_Description,\n",
        "                    null as Level3_Unit_ID,\n",
        "                    null as Level3_Description,\n",
        "                    null as Full_Level3_Description,\n",
        "                    null as Level2_Unit_ID,\n",
        "                    null as Level2_Description,\n",
        "                    null as Full_Level2_Description,\n",
        "                    Unit_ID as Level1_Unit_ID,\n",
        "                    h.Description as Level1_Description,\n",
        "                    concat(h.Unit_ID,'_', h.Description) as Full_Level1_Description\n",
        "        FROM        helios_unit h\n",
        "        WHERE       Unit_ID = \"12000\" \n",
        "    ), \n",
        "    sold_goods AS (\n",
        "        SELECT      sd.Common_Item_ID,\n",
        "                    sd.Unit_ID,\n",
        "                    sd.Unit,\n",
        "                    sd.Market_ID,\n",
        "                    sd.Market,\n",
        "                    sd.Level4_Unit,\n",
        "                    sd.Level5_Unit,\n",
        "                    sd.Level6_Unit,\n",
        "                    sd.Period,\n",
        "                    sd.Year,\n",
        "                    sum(sd.Tonnes) as Tonnes,\n",
        "                    max(NSV_Cat) as NSV_Cat,\n",
        "                    sum(sd.GSV) as GSV\n",
        "        FROM        helios_sold_goods sd\n",
        "        GROUP BY    sd.Common_Item_ID,\n",
        "                    sd.Unit_ID,\n",
        "                    sd.Unit,\n",
        "                    sd.Market_ID,\n",
        "                    sd.Market,\n",
        "                    sd.Level4_Unit,\n",
        "                    sd.Level5_Unit,\n",
        "                    sd.Level6_Unit,\n",
        "                    sd.Period,\n",
        "                    sd.Year\n",
        "    ),\n",
        "    main_query AS (\n",
        "        SELECT      sd.Common_Item_ID,\n",
        "                    sd.Unit_ID,\n",
        "                    sd.Unit,\n",
        "                    sd.Market_ID,\n",
        "                    sd.Market,\n",
        "                    u.Country_Code,\n",
        "                    u.Country_Name,\n",
        "                    sd.Level4_Unit,\n",
        "                    sd.Level5_Unit,\n",
        "                    sd.Level6_Unit,\n",
        "                    sd.Period,\n",
        "                    sd.Year,\n",
        "                    sd.Tonnes,\n",
        "                    sd.NSV_Cat,\n",
        "                    sd.GSV,\n",
        "                    CASE    WHEN sd.Tonnes > 0 THEN 'Positive Tonnes'\n",
        "                            WHEN sd.Tonnes < 0 THEN 'Negative Tonnes'\n",
        "                            WHEN sd.Tonnes = 0 THEN 'Zero Tonnes'\n",
        "                            ELSE ''\n",
        "                    END as Tonnes_Category,\n",
        "                    m.Mars_Segment,\n",
        "                    m.Mars_Division,\n",
        "                    h.Unit_Level,\n",
        "                    h.Level1_Unit_ID,\n",
        "                    h.Level1_Description,\n",
        "                    h.Full_Level1_Description,\n",
        "                    h.Level2_Unit_ID,\n",
        "                    h.Level2_Description,\n",
        "                    h.Full_Level2_Description,\n",
        "                    h.Level3_Unit_ID,\n",
        "                    h.Level3_Description,\n",
        "                    h.Full_Level3_Description,\n",
        "                    h.Level4_Unit_ID,\n",
        "                    h.Level4_Description,\n",
        "                    h.Full_Level4_Description,\n",
        "                    h.Level5_Unit_ID,\n",
        "                    h.Level5_Description,\n",
        "                    h.Full_Level5_Description,\n",
        "                    h.Level6_Unit_ID,\n",
        "                    h.Level6_Description,\n",
        "                    h.Full_Level6_Description,\n",
        "                    h.Level7_Unit_ID,\n",
        "                    h.Level7_Description,\n",
        "                    h.Full_Level7_Description,\n",
        "                    h.Level8_Unit_ID,\n",
        "                    h.Level8_Description,\n",
        "                    h.Full_Level8_Description,\n",
        "                    h.Level9_Unit_ID,\n",
        "                    h.Level9_Description,\n",
        "                    h.Full_Level9_Description,\n",
        "                    h.Level10_Unit_ID,\n",
        "                    h.Level10_Description,\n",
        "                    h.Full_Level10_Description,\n",
        "                    h.Level11_Unit_ID,\n",
        "                    h.Level11_Description,\n",
        "                    h.Full_Level11_Description,\n",
        "                    h.Level12_Unit_ID,\n",
        "                    h.Level12_Description,\n",
        "                    h.Full_Level12_Description\n",
        "        FROM        sold_goods sd\n",
        "        LEFT JOIN   hierarchy h\n",
        "            ON      sd.Unit_ID = h.Unit_ID\n",
        "        LEFT JOIN   helios_unit_table u\n",
        "            ON      sd.Unit_ID = u.Unit_ID\n",
        "        LEFT JOIN   dim_material m\n",
        "            ON      m.Material = sd.Common_Item_ID\n",
        "    ), \n",
        "    main_query_with_rules AS (\n",
        "        SELECT      Common_Item_ID,\n",
        "                    Unit_ID,\n",
        "                    Unit,\n",
        "                    Market_ID,\n",
        "                    Market,\n",
        "                    Country_Code,\n",
        "                    Country_Name,\n",
        "                    Level4_Unit,\n",
        "                    Level5_Unit,\n",
        "                    Level6_Unit,\n",
        "                    Period,\n",
        "                    Year,\n",
        "                    Tonnes,\n",
        "                    NSV_Cat,\n",
        "                    GSV,\n",
        "                    Tonnes_Category,\n",
        "                    coalesce({case_statement_Segment_Final},\"Unknown\") as Business_Segment,\n",
        "                    coalesce({case_statement_Segment_Division},\"Unknown\") as Business_Division,\n",
        "                    coalesce({join_statement_SubSegment_Division},\"Unknown\") as join_Business_Division,\n",
        "                    Mars_Segment,\n",
        "                    Mars_Division,\n",
        "                    Unit_Level,\n",
        "                    Level1_Unit_ID,\n",
        "                    Level1_Description,\n",
        "                    Full_Level1_Description,\n",
        "                    Level2_Unit_ID,\n",
        "                    Level2_Description,\n",
        "                    Full_Level2_Description,\n",
        "                    Level3_Unit_ID,\n",
        "                    Level3_Description,\n",
        "                    Full_Level3_Description,\n",
        "                    Level4_Unit_ID,\n",
        "                    Level4_Description,\n",
        "                    Full_Level4_Description,\n",
        "                    Level5_Unit_ID,\n",
        "                    Level5_Description,\n",
        "                    Full_Level5_Description,\n",
        "                    Level6_Unit_ID,\n",
        "                    Level6_Description,\n",
        "                    Full_Level6_Description,\n",
        "                    Level7_Unit_ID,\n",
        "                    Level7_Description,\n",
        "                    Full_Level7_Description,\n",
        "                    Level8_Unit_ID,\n",
        "                    Level8_Description,\n",
        "                    Full_Level8_Description,\n",
        "                    Level9_Unit_ID,\n",
        "                    Level9_Description,\n",
        "                    Full_Level9_Description,\n",
        "                    Level10_Unit_ID,\n",
        "                    Level10_Description,\n",
        "                    Full_Level10_Description,\n",
        "                    Level11_Unit_ID,\n",
        "                    Level11_Description,\n",
        "                    Full_Level11_Description,\n",
        "                    Level12_Unit_ID,\n",
        "                    Level12_Description,\n",
        "                    Full_Level12_Description,\n",
        "                    CASE {case_statement_OE_Field_Level4[5:-20]} {case_statement_OE_Field_Level5[5:-20]} {case_statement_OE_Field_Level6[5:-20]} {case_statement_OE_Field_Level4_Unit[5:-20]}  END as Operation_Entity\n",
        "        FROM        main_query\n",
        "    )\n",
        "    SELECT      q.Common_Item_ID,\n",
        "                q.Unit_ID,\n",
        "                q.Unit,\n",
        "                q.Market_ID,\n",
        "                q.Market,\n",
        "                q.Country_Code,\n",
        "                q.Country_Name,\n",
        "                q.Level4_Unit,\n",
        "                q.Level5_Unit,\n",
        "                q.Level6_Unit,\n",
        "                q.Period,\n",
        "                q.Year,\n",
        "                q.Tonnes,\n",
        "                q.NSV_Cat,\n",
        "                q.GSV,\n",
        "                q.Tonnes_Category,\n",
        "                q.Business_Segment,\n",
        "                q.Business_Division,\n",
        "                q.Mars_Segment,\n",
        "                q.Mars_Division,\n",
        "                um.SubSegment_Name,\n",
        "                um.SubSegment,\n",
        "                q.Unit_Level,\n",
        "                q.Level1_Unit_ID,\n",
        "                q.Level1_Description,\n",
        "                q.Full_Level1_Description,\n",
        "                q.Level2_Unit_ID,\n",
        "                q.Level2_Description,\n",
        "                q.Full_Level2_Description,\n",
        "                q.Level3_Unit_ID,\n",
        "                q.Level3_Description,\n",
        "                q.Full_Level3_Description,\n",
        "                q.Level4_Unit_ID,\n",
        "                q.Level4_Description,\n",
        "                q.Full_Level4_Description,\n",
        "                q.Level5_Unit_ID,\n",
        "                q.Level5_Description,\n",
        "                q.Full_Level5_Description,\n",
        "                q.Level6_Unit_ID,\n",
        "                q.Level6_Description,\n",
        "                q.Full_Level6_Description,\n",
        "                q.Level7_Unit_ID,\n",
        "                q.Level7_Description,\n",
        "                q.Full_Level7_Description,\n",
        "                q.Level8_Unit_ID,\n",
        "                q.Level8_Description,\n",
        "                q.Full_Level8_Description,\n",
        "                q.Level9_Unit_ID,\n",
        "                q.Level9_Description,\n",
        "                q.Full_Level9_Description,\n",
        "                q.Level10_Unit_ID,\n",
        "                q.Level10_Description,\n",
        "                q.Full_Level10_Description,\n",
        "                q.Level11_Unit_ID,\n",
        "                q.Level11_Description,\n",
        "                q.Full_Level11_Description,\n",
        "                q.Level12_Unit_ID,\n",
        "                q.Level12_Description,\n",
        "                q.Full_Level12_Description,\n",
        "                q.Operation_Entity\n",
        "    FROM        main_query_with_rules q\n",
        "    LEFT JOIN   helios_unit_mapping_skuprint_unpivot um\n",
        "        ON      q.Country_Code = um.cr579_country_code_helios\n",
        "        AND     um.Mars_Division = q.join_Business_Division\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<mark>**USAGE STATISTICS - FACT ACTIVITY**</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"usr_stats_fact_activity\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"raw\",        \"table\":\"user_stats_power_bi\",          \"view\":\"df_act_pbi\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"user_stats_pbi_user_list\",     \"view\":\"df_act_pbi_user_list\"},\n",
        "        {\"schema\":\"raw\",        \"table\":\"user_stats_ms_clarity\",        \"view\":\"df_act_web\"},\n",
        "        {\"schema\":\"process\",    \"table\":\"usr_stats_dim_user\",           \"view\":\"df_usr\"},\n",
        "        {\"view_name\":\"df_act_pbi_to_include\", \"path\":\"/USER_STATS/activity_to_include.csv\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}},            \n",
        "        {\"view_name\":\"df_act_gpt\", \"path\":\"/USER_STATS/PACKGPT/\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}},\n",
        "        {\"view_name\":\"df_act_pwa\", \"path\":\"/USER_STATS/POWERAPP_REPORTS/\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}},\n",
        "        {\"view_name\":\"df_wrk\", \"path\":\"/USER_STATS/workspaces.csv\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.functions import regexp_replace, trim, col\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# FACT TABLE\n",
        "# CONCATENATES DATA FROM MULTIPLE SOURCES:\n",
        "# 1.PBI ACTIVITY REPORT FROM PBI SUPPORT TEAM\n",
        "# 2.PBI USER LIST FROM PBI API\n",
        "# 3.PackGTP USAGE STATS FROM MANUAL EXTRACT\n",
        "# 4.MS CLARITY DATA FOR WEBAPP USAGE STATS\n",
        "# 5.POWERAPP USER ACTIVITY FROM MANUAL EXTRACT\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "df_act_p = spark.sql(\"\"\"\n",
        "    WITH df_main_pbi AS (\n",
        "        SELECT      a.ID as id,\n",
        "                    md5(lower(a.UserID)) as usr_id,\n",
        "                    a.UserID as usr_email,\n",
        "                    a.WorkspaceID as workspace_id,\n",
        "                    a.AppID as app_id,\n",
        "                    a.Activity as activity,\n",
        "                    a.DistributionMethod as distribution_method,\n",
        "                    a.DatasetName as dataset_name,\n",
        "                    a.ItemName as item_name,\n",
        "                    a.ReportName as report_name,\n",
        "                    DATE_FORMAT(TO_DATE(a.CreationDate, 'M/d/yyyy'), 'yyyy-MM-dd') as creation_date,\n",
        "                    a.CreationTime as creation_time,\n",
        "                    '1' as usr_cnt\n",
        "        FROM        df_act_pbi a\n",
        "        JOIN        df_act_pbi_to_include b\n",
        "            ON      a.activity = b.activity\n",
        "            AND     b.include = 'Y'\n",
        "    ),\n",
        "    df_usr_trunc AS (\n",
        "        SELECT      LEFT(usr_email,15) as usr_email_trunc,\n",
        "                    usr_email\n",
        "        FROM        df_usr\n",
        "    ),\n",
        "    df_main_gpt_0 AS (\n",
        "        SELECT      coalesce(b.usr_email,a.user_id) as usr_email,\n",
        "                    a.message_creation_timestamp as creation_timestamp,\n",
        "                    TO_DATE(LEFT(a.message_creation_timestamp,10), 'MM/dd/yyyy') as creation_date,\n",
        "                    DATE_FORMAT(a.message_creation_timestamp, 'hh:mm:ss') as creation_time,\n",
        "                    HOUR(DATE_FORMAT(a.message_creation_timestamp, 'HH:mm:ss')) as creation_hour,\n",
        "                    a.*\n",
        "        FROM        df_act_gpt a\n",
        "        LEFT JOIN   df_usr_trunc b\n",
        "            ON      a.user_id = b.usr_email_trunc\n",
        "        WHERE       a.user_id NOT IN ('test_user')\n",
        "            AND     a.message_type = ('session_start')\n",
        "        ORDER BY    a.message_creation_timestamp DESC\n",
        "    ),\n",
        "    df_main_gpt_1 AS (\n",
        "        SELECT      usr_email,\n",
        "                    creation_date,\n",
        "                    creation_hour,\n",
        "                    message_type,\n",
        "                    MIN(session_id) as session_id\n",
        "        FROM        df_main_gpt_0\n",
        "        GROUP BY    usr_email,\n",
        "                    creation_date,\n",
        "                    creation_hour,\n",
        "                    message_type\n",
        "        ORDER BY    creation_date DESC\n",
        "    ),\n",
        "    df_main_gpt_2 AS (\n",
        "        SELECT  session_id as ID,\n",
        "                md5(lower(usr_email)) as usr_id,\n",
        "                usr_email,\n",
        "                'packgptdevsqls.database.windows.net' as workspace_id,\n",
        "                'PackGTP' as app_id,\n",
        "                --message_type  as activity,\n",
        "                'SessionStart' as activity,\n",
        "                'GenAI' as distribution_method,\n",
        "                'PackGPT' as dataset_name,\n",
        "                'PackGPT' as item_name,            \n",
        "                'PackGPT' as report_name,\n",
        "                creation_date,\n",
        "                CONCAT(creation_hour,':00:00') as creation_time,\n",
        "                '1' as usr_cnt\n",
        "        FROM    df_main_gpt_1\n",
        "    ),\n",
        "    df_act_pbi_user_list_0 AS (\n",
        "        SELECT  hashkey as ID,\n",
        "                md5(lower(emailAddress)) as usr_id,\n",
        "                lower(emailAddress) as usr_email,\n",
        "                workspace_id,\n",
        "                app_id,\n",
        "                'NewUserEvent' as activity,\n",
        "                'Apps' as distribution_method,\n",
        "                'Not Applicable' as dataset_name,\n",
        "                'Not Applicable' as item_name,\n",
        "                'Not Applicable' as report_name,\n",
        "                DATE_FORMAT(timestamp, 'yyyy-MM-dd') as creation_date,\n",
        "                DATE_FORMAT(timestamp, 'hh:mm:ss') as creation_time,\n",
        "                '1' as usr_cnt\n",
        "        FROM    df_act_pbi_user_list\n",
        "        WHERE   userType IS NOT NULL\n",
        "            AND Deleted = False\n",
        "    ),\n",
        "    df_pwa_0 AS (\n",
        "        SELECT  DISTINCT     \n",
        "                `App name` AS app_name,\n",
        "                `'Environments'[environmentId]` as env_id,\n",
        "                `Unique users` as unique_users,\n",
        "                `tenantId` as tenant_id,\n",
        "                `App id` as app_id,\n",
        "                `subType` as sub_type,\n",
        "                `Platform` as platform,\n",
        "                `Time accessed` as time_accessed,\n",
        "                `tenantIdplusenvIdplusappId` as tenant_env_app_id\n",
        "        FROM    df_act_pwa\n",
        "    ),\n",
        "    df_pwa_1 AS (\n",
        "        SELECT  md5(concat(a.tenant_env_app_id,a.time_accessed)) as ID,\n",
        "                'Unknown' AS usr_id,\n",
        "                'Unknown' AS usr_email,\n",
        "                concat(a.env_id,'|',a.app_id) as workspace_id,\n",
        "                a.app_id,\n",
        "                'ViewReport' as activity,\n",
        "                'PowerApps' as distribution_method,\n",
        "                'Not Applicable' as dataset_name,\n",
        "                'Not Applicable' as item_name,\n",
        "                'Not Applicable' as report_name,\n",
        "                TO_DATE(replace(a.time_accessed,' 0:00',''), 'M/d/yyyy') as creation_date,\n",
        "                '00:00:00' as creation_time,\n",
        "                a.unique_users as usr_cnt\n",
        "        FROM    df_pwa_0 a\n",
        "        JOIN    df_wrk b\n",
        "            ON  concat(a.env_id,'|',a.app_id) = b.workspace_id\n",
        "    ),\n",
        "    df_web_0 AS (\n",
        "        SELECT  hashkey as ID,\n",
        "                md5(upper(Country)) as usr_id,\n",
        "                lower(Country) as usr_email,\n",
        "                concat(workspace_name,'|',workspace_tier) as workspace_id,\n",
        "                concat(workspace_name,'|',workspace_tier) as app_id,\n",
        "                'SessionCount' as activity,\n",
        "                'WebApps' as distribution_method,\n",
        "                'Not Applicable' as dataset_name,\n",
        "                'Not Applicable' as item_name,\n",
        "                'Not Applicable' as report_name,\n",
        "                DATE_FORMAT(IngestDate, 'yyyy-MM-dd') as creation_date,\n",
        "                DATE_FORMAT(IngestDate, 'hh:mm:ss') as creation_time,\n",
        "                totalSessionCount as usr_cnt\n",
        "        FROM    df_act_web\n",
        "    ),\n",
        "    df_final AS (\n",
        "                SELECT * FROM df_main_pbi WHERE creation_date >= '2023-01-01'   -- PBI\n",
        "        UNION   SELECT * FROM df_main_gpt_2                                     -- GPT\n",
        "        UNION   SELECT * FROM df_act_pbi_user_list_0                            -- PBI USER LIST\n",
        "        UNION   SELECT * FROM df_pwa_1                                          -- POWER APP\n",
        "        UNION   SELECT * FROM df_web_0                                          -- WEB APP\n",
        "    )\n",
        "\n",
        "    SELECT * FROM df_final\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_act_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<mark>**USAGE STATISTICS - DIM CALENDAR**</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"usr_stats_dim_calendar\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\",     \"table\":\"dim_calendar_day\",                                \"view\":\"df_cal\"},\n",
        "        {\"schema\":\"output\",     \"table\":\"usr_stats_fact_activity\",                         \"view\":\"df_act\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "df_cal_o = spark.sql(\"\"\"\n",
        "    WITH df_day_list AS (\n",
        "        SELECT      DISTINCT creation_date\n",
        "        FROM        df_act a\n",
        "        ORDER BY    creation_date     \n",
        "    ),\n",
        "    df_cal_final AS (\n",
        "        SELECT      a.* \n",
        "        FROM        df_cal a\n",
        "        JOIN        df_day_list b\n",
        "        ON          a.day = b.creation_date\n",
        "    )\n",
        "\n",
        "    SELECT      day,\n",
        "                year,\n",
        "                period,\n",
        "                quarter,\n",
        "                week\n",
        "    FROM        df_cal_final \n",
        "    ORDER BY    day\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_cal_o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<mark>**USAGE STATISTICS - DIM WORKSPACE**</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"usr_stats_dim_workspace\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"view_name\":\"df_wrk\", \"path\":\"/USER_STATS/workspaces.csv\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df_wrk_o = spark.sql(\"\"\"\n",
        "    SELECT      *\n",
        "    FROM        df_wrk\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_wrk_o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<mark>**USAGE STATISTICS - DIM USER**</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"usr_stats_dim_user\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"process\",        \"table\":\"usr_stats_dim_user\",               \"view\":\"df_usr\"},\n",
        "        {\"schema\":\"output\",         \"table\":\"usr_stats_fact_activity\",          \"view\":\"df_act\"},\n",
        "        {\"schema\":\"raw\",            \"table\":\"units_guide_units_details\",        \"view\":\"df_units\"},\n",
        "        {\"schema\":\"raw\",            \"table\":\"user_stats_ms_clarity\",            \"view\":\"df_act_web\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df_usr_o = spark.sql(\"\"\"\n",
        "    WITH df_email_act AS (\n",
        "        SELECT DISTINCT     a.usr_id,\n",
        "                            a.usr_email\n",
        "        FROM                df_act a \n",
        "        WHERE               distribution_method not in ('WebApps')    \n",
        "    ),\n",
        "    df_user_list AS (\n",
        "        SELECT              a.*\n",
        "        FROM                df_email_act a\n",
        "        WHERE NOT EXISTS    (   SELECT 1 \n",
        "                                FROM df_usr b \n",
        "                                WHERE a.usr_id = b.usr_id\n",
        "                            )\n",
        "    ),\n",
        "    df_user_missing AS (\n",
        "        SELECT          usr_id,\n",
        "                        usr_email,\n",
        "                        'Unknown' as usr_type,\n",
        "                        'WNL' as usr_site_code,\n",
        "                        'Unknown' as usr_segment,\n",
        "                        'Unknown' as usr_sub_segment,\n",
        "                        'Unknown' as usr_country,\n",
        "                        'Unknown' as usr_region,\n",
        "                        'Unknown' as usr_job_family_group,\n",
        "                        'Unknown' as usr_job_family,\n",
        "                        'Unknown' as usr_supervisory_org,\n",
        "                        'Unknown' as usr_level_2,\n",
        "                        'Unknown' as usr_level_3,\n",
        "                        'Unknown' as usr_level_4,\n",
        "                        'Unknown' as usr_level_5,\n",
        "                        'Unknown' as usr_level_6,\n",
        "                        'Unknown' as usr_level_7,\n",
        "                        'Unknown' as usr_level_8,\n",
        "                        'no_source' as src\n",
        "        FROM            df_user_list\n",
        "    ),\n",
        "    df_units_0 AS (\n",
        "        SELECT      UNIT_SITE_CODE as unit_site_code,\n",
        "                    UNIT_ID as unit_id,\n",
        "                    UNIT_NAME as unit_name,\n",
        "                    UNIT_LOCATION_ADDRESS_1 as unit_address,\n",
        "                    UNIT_LOCATION_ADDRESS_3 as unit_city,\n",
        "                    UNIT_LOCATION_ADDRESS_5 as unit_postal_code,\n",
        "                    UNIT_COUNTRY as unit_country,\n",
        "                    UNIT_BUSINESS_SEGMENT as unit_business_segment,\n",
        "                    UNIT_CONSOLIDATION_UNIT as unit_consolidation_unit,\n",
        "                    UNIT_REGION as unit_region,\n",
        "                    UNIT_LATITUDE as unit_latitude,\n",
        "                    UNIT_LONGITUDE as unit_longitude,\n",
        "                    UNIT_SITE_TYPE as unit_site_type\n",
        "        FROM        df_units\n",
        "    ),\n",
        "    df_web_0 AS (\n",
        "        SELECT      DISTINCT\n",
        "                    UNIT_COUNTRY,\n",
        "                    UNIT_SITE_CODE,\n",
        "                    ROW_NUMBER() OVER (PARTITION BY UNIT_COUNTRY ORDER BY UNIT_COUNTRY,UNIT_SITE_CODE) as rn\n",
        "        FROM        df_units_0\n",
        "        WHERE       UNIT_SITE_TYPE IN ('Office (O)','Factory and Office (F&O)')\n",
        "        ORDER BY    UNIT_COUNTRY,UNIT_SITE_CODE\n",
        "    ),\n",
        "    df_web_1 AS (\n",
        "        SELECT      UNIT_COUNTRY,\n",
        "                    UNIT_SITE_CODE\n",
        "        FROM        df_web_0\n",
        "        WHERE       rn=1\n",
        "    ),\n",
        "    df_web_2 AS (\n",
        "        SELECT      DISTINCT(UPPER(Country)) as Country\n",
        "        FROM        df_act_web\n",
        "        WHERE       metricName = 'Traffic'                \n",
        "    ),\n",
        "    df_web_3 AS (\n",
        "        SELECT      a.Country,\n",
        "                    b.*\n",
        "        FROM        df_web_2 a\n",
        "        LEFT JOIN   df_web_1 b\n",
        "            ON      b.UNIT_COUNTRY LIKE CONCAT('%',a.Country,'%')\n",
        "    ),\n",
        "    df_web_4 AS (\n",
        "        SELECT  *\n",
        "        FROM (\n",
        "            SELECT  *,\n",
        "                    ROW_NUMBER() OVER (PARTITION BY Country ORDER BY Country,UNIT_SITE_CODE) as rn\n",
        "            FROM    df_web_3\n",
        "        )\n",
        "        WHERE   rn=1\n",
        "    ),\n",
        "    df_web_5 AS (\n",
        "        SELECT  md5(upper(Country)) as usr_id,\n",
        "                upper(Country) as usr_email,\n",
        "                'WebAppUsers' as usr_type,\n",
        "                UNIT_SITE_CODE as usr_site_code,\n",
        "                'Unknown' as usr_segment,\n",
        "                'Unknown' as usr_sub_segment,\n",
        "                'Unknown' as usr_country,\n",
        "                'Unknown' as usr_region,\n",
        "                'Unknown' as usr_job_family_group,\n",
        "                'Unknown' as usr_job_family,\n",
        "                'Unknown' as usr_supervisory_org,\n",
        "                'Unknown' as usr_level_2,\n",
        "                'Unknown' as usr_level_3,\n",
        "                'Unknown' as usr_level_4,\n",
        "                'Unknown' as usr_level_5,\n",
        "                'Unknown' as usr_level_6,\n",
        "                'Unknown' as usr_level_7,\n",
        "                'Unknown' as usr_level_8,\n",
        "                'no_source' as src\n",
        "        FROM    df_web_4\n",
        "    )\n",
        "\n",
        "            SELECT a.* FROM df_usr a JOIN df_email_act b on a.usr_id = b.usr_id\n",
        "    UNION   SELECT * FROM df_user_missing\n",
        "    UNION   SELECT * FROM df_web_5\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_usr_o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<mark>**USAGE STATISTICS - DIM UNITS**</mark>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"output\", \"target_table\": \"usr_stats_dim_unit\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\",      \"table\":\"usr_stats_dim_user\",                                        \"view\":\"df_usr\"},\n",
        "        {\"schema\":\"raw\",         \"table\":\"units_guide_units_details\",                                 \"view\":\"df_units\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "\n",
        "df_units_o = spark.sql(\"\"\"\n",
        "WITH df_units_0 AS (\n",
        "    SELECT      UNIT_SITE_CODE as unit_site_code,\n",
        "                UNIT_ID as unit_id,\n",
        "                UNIT_NAME as unit_name,\n",
        "                UNIT_LOCATION_ADDRESS_1 as unit_address,\n",
        "                UNIT_LOCATION_ADDRESS_3 as unit_city,\n",
        "                UNIT_LOCATION_ADDRESS_5 as unit_postal_code,\n",
        "                UNIT_COUNTRY as unit_country,\n",
        "                UNIT_BUSINESS_SEGMENT as unit_business_segment,\n",
        "                UNIT_CONSOLIDATION_UNIT as unit_consolidation_unit,\n",
        "                UNIT_REGION as unit_region,\n",
        "                UNIT_LATITUDE as unit_latitude,\n",
        "                UNIT_LONGITUDE as unit_longitude,\n",
        "                UNIT_SITE_TYPE as unit_site_type\n",
        "    FROM        df_units\n",
        "),\n",
        "df_units_list AS (\n",
        "    SELECT DISTINCT     a.usr_site_code\n",
        "    FROM                df_usr a        \n",
        "),\n",
        "df_units_final AS (\n",
        "    SELECT      a.*\n",
        "    FROM        df_units_0 a\n",
        "    JOIN        df_units_list b\n",
        "    ON          a.unit_site_code = b.usr_site_code\n",
        ")\n",
        "\n",
        "SELECT * FROM df_units_final\n",
        "\"\"\").withColumns({\n",
        "        \"_run_id\": lit(job_id),\n",
        "        \"_run_timestamp\" :  F.current_timestamp()\n",
        "    })\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "create_table(metadata_json, df_units_o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Send Helios Automated Data Checks report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\n",
        "import requests \n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import date\n",
        "\n",
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\":\"output\", \"target_table\":\"helios_automated_data_checks\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"output\"     , \"table\": \"fact_helios_sales_skuprint\" , \"view\": \"fact_helios_sales_skuprint\"},\n",
        "        {\"schema\":\"output\"     , \"table\": \"dim_calendar_period\" ,        \"view\": \"dim_calendar_period\"},\n",
        "        {\"schema\":\"output\"     , \"table\": \"dim_calendar_day\" ,        \"view\": \"dim_calendar_day\"},\n",
        "        {\"table\":\"dbo.v_odsrules\", \"view\":\"v_odsrules\", \"linked_service_name\":\"DATAVERSE_SL_SQL_LS\", \"opertation_type\":\"serverless jdbc ls\"}\n",
        "    ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "### ------------------------------------------------------------------------------------\n",
        "\n",
        "ods_rules_df = spark.sql(\"\"\"\n",
        "        SELECT      key1,\n",
        "                    value1,\n",
        "                    criteria                 \n",
        "        FROM        v_odsrules\n",
        "        WHERE       Function = \"HELIOS_AUTOMATED_DATA_CHECKS\"\n",
        "\"\"\")\n",
        "\n",
        "distribution_list = ods_rules_df.filter(F.col(\"criteria\")==\"DISTRIBUTION_LIST\").select(\"value1\").collect()[0][0]\n",
        "print(f\"Report will be sent to {distribution_list}\")\n",
        "\n",
        "send_report_on_dates = ods_rules_df.filter(F.col(\"criteria\")==\"SEND_REPORTS_ON_SPECIFIC_DATES\").select(\"value1\").collect()[0][0]\n",
        "\n",
        "periods_to_report = ods_rules_df.filter(F.col(\"criteria\")==\"PERIODS_TO_REPORT\").select(\"value1\").collect()[0][0]\n",
        "print(f\"Number of periods to report {periods_to_report}\")\n",
        "periods_to_report = int(periods_to_report) + 1 \n",
        "\n",
        "percent_threshold = ods_rules_df.filter(F.col(\"criteria\")==\"PERCENT_THRESHOLD\").select(\"value1\").collect()[0][0]\n",
        "percent_threshold = float(str(percent_threshold).strip().replace(\"%\", \"\"))\n",
        "print(f\"Threshold: {percent_threshold} %\")\n",
        "\n",
        "email_header = ods_rules_df.filter(F.col(\"criteria\")==\"EMAIL_HEADER\").select(\"value1\").collect()[0][0]\n",
        "email_subject = ods_rules_df.filter(F.col(\"criteria\")==\"EMAIL_SUBJECT\").select(\"value1\").collect()[0][0]\n",
        "\n",
        "today = date.today().strftime('%Y-%m-%d')\n",
        "print(f\"\\nToday's date: {today}\")\n",
        "\n",
        "df = spark.table(\"dim_calendar_day\")\n",
        "\n",
        "# Convert 'day' column to Date type if it's not already\n",
        "df = df.withColumn(\"day\", F.col(\"day\").cast(DateType()))\n",
        "\n",
        "# Sort data by date\n",
        "df_sorted = df.orderBy(\"day\")\n",
        "\n",
        "# Create window functions for week and day numbering\n",
        "week_window = Window.partitionBy(\"year\", \"period\").orderBy(\"week\")\n",
        "day_window = Window.partitionBy(\"year\", \"period\", \"week\").orderBy(\"day\")\n",
        "\n",
        "# Add week and day numbers\n",
        "df_with_numbers = df_sorted.withColumn(\n",
        "    \"week_number_in_year_period\", \n",
        "    F.dense_rank().over(week_window)\n",
        ").withColumn(\n",
        "    \"day_number_in_week\", \n",
        "    F.row_number().over(day_window)\n",
        ")\n",
        "\n",
        "# Create final format: YYYYPXXWXDX\n",
        "df_final = df_with_numbers.withColumn(\n",
        "    \"calendar_code\",\n",
        "    F.concat(\n",
        "        F.col(\"year\").cast(\"string\"),\n",
        "        F.col(\"period\").cast(\"string\"), \n",
        "        F.lit(\"W\"),\n",
        "        F.col(\"week_number_in_year_period\").cast(\"string\"),\n",
        "        F.lit(\"D\"),\n",
        "        F.col(\"day_number_in_week\").cast(\"string\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_result = df_final.select(\n",
        "    \"day\", \"year\", \"period\", \"week\", \n",
        "    \"week_number_in_year_period\", \n",
        "    \"day_number_in_week\", \n",
        "    \"calendar_code\"\n",
        ")\n",
        "\n",
        "df_filtered = df_result.filter(F.col(\"day\")==F.lit(today)).select(\"calendar_code\")\n",
        "current_period_week_day = df_filtered.collect()[0][0]\n",
        "print(f\"Today is {current_period_week_day}\")\n",
        "print(send_report_on_dates)\n",
        "dates_list = [date.strip() for date in send_report_on_dates.split(',')]\n",
        "\n",
        "# Check if today is in the list of report dates\n",
        "if current_period_week_day in dates_list:\n",
        "    print(\"Send report\")\n",
        "\n",
        "    # Create DataFrame from your SQL query\n",
        "    df_dim_calendar_period = spark.sql(f\"\"\"\n",
        "        SELECT      DISTINCT \n",
        "                    year_Period \n",
        "        FROM        dim_calendar_period\n",
        "        WHERE       period_last_day <= '{today}'\n",
        "    \"\"\")\n",
        "\n",
        "    # Split year_Period by \"P\" to get year and period separately\n",
        "    df_split = df_dim_calendar_period.withColumn(\"split_parts\", F.split(F.col(\"year_Period\"), \"P\"))\n",
        "\n",
        "    df_with_parts = df_split.withColumn(\"year\", F.col(\"split_parts\")[0].cast(\"int\")) \\\n",
        "                        .withColumn(\"period\", F.col(\"split_parts\")[1].cast(\"int\"))\n",
        "\n",
        "    # Convert to integer format: year + period with proper padding\n",
        "    df_with_int_period = df_with_parts.withColumn(\n",
        "        \"period_int\",\n",
        "        (F.col(\"year\") * 100 + F.col(\"period\")).cast(\"int\")\n",
        "    ).select(\"year_Period\", \"year\", \"period\", \"period_int\") \\\n",
        "    .orderBy(F.col(\"period_int\").desc()) \\\n",
        "    .limit(periods_to_report)\n",
        "\n",
        "    # Sort by integer period and use lag to find previous\n",
        "    window_spec = Window.orderBy(F.col(\"period_int\"))\n",
        "\n",
        "    df_with_previous_int = df_with_int_period.withColumn(\n",
        "        \"previous_period_int\", \n",
        "        F.lag(F.col(\"period_int\"), 1).over(window_spec)\n",
        "    )\n",
        "\n",
        "    # Convert previous period back to YYYYPXX format\n",
        "    df_with_previous = df_with_previous_int.withColumn(\n",
        "        \"previous_year_Period\",\n",
        "        F.when(F.col(\"previous_period_int\").isNotNull(),\n",
        "            F.concat(\n",
        "                (F.col(\"previous_period_int\") / 100).cast(\"int\").cast(\"string\"),\n",
        "                F.lit(\"P\"),\n",
        "                F.lpad((F.col(\"previous_period_int\") % 100).cast(\"string\"), 2, \"0\")\n",
        "            )\n",
        "        ).otherwise(F.lit(None))\n",
        "    ).select(\"year_Period\", \"year\", \"period\", \"period_int\", \"previous_period_int\", \"previous_year_Period\")\n",
        "\n",
        "    # Verify that previous period exists in original dataset\n",
        "    df_dim_calendar_period_with_previous = df_with_previous.alias(\"current\") \\\n",
        "        .join(df_dim_calendar_period.alias(\"prev\"), \n",
        "            F.col(\"current.previous_year_Period\") == F.col(\"prev.year_Period\"), \n",
        "            \"left\") \\\n",
        "        .select(\n",
        "            F.col(\"current.year_Period\"),\n",
        "            F.col(\"current.period_int\"),\n",
        "            F.col(\"current.previous_year_Period\"),\n",
        "            F.col(\"current.previous_period_int\"),\n",
        "            F.when(F.col(\"prev.year_Period\").isNotNull(), F.lit(\"EXISTS\"))\n",
        "            .otherwise(F.lit(\"NOT_EXISTS\")).alias(\"previous_exists\")\n",
        "        ) \\\n",
        "        .filter(F.col(\"previous_exists\")==\"EXISTS\")\n",
        "\n",
        "    df_dim_calendar_period_with_previous.createOrReplaceTempView(\"dim_calendar_period_with_previous\")\n",
        "\n",
        "    # Get your data from SQL\n",
        "    df_sales = spark.sql(\"\"\"\n",
        "    WITH factdata AS \n",
        "    (\n",
        "        SELECT      Unit_ID, Unit, concat(Year,Period) as year_Period, COALESCE(SUM(Tonnes),0) as Tonnes \n",
        "        FROM        fact_helios_sales_skuprint\n",
        "        GROUP BY    Unit_ID, Unit, concat(Year,Period)\n",
        "    ) \n",
        "    SELECT      f.Unit_ID,\n",
        "                f.Unit,\n",
        "                f.year_Period,\n",
        "                f.Tonnes,\n",
        "                CASE WHEN coalesce(f.Tonnes,0) + coalesce(p.Tonnes,0) = 0 THEN 0 \n",
        "                        ELSE round(100.00 * (coalesce(f.Tonnes,0) - coalesce(p.Tonnes,0)) / ( ( coalesce(f.Tonnes,0) + coalesce(p.Tonnes,0)) / 2 ),2)\n",
        "                END as percent\n",
        "    FROM        factdata f \n",
        "    JOIN        dim_calendar_period_with_previous d \n",
        "    ON          f.year_Period = d.year_Period\n",
        "    LEFT JOIN   factdata p \n",
        "    ON          f.Unit_ID = p.Unit_ID\n",
        "    AND         p.year_Period  = d.previous_year_Period     \n",
        "    \"\"\")\n",
        "\n",
        "    # Get unique year_Period values for pivot\n",
        "    year_periods = df_sales.select(\"year_Period\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    year_periods = sorted(year_periods)\n",
        "\n",
        "    print(f\"Found {len(year_periods)} unique year periods\")\n",
        "\n",
        "    # Create pivot using standard PySpark method\n",
        "    pivot_data = df_sales.groupBy(\"Unit_ID\", \"Unit\") \\\n",
        "        .pivot(\"year_Period\", year_periods) \\\n",
        "        .agg(\n",
        "            F.first(F.col(\"Tonnes\")).alias(\"Tonnes\"),\n",
        "            F.first(F.col(\"percent\")).alias(\"percent\")\n",
        "        )\n",
        "\n",
        "    # Clean data - TYLKO RAZ!\n",
        "    all_columns = pivot_data.columns\n",
        "    pivot_data_clean = pivot_data\n",
        "    for column_name in all_columns:\n",
        "        pivot_data_clean = pivot_data_clean.withColumn(\n",
        "            column_name,\n",
        "            F.when(F.col(column_name) == \"undefined\", F.lit(\"\"))\n",
        "            .when(F.col(column_name).isNull(), F.lit(\"\"))\n",
        "            .otherwise(F.col(column_name))\n",
        "        )\n",
        "\n",
        "    pivot_data_clean = pivot_data_clean.orderBy(F.col(\"Unit\").asc())\n",
        "\n",
        "    # Get all columns from cleaned data\n",
        "    all_columns = pivot_data_clean.columns\n",
        "\n",
        "    # Separate basic columns from data columns\n",
        "    basic_columns = [\"Unit_ID\", \"Unit\"]\n",
        "    data_columns = [col for col in all_columns if col not in basic_columns]\n",
        "\n",
        "    # Extract unique periods from column names\n",
        "    periods = set()\n",
        "    for col in data_columns:\n",
        "        if \"_Tonnes\" in col:\n",
        "            period = col.replace(\"_Tonnes\", \"\")\n",
        "            periods.add(period)\n",
        "        elif \"_percent\" in col:\n",
        "            period = col.replace(\"_percent\", \"\")\n",
        "            periods.add(period)\n",
        "\n",
        "    # Sort periods chronologically\n",
        "    periods_sorted = sorted(list(periods))\n",
        "\n",
        "    # Create HTML content with email-safe CSS\n",
        "    html_content = \"\"\"\n",
        "    <style>\n",
        "    table {\n",
        "        border-collapse: collapse;\n",
        "        width: 100%;\n",
        "        font-family: Arial, sans-serif;\n",
        "        font-size: 12px;\n",
        "    }\n",
        "    th, td {\n",
        "        border: 1px solid #ddd;\n",
        "        padding: 6px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    th {\n",
        "        background-color: #f2f2f2;\n",
        "        font-weight: bold;\n",
        "        font-size: 14px;\n",
        "        color: black;\n",
        "    }\n",
        "    .unit-info {\n",
        "        text-align: left;\n",
        "        font-weight: normal;\n",
        "        color: black;\n",
        "    }\n",
        "    .unit-id-col {\n",
        "        width: 80px;\n",
        "    }\n",
        "    .unit-name-col {\n",
        "        width: 200px;\n",
        "        min-width: 200px;\n",
        "    }\n",
        "\n",
        "    tr td {\n",
        "        background-color: #ffffff;\n",
        "        color: black;\n",
        "    }\n",
        "\n",
        "    tr.even-row td {\n",
        "        background-color: #f5f5f5;\n",
        "        color: black;\n",
        "    }\n",
        "\n",
        "    .red-cell {\n",
        "        background-color: #ffcccc !important;\n",
        "        color: #cc0000 !important;\n",
        "        font-weight: bold !important;\n",
        "    }\n",
        "    </style>\n",
        "    <table>\n",
        "    <tr>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add table headers\n",
        "    html_content += '<th class=\"unit-info unit-id-col\">Unit_ID</th>'\n",
        "    html_content += '<th class=\"unit-info unit-name-col\">Unit</th>'\n",
        "\n",
        "    for i, period in enumerate(periods_sorted):\n",
        "        html_content += f'<th class=\"tonnes-col\">{period}<br>Tonnes</th>'\n",
        "        html_content += f'<th class=\"percent-col\">{period}<br>%</th>'\n",
        "\n",
        "    html_content += \"</tr>\"\n",
        "\n",
        "    # Add data rows\n",
        "    row_number = 0\n",
        "    styled_cells_count = 0\n",
        "\n",
        "    for row in pivot_data_clean.collect():\n",
        "        row_number += 1\n",
        "        row_class = \"odd-row\" if row_number % 2 == 1 else \"even-row\"\n",
        "        html_content += f'<tr class=\"{row_class}\">'\n",
        "        \n",
        "        html_content += f'<td class=\"unit-info unit-id-col\">{row[\"Unit_ID\"] if row[\"Unit_ID\"] is not None else \"\"}</td>'\n",
        "        html_content += f'<td class=\"unit-info unit-name-col\">{row[\"Unit\"] if row[\"Unit\"] is not None else \"\"}</td>'\n",
        "        \n",
        "        for i, period in enumerate(periods_sorted):\n",
        "            # Tonnes column\n",
        "            tonnes_col = f\"{period}_Tonnes\"\n",
        "            tonnes_value = row[tonnes_col] if tonnes_col in row and row[tonnes_col] is not None else \"\"\n",
        "            \n",
        "            if tonnes_value != \"\":\n",
        "                try:\n",
        "                    formatted_tonnes = f\"{float(tonnes_value):,.2f}\" if float(tonnes_value) != 0 else \"0.00\"\n",
        "                except (ValueError, TypeError):\n",
        "                    formatted_tonnes = str(tonnes_value)\n",
        "            else:\n",
        "                formatted_tonnes = \"\"\n",
        "            \n",
        "            html_content += f'<td class=\"tonnes-col\">{formatted_tonnes}</td>'\n",
        "\n",
        "            # Percent column\n",
        "            percent_col = f\"{period}_percent\"\n",
        "            percent_value = row[percent_col] if percent_col in row and row[percent_col] is not None else \"\"\n",
        "            \n",
        "            # Format percentage value\n",
        "            if percent_value != \"\":\n",
        "                try:\n",
        "                    formatted_percent = f\"{float(percent_value):.2f}%\" if float(percent_value) != 0 else \"0.00%\"\n",
        "                    numeric_value = float(percent_value)\n",
        "                    \n",
        "                    # Apply red styling if value exceeds threshold\n",
        "                    if numeric_value > percent_threshold or numeric_value < -percent_threshold:\n",
        "                        html_content += f'<td class=\"percent-col red-cell\">{formatted_percent}</td>'\n",
        "                        styled_cells_count += 1\n",
        "                    else:\n",
        "                        html_content += f'<td class=\"percent-col\">{formatted_percent}</td>'\n",
        "                except (ValueError, TypeError):\n",
        "                    formatted_percent = str(percent_value)\n",
        "                    html_content += f'<td class=\"percent-col\">{formatted_percent}</td>'\n",
        "            else:\n",
        "                html_content += f'<td class=\"percent-col\"></td>'\n",
        "        \n",
        "        html_content += \"</tr>\"\n",
        "\n",
        "    html_content += \"</table>\"\n",
        "\n",
        "    print(\"HTML table generated successfully!\")\n",
        "    print(f\"Number of rows: {pivot_data_clean.count()}\")\n",
        "    print(f\"Number of periods: {len(periods_sorted)}\")\n",
        "    print(f\"Periods: {periods_sorted}\")\n",
        "    print(f\"Styled cells count: {styled_cells_count}\")\n",
        "\n",
        "    # Get workspace environment\n",
        "    workspace_name = mssparkutils.env.getWorkspaceName()\n",
        "    workspace_env = re.findall(r\"(dev|prod|uat)(?=syn)\", workspace_name)[0]\n",
        "    print(f\"Workspace environment: {workspace_env}\")\n",
        "\n",
        "    # Recipients\n",
        "    recipients = distribution_list\n",
        "\n",
        "    # Send notification\n",
        "    send_notification(\n",
        "        message=html_content, \n",
        "        subject=email_subject, \n",
        "        header=email_header, \n",
        "        status=\"Success\", \n",
        "        color=\"Green\", \n",
        "        recipients=recipients, \n",
        "        env=workspace_env\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print(\"No report scheduled for today\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"process\", \"target_table\": \"calculated_tuc_temp\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",        \"table\":\"material_unit_conversion\",                                  \"view\":\"df_uom\",                \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"normalized\",        \"table\":\"uom_unit_conversion\",                                       \"view\":\"df_uom_unit_conv\",      \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"normalized\",        \"table\":\"bom_consolidated\",                                          \"view\":\"df_bom_complex\",        \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"lorax_dim_bill_of_material_persist_history\",                \"view\":\"df_bom_hist\",           \"project_name\":\"lorax\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"lorax_dim_bill_of_material_persist_header\",                 \"view\":\"df_bom_hdr\",            \"project_name\":\"lorax\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"dim_material\",                                              \"view\":\"dim_material\",          \"project_name\":\"ods\"},\n",
        "        {\"schema\":\"raw\",               \"table\":\"mdg_product_chain_bom\",                                     \"view\":\"df_pc_bom\",             \"project_name\":\"ods\"}\n",
        "           ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "# Cache the product chain bom for performance reason\n",
        "df_pc_bom_0 = spark.sql(\"\"\"\n",
        "    SELECT  a.PARENT as parent,\n",
        "            a.CHILD as child,\n",
        "            a.QUANTITY as qty,\n",
        "            a.LEVEL as lvl\n",
        "    FROM    df_pc_bom a\n",
        "\"\"\")\n",
        "df_pc_bom_0.createOrReplaceTempView('df_pc_bom_0')\n",
        "df_pc_bom_0.cache()\n",
        "\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## 1. DETERMINE THE PORTFOLIO OF PRODUCTS TO CONSIDER FOR THIS REPORT. SPLIT COMPLEX vs. REGULAR MATERIALS\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "MATERIAL_TYPES = ['FERT', 'ZREP']\n",
        "\n",
        "df_mat = spark.sql(f\"\"\"SELECT * FROM dim_material WHERE material_type IN {tuple(MATERIAL_TYPES)}\"\"\")\n",
        "df_mat.createOrReplaceTempView('df_mat')\n",
        "\n",
        "print(\"1. DETERMINE THE PORTFOLIO\")\n",
        "\n",
        "df_product_portfolio = spark.sql(\"\"\"\n",
        "WITH df_bom_cplx AS (\n",
        "    --Build the bom for complex items\n",
        "    SELECT DISTINCT\n",
        "                'COMPLEX' as src,\n",
        "                b.representative_item as complex_representative_item,\n",
        "                a.material_number,\n",
        "                a.bom_component,\n",
        "                a.component_quantity / a.base_quantity as qty\n",
        "    FROM        df_bom_complex a\n",
        "    LEFT JOIN   df_mat b\n",
        "    ON          a.material_number = b.src_agnostic_unique_id\n",
        "    WHERE       a.bom_usage = 5\n",
        "    AND         a.plant IS NOT NULL\n",
        "),\n",
        "df_cplx_mat_list AS (\n",
        "    --Get list of complex materials to be able to distinguish complex from non-complex items\n",
        "    SELECT DISTINCT\n",
        "                a.material_number\n",
        "    FROM        df_bom_cplx a\n",
        "),\n",
        "df_all_materials AS (\n",
        "    --Get the portfolio of products in scope of this scheme\n",
        "    SELECT      DISTINCT\n",
        "                b.src_agnostic_unique_id as material_number,\n",
        "                b.description,\n",
        "                b.representative_item,\n",
        "                CASE WHEN c.material_number IS NOT NULL Then 'Y' ELSE 'N' END AS complex_flag  \n",
        "    FROM        df_mat b\n",
        "    LEFT JOIN   df_cplx_mat_list c\n",
        "    ON          b.src_agnostic_unique_id = c.material_number\n",
        "),\n",
        "df_bom_hdr AS (\n",
        "    --Find corresponding BOMs in BOM Version. Level = 2 \n",
        "    SELECT      a.*\n",
        "    FROM        df_bom_hdr a\n",
        "    JOIN        df_all_materials b\n",
        "    ON          a.parent = b.material_number\n",
        "    WHERE       a.eff_valid_from_date <= date_format(concat(cast(date_format(current_date(),'y') as int) - 1,'-12-31'),'yyyy-MM-dd')\n",
        "    AND         a.eff_valid_to_date >= date_format(concat(cast(date_format(current_date(),'y') as int) - 1,'-12-31'),'yyyy-MM-dd')\n",
        "    AND         b.complex_flag = 'Y'         \n",
        "),\n",
        "df_bom_hst AS (\n",
        "    SELECT DISTINCT\n",
        "                'BOM' as src,\n",
        "                '' as complex_representative_item,\n",
        "                a.header_material as material_number,\n",
        "                split(a.path, ',')[1] as bom_component,\n",
        "                '' as qty\n",
        "    FROM        df_bom_hist a\n",
        "    JOIN        df_bom_hdr b\n",
        "    ON          a.header_material = b.parent\n",
        "    AND         a.header_plant = b.plant\n",
        "    AND         a.version = b.version\n",
        "    AND         a.level = 2\n",
        "),\n",
        "df_both AS (\n",
        "SELECT * FROM df_bom_hst\n",
        "UNION\n",
        "SELECT * FROM df_bom_cplx\n",
        "),\n",
        "df_both_1 AS (\n",
        "--Find corresponding representative items and EAN code\n",
        "SELECT      a.*,\n",
        "            b.representative_item,\n",
        "            b.ean\n",
        "FROM        df_both a\n",
        "LEFT JOIN   df_mat b\n",
        "ON          a.bom_component = b.src_agnostic_unique_id\n",
        "ORDER BY    a.material_number,\n",
        "            b.representative_item\n",
        "),\n",
        "df_pc_bom_1 AS (\n",
        "    --Grab the correct zrep using product chain BOM using the component ZREP code\n",
        "    SELECT      a.parent as parent_lvl2,\n",
        "                a.child as child_lvl2,\n",
        "                a.qty as qty_lvl2,\n",
        "                b.merchandising_unit\n",
        "    FROM        df_pc_bom_0 a\n",
        "    LEFT JOIN   df_mat b\n",
        "    ON          a.child = b.src_agnostic_unique_id\n",
        "    WHERE       a.lvl = 2\n",
        "),\n",
        "df_pc_bom_2 AS (\n",
        "    --One more iteration needed if the subsequent component ZREP is an MCU (It has another level to get to the RSU)\n",
        "    SELECT      a.*,\n",
        "                b.child as child_lvl3\n",
        "    FROM        df_pc_bom_1 a\n",
        "    LEFT JOIN   df_pc_bom_0 b\n",
        "    ON          a.child_lvl2 = b.parent\n",
        "    AND         b.lvl=3\n",
        "    AND         a.merchandising_unit = 'X'\n",
        "),\n",
        "df_joined_0 AS (\n",
        "    SELECT          a.*,\n",
        "                    b.child_lvl2,\n",
        "                    b.merchandising_unit,\n",
        "                    b.child_lvl3,\n",
        "                    coalesce(b.child_lvl3,b.child_lvl2,a.bom_component) as link_fert\n",
        "    FROM            df_both_1 a\n",
        "    LEFT JOIN       df_pc_bom_2 b\n",
        "    ON              a.bom_component = b.parent_lvl2\n",
        "    ORDER BY        a.material_number,coalesce(b.child_lvl2,a.bom_component)\n",
        "),\n",
        "df_joined AS (\n",
        "    SELECT          a.*,\n",
        "                    b.representative_item as link\n",
        "    FROM            df_joined_0 a\n",
        "    LEFT JOIN       df_mat b\n",
        "    ON              a.link_fert = b.src_agnostic_unique_id\n",
        "),\n",
        "df_final_1 AS (\n",
        "    SELECT          a.bom_component as mat_key,\n",
        "                    concat(a.material_number,'-',a.bom_component) as material_number,\n",
        "                    d.description,\n",
        "                    d.representative_item,\n",
        "                    b.ean as ean_uc_complex,\n",
        "                    'Y' as complex_flag,\n",
        "                    a.material_number as complex_material_number,\n",
        "                    c.description as complex_material_description,\n",
        "                    c.representative_item as complex_representative_item,\n",
        "                    a.link\n",
        "    FROM            df_joined a\n",
        "    LEFT JOIN       df_mat b\n",
        "    ON              a.link = b.src_agnostic_unique_id\n",
        "    LEFT JOIN       df_mat c\n",
        "    ON              a.material_number = c.src_agnostic_unique_id\n",
        "    LEFT JOIN       df_mat d\n",
        "    ON              a.bom_component = d.src_agnostic_unique_id\n",
        "    WHERE             a.src = 'BOM'\n",
        "),\n",
        "df_final_2 AS (\n",
        "    SELECT          a.*,\n",
        "                    b.bom_component as complex_pricing_material_number,\n",
        "                    b.qty as complex_tuc_qty\n",
        "    FROM            df_final_1 a\n",
        "    LEFT JOIN       df_joined b\n",
        "    ON              a.link = b.link\n",
        "    AND             a.complex_material_number = b.material_number\n",
        "    AND             b.src = 'COMPLEX'\n",
        "),\n",
        "df_all AS (\n",
        "    SELECT      a.mat_key,\n",
        "                a.representative_item,\n",
        "                a.material_number,\n",
        "                a.description,\n",
        "                a.ean_uc_complex,\n",
        "                a.complex_flag,\n",
        "                a.complex_material_number,\n",
        "                a.complex_material_description,\n",
        "                a.complex_representative_item,\n",
        "                a.complex_pricing_material_number,\n",
        "                a.complex_tuc_qty\n",
        "    FROM        df_final_2 a\n",
        "    UNION\n",
        "    SELECT      a.material_number as mat_key,\n",
        "                a.representative_item,\n",
        "                a.material_number,\n",
        "                a.description,\n",
        "                NULL as ean_uc_complex,\n",
        "                a.complex_flag,\n",
        "                NULL as complex_material_number,\n",
        "                NULL as complex_material_description,\n",
        "                NULL as complex_representative_item,\n",
        "                NULL as complex_pricing_material_number,\n",
        "                NULL as complex_tuc_qty\n",
        "    FROM        df_all_materials a\n",
        "    WHERE       a.complex_flag = 'N'\n",
        "),\n",
        "df_all_final AS (\n",
        "    SELECT      a.mat_key,\n",
        "                a.representative_item,\n",
        "                a.material_number,\n",
        "                --Adding classifications needed for product code determination\n",
        "                d.Business_Segment as business_segment,\n",
        "                d.Tech as tech,\n",
        "                d.Market_Segment as market_segment,\n",
        "                d.Supply_Segment as supply_segment,\n",
        "                d.Product_Category as product_category,\n",
        "                d.Ingredient_Variety as ingredient_variety,\n",
        "                d.Product_Type as product_type,\n",
        "                lower(a.Description) as description,\n",
        "                --Addition classifications needed for users to see in the Power BI report\n",
        "                CASE    WHEN d.Business_Segment = 'Chocolate' and d.market_segment = 'Frozen Snacks'                THEN 'ICE CREAM ITEMS'\n",
        "                        WHEN d.Business_Segment = 'Chocolate' and d.market_segment <> 'Frozen Snacks'               THEN 'CHOCO ITEMS'\n",
        "                        WHEN d.Business_Segment = 'Gum and Confections' and d.market_segment <> 'Frozen Snacks'     THEN 'GUM ITEMS'   \n",
        "                        WHEN d.Business_Segment = 'Petcare'                                                         THEN 'PETFOOD ITEMS'\n",
        "                        WHEN d.Business_Segment = 'Food'                                                            THEN 'FOOD ITEMS'\n",
        "                ELSE    d.Business_Segment\n",
        "                END as item_type,\n",
        "                d.Brand_Flag as brand,\n",
        "                d.Consumer_Pack_Format as consumer_pack_format,\n",
        "                d.Consumer_Pack_Type as consumer_pack_type,\n",
        "                d.Product_Pack_Size as pack_size,\n",
        "                replace(d.Multipack_Quantity,'-pack','') as multipack_quantity,\n",
        "                d.Traded_Unit_Configuration as traded_unit_configuration,\n",
        "                a.complex_flag,\n",
        "                a.complex_material_number,\n",
        "                a.complex_material_description,\n",
        "                a.complex_representative_item,\n",
        "                a.complex_pricing_material_number,\n",
        "                a.complex_tuc_qty,\n",
        "                a.ean_uc_complex\n",
        "    FROM        df_all a\n",
        "    LEFT JOIN   df_mat d\n",
        "        ON      a.mat_key = d.src_agnostic_unique_id\n",
        ")\n",
        "SELECT * FROM df_all_final --where complex\n",
        "\"\"\")\n",
        "# display(df_product_portfolio)\n",
        "df_product_portfolio.createOrReplaceTempView('df_product_portfolio')\n",
        "df_product_portfolio.cache()\n",
        "# record_count = df_product_portfolio.count()\n",
        "# print(f\"The total number of records in the DataFrame is: {record_count}\")\n",
        "\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## 2. DETERMINE THE TRADED UNIT CONFIGURATION\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"2. DETERMINE THE TRADED UNIT CONFIGURATION\")\n",
        "\n",
        "# Convert UOM table to proper uom unit item (TH vs TS)\n",
        "df_uom = spark.sql(\"\"\"\n",
        "    WITH df_uom_unit_conv_2 AS (\n",
        "        SELECT  Unit_of_Measurement as source_uom,\n",
        "                External_Unit_of_Measurement_Commercial_Format as target_uom\n",
        "        FROM    df_uom_unit_conv\n",
        "        WHERE   language_key = 'EN'\n",
        "    )\n",
        "\n",
        "    SELECT  a.Material_Number as material,\n",
        "            a.Alternative_Unit_Of_Measure_For_Stockkeeping_Unit,\n",
        "            b.target_uom as uom,\n",
        "            Denominator_For_Conversion_To_Base_Units_Of_Measure/Numerator_For_Conversion_To_Base_Units_Of_Measure as tuc_pc\n",
        "    FROM    df_uom a, df_uom_unit_conv_2 b\n",
        "    WHERE   a.Alternative_Unit_Of_Measure_For_Stockkeeping_Unit = b.source_uom\n",
        "        AND b.target_uom = 'PC'\n",
        "\n",
        "\"\"\")\n",
        "df_uom.createOrReplaceTempView('df_uom')\n",
        "df_uom.cache()\n",
        "\n",
        "# TUC (Traded Unit Configuration) extraction patterns\n",
        "# Method 1: From MARM table (tuc_pc_m1)\n",
        "# Method 2: From ZZGLOBAL classification when it's numeric (tuc_pc_m2)\n",
        "# Methods 3-14: Extract from description field using various patterns\n",
        "\n",
        "patterns = [\n",
        "    # UVC/UCV/UC patterns (m3-m8) - Consumer unit markers\n",
        "    r\"(\\d+)\\s+uvc\",  # m3: \"24 uvc\" - number with space before uvc\n",
        "    r\"(\\d+)uvc\",     # m4: \"24uvc\" - number directly before uvc\n",
        "    r\"(\\d+)\\s+uvc\",  # m5: \"24 ucv\" - number with space before ucv\n",
        "    r\"(\\d+)+uvc\",     # m6: \"24ucv\" - number directly before ucv\n",
        "    r\"(\\d+)\\s+uc\",   # m7: \"24 uc\" - number with space before uc\n",
        "    r\"(\\d+)uc\",      # m8: \"24uc\" - number directly before uc\n",
        "    \n",
        "    # Weight-based patterns (m9-m11) - Extract from product weight notation\n",
        "    #r'(\\d+(?:[*x]\\d+)+)[*x]\\d+g',  # m9: \"2*12*150g\" or \"2x12x150g\" -> \"2*12\" - multi-level before weight\n",
        "    #r'\\d+g[*x](\\d+(?:[*x]\\d+)*)',  # m10: \"56gx12x12\" -> \"12*12\" - multi-level after weight\n",
        "    #r'(?<![*x\\d])(\\d+)[*x]\\d+g',   # m11: \"24*400g\" -> \"24\" - single number before weight\n",
        "]\n",
        "\n",
        "# Generate CASE statements for pattern matching with x->* normalization\n",
        "pattern_cases = []\n",
        "for i, pattern in enumerate(patterns, start=3):\n",
        "    pattern_cases.append(f\"\"\"\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"{pattern}\", 1)) > 0 \n",
        "                         THEN replace(regexp_extract(lower(description), r\"{pattern}\", 1), 'x', '*')\n",
        "                         ELSE NULL\n",
        "                    END AS tuc_pc_m{i}\"\"\")\n",
        "\n",
        "pattern_sql = \",\".join(pattern_cases)\n",
        "coalesce_cols = \",\".join([\"tuc_pc_m1\", \"tuc_pc_m2\"] + [f\"tuc_pc_m{i}\" for i in range(3, 3 + len(patterns))])\n",
        "\n",
        "print(f\"Tuc columns: {coalesce_cols}\")\n",
        "df_tuc_marm = spark.sql(f\"\"\"\n",
        "    WITH df_0 AS (\n",
        "        -- Method 1: Get TUC from MARM (Material Unit of Measure) table\n",
        "        SELECT              a.*,\n",
        "                            b.tuc_pc as tuc_pc_m1\n",
        "        FROM                df_product_portfolio a\n",
        "        LEFT JOIN           df_uom b\n",
        "        ON                  a.mat_key = b.material\n",
        "    ),\n",
        "    df_1 AS (\n",
        "        -- Method 2: Extract TUC from ZZGLOBAL classification when it contains only digits\n",
        "        SELECT      *,\n",
        "                    CASE WHEN traded_unit_configuration not rlike '[^0-9]' \n",
        "                         THEN traded_unit_configuration \n",
        "                    END AS tuc_pc_m2              \n",
        "        FROM        df_0 \n",
        "    ),\n",
        "    df_2 AS (\n",
        "        -- Methods 3-11: Extract TUC from description using regex patterns\n",
        "        -- Covers: UVC/UCV/UC markers and weight-based notations (e.g., 24*400g, 2*12*150g, 56gx12x12)\n",
        "        -- All separators normalized to '*' (e.g., \"12x24\" becomes \"12*24\")\n",
        "        SELECT      *, {pattern_sql}\n",
        "        FROM        df_1\n",
        "    ),\n",
        "    df_3 AS (\n",
        "        -- Final selection: Add coalesced result while keeping all individual method columns for debugging\n",
        "        SELECT      material_number,\n",
        "                    description,\n",
        "                    traded_unit_configuration,\n",
        "                    coalesce({coalesce_cols}) as tuc_pc_m1\n",
        "        FROM        df_2\n",
        "    )\n",
        "    SELECT * FROM df_3\n",
        "\"\"\")\n",
        "df_tuc_marm.createOrReplaceTempView('df_tuc_marm')\n",
        "df_tuc_marm.cache()\n",
        "\n",
        "# record_count = df_product_portfolio.count()\n",
        "# print(f\"The total number of records in df_tuc_marm is: {record_count}\")\n",
        "\n",
        "df_tuc_product_chain = spark.sql(\"\"\"\n",
        "    WITH df_lvl1 AS (\n",
        "        --Consider only products in the portfolio\n",
        "        SELECT  a.parent as material,\n",
        "                a.child as lvl1_child,\n",
        "                a.qty as lvl1_qty,\n",
        "                a.lvl as lvl1_lvl\n",
        "        FROM    df_pc_bom_0 a\n",
        "        JOIN    df_product_portfolio c\n",
        "        ON      c.material_number = a.parent\n",
        "    ),\n",
        "    df_lvl2 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl2_parent,\n",
        "                    b.child as lvl2_child,\n",
        "                    b.qty as lvl2_qty,\n",
        "                    b.lvl as lvl2_lvl\n",
        "        FROM        df_lvl1 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl1_child = b.parent\n",
        "            AND     a.lvl1_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_lvl3 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl3_parent,\n",
        "                    b.child as lvl3_child,\n",
        "                    b.qty as lvl3_qty,\n",
        "                    b.lvl as lvl3_lvl\n",
        "        FROM        df_lvl2 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl2_child = b.parent\n",
        "            AND     a.lvl2_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_lvl4 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl4_parent,\n",
        "                    b.child as lvl4_child,\n",
        "                    b.qty as lvl4_qty,\n",
        "                    b.lvl as lvl4_lvl\n",
        "        FROM        df_lvl3 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl3_child = b.parent\n",
        "            AND     a.lvl3_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_with_rsu_flag AS (\n",
        "        SELECT * FROM (\n",
        "            SELECT      a.*,\n",
        "                        b.retail_sales_unit as rsu_lvl1,\n",
        "                        c.retail_sales_unit as rsu_lvl2,\n",
        "                        d.retail_sales_unit as rsu_lvl3,\n",
        "                        e.retail_sales_unit as rsu_lvl4\n",
        "            FROM        df_lvl4 a\n",
        "            LEFT JOIN   df_mat b\n",
        "                ON      a.lvl1_child = b.src_agnostic_unique_id\n",
        "            LEFT JOIN   df_mat c\n",
        "                ON      a.lvl2_child = c.src_agnostic_unique_id\n",
        "            LEFT JOIN   df_mat d\n",
        "                ON      a.lvl3_child = d.src_agnostic_unique_id\n",
        "            LEFT JOIN   df_mat e\n",
        "                ON      a.lvl4_child = e.src_agnostic_unique_id\n",
        "        ) WHERE coalesce(rsu_lvl1,rsu_lvl2,rsu_lvl3,rsu_lvl4) IS NOT NULL\n",
        "    ),\n",
        "    df_lvl1_1_tuc_0 AS (\n",
        "        SELECT DISTINCT\n",
        "                    a.material,\n",
        "                    a.lvl1_child,\n",
        "                    a.lvl1_qty,\n",
        "                    a.rsu_lvl1,\n",
        "                    a.rsu_lvl2\n",
        "        FROM        df_with_rsu_flag a  \n",
        "    ),\n",
        "    df_lvl1_1_tuc AS (\n",
        "        SELECT      a.material,\n",
        "                    sum(lvl1_qty) as tuc_pc_m2\n",
        "        FROM        df_lvl1_1_tuc_0 a\n",
        "        WHERE       rsu_lvl1 = 'X'\n",
        "        AND         rsu_lvl2 = 'X'\n",
        "        GROUP BY    a.material\n",
        "    ),\n",
        "    df_lvl1_2_tuc AS (\n",
        "        SELECT      a.material,\n",
        "                    sum(lvl1_qty) as tuc_pc_m2\n",
        "        FROM        df_lvl1_1_tuc_0 a\n",
        "        WHERE       rsu_lvl1 = 'X'\n",
        "        AND         rsu_lvl2 IS NULL\n",
        "        AND         NOT EXISTS (SELECT 1 FROM df_lvl1_1_tuc b WHERE a.material = b.material)\n",
        "        GROUP BY    a.material\n",
        "    ),\n",
        "    df_lvl2_tuc_0 AS (\n",
        "        SELECT      a.material,\n",
        "                    max(lvl1_qty) as lvl1_qty,\n",
        "                    max(lvl2_qty) as lvl2_qty\n",
        "        FROM        df_with_rsu_flag a\n",
        "        WHERE       rsu_lvl1 IS NULL\n",
        "        AND         rsu_lvl2 = 'X'\n",
        "        GROUP BY    a.material\n",
        "    ),\n",
        "    df_lvl2_tuc AS (\n",
        "        SELECT      material,\n",
        "                    lvl1_qty * lvl2_qty as tuc_pc_m2\n",
        "        FROM        df_lvl2_tuc_0\n",
        "    ),\n",
        "    df_final AS (\n",
        "    SELECT * FROM df_lvl1_1_tuc\n",
        "    UNION\n",
        "    SELECT * FROM df_lvl1_2_tuc\n",
        "    UNION\n",
        "    SELECT * FROM df_lvl2_tuc\n",
        "    )\n",
        "    SELECT * FROM df_final\n",
        "    \"\"\")\n",
        "# display(df_tuc_product_chain)\n",
        "df_tuc_product_chain.createOrReplaceTempView('df_tuc_product_chain')\n",
        "df_tuc_product_chain.cache()\n",
        "# record_count = df_tuc_product_chain.count()\n",
        "# print(f\"The total number of records in df_tuc_product_chain is: {record_count}\")\n",
        "\n",
        "df_product_portfolio_1 = spark.sql(\"\"\"\n",
        "WITH df_0 AS (\n",
        "        SELECT      a.material_number,\n",
        "                    coalesce(a.tuc_pc_m1,b.tuc_pc_m2) as tuc_pc\n",
        "        FROM        df_tuc_marm a\n",
        "        LEFT JOIN   df_tuc_product_chain b\n",
        "        ON          a.material_number = b.material\n",
        "    ),\n",
        "    df_1 AS (\n",
        "        SELECT      a.*,\n",
        "                    coalesce (a.complex_tuc_qty,b.tuc_pc) as tuc_pc\n",
        "        FROM        df_product_portfolio a\n",
        "        LEFT JOIN   df_0 b\n",
        "        ON          a.material_number = b.material_number\n",
        "    )\n",
        "    SELECT * FROM df_1\n",
        "\"\"\")\n",
        "# display(df_product_portfolio_1)\n",
        "df_product_portfolio_1.createOrReplaceTempView('df_product_portfolio_1')\n",
        "df_product_portfolio_1.cache()\n",
        "# record_count = df_product_portfolio_1.count()\n",
        "# print(f\"The total number of records in the DataFrame is: {record_count}\")\n",
        "\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## 3. EAN UC DETERMINATION (Using Product Chain BOM to determine the EAN of the RSU). ONLY FOR REGULAR MATERIALS\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"3. DETERMINE THE EAN UC\")\n",
        "\n",
        "df_product_portfolio_2 = spark.sql(\"\"\"\n",
        "    WITH df_lvl1 AS (\n",
        "        --Consider only products in the portfolio\n",
        "        SELECT  a.parent as material,\n",
        "                a.child as lvl1_child,\n",
        "                a.qty as lvl1_qty,\n",
        "                a.lvl as lvl1_lvl\n",
        "        FROM    df_pc_bom_0 a\n",
        "        JOIN    df_product_portfolio_1 c\n",
        "            ON  c.mat_key = a.parent\n",
        "    ),\n",
        "    df_lvl2 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl2_parent,\n",
        "                    b.child as lvl2_child,\n",
        "                    b.qty as lvl2_qty,\n",
        "                    b.lvl as lvl2_lvl\n",
        "        FROM        df_lvl1 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl1_child = b.parent\n",
        "            AND     a.lvl1_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_lvl3 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl3_parent,\n",
        "                    b.child as lvl3_child,\n",
        "                    b.qty as lvl3_qty,\n",
        "                    b.lvl as lvl3_lvl\n",
        "        FROM        df_lvl2 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl2_child = b.parent\n",
        "            AND     a.lvl2_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_lvl4 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl4_parent,\n",
        "                    b.child as lvl4_child,\n",
        "                    b.qty as lvl4_qty,\n",
        "                    b.lvl as lvl4_lvl\n",
        "        FROM        df_lvl3 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl3_child = b.parent\n",
        "            AND     a.lvl3_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_with_ean AS (\n",
        "        SELECT      a.*,\n",
        "                    CASE WHEN b.retail_sales_unit = 'X' THEN b.ean ELSE NULL END AS ean_lvl1_child,\n",
        "                    CASE WHEN c.retail_sales_unit = 'X' THEN c.ean ELSE NULL END AS ean_lvl2_child,\n",
        "                    CASE WHEN d.retail_sales_unit = 'X' THEN d.ean ELSE NULL END AS ean_lvl3_child,\n",
        "                    CASE WHEN e.retail_sales_unit = 'X' THEN e.ean ELSE NULL END AS ean_lvl4_child\n",
        "        FROM        df_lvl4 a\n",
        "        LEFT JOIN   df_mat b\n",
        "            ON      a.lvl1_child = b.src_agnostic_unique_id\n",
        "        LEFT JOIN   df_mat c\n",
        "            ON      a.lvl2_child = c.src_agnostic_unique_id\n",
        "        LEFT JOIN   df_mat d\n",
        "            ON      a.lvl3_child = d.src_agnostic_unique_id\n",
        "        LEFT JOIN   df_mat e\n",
        "            ON      a.lvl4_child = e.src_agnostic_unique_id\n",
        "    ),\n",
        "    df_0 AS (\n",
        "        SELECT DISTINCT      \n",
        "                    material,\n",
        "                    coalesce(ean_lvl1_child,ean_lvl2_child,ean_lvl3_child,ean_lvl4_child) as ean_uc_regular\n",
        "        FROM        df_with_ean\n",
        "        WHERE       coalesce(ean_lvl1_child,ean_lvl2_child,ean_lvl3_child,ean_lvl4_child) IS NOT NULL\n",
        "    ),\n",
        "    df_1 AS (\n",
        "        SELECT      material,\n",
        "                    count(ean_uc_regular) as cnt\n",
        "        FROM        df_0\n",
        "        GROUP BY    material\n",
        "    ),\n",
        "    df_2 AS (\n",
        "        SELECT      *\n",
        "        FROM        (\n",
        "                    SELECT      a.*,\n",
        "                                b.cnt\n",
        "                    FROM        df_0 a\n",
        "                    LEFT JOIN   df_1 b\n",
        "                    ON          a.material = b.material\n",
        "                    )\n",
        "        WHERE       cnt = 1\n",
        "    )\n",
        "    SELECT      a.*,\n",
        "                coalesce(a.ean_uc_complex,b.ean_uc_regular) as ean_uc\n",
        "    FROM        df_product_portfolio_1 a\n",
        "    LEFT JOIN   df_2 b\n",
        "    ON          a.material_number = b.material\n",
        "\"\"\")\n",
        "df_product_portfolio_2 = df_product_portfolio_2.drop('ean_uc_regular','ean_uc_complex')\n",
        "# display(df_product_portfolio_2)\n",
        "df_product_portfolio_2.createOrReplaceTempView('df_product_portfolio_2')\n",
        "df_product_portfolio_2.cache()\n",
        "# record_count = df_product_portfolio_3.count()\n",
        "# print(f\"The total number of records in the DataFrame is: {record_count}\")\n",
        "\n",
        "print(\"4. CREATE TABLE\")\n",
        "## ------------------------------------------------------------------------------------\n",
        "# full_output_path = 'abfss://files@globalxsegcor1051devsa.dfs.core.windows.net/test/ean_uc'\n",
        "# df_tuc_product_chain.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(full_output_path, sep='|')\n",
        "create_table(metadata_json, df_product_portfolio_2)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \n",
        "\n",
        "\n",
        "SELECT * FROM \n",
        "ods_process.calculated_tuc_temp\n",
        "where mat_key in \n",
        "('BA02N',\n",
        "'CM96H',\n",
        "'CR93M',\n",
        "'CS63A',\n",
        "'CT78D',\n",
        "'DC95J',\n",
        "'DH16H',\n",
        "'DH21T',\n",
        "'DH48A',\n",
        "'DH95F',\n",
        "'DJ31C',\n",
        "'DN17X',\n",
        "'DP75Y',\n",
        "'DR67V',\n",
        "'FA12S',\n",
        "'FA27J',\n",
        "'FA36J',\n",
        "'FB97X',\n",
        "'FC57G',\n",
        "'FE49H',\n",
        "'FE50E',\n",
        "'FF00S',\n",
        "'VC475',\n",
        "'VC939',\n",
        "'WE680',\n",
        "'YV349',\n",
        "'YV709')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \n",
        "\n",
        "SELECT DISTINCT m.material TUC FROM lorax_output.lorax_dim_material m \n",
        "left join ods_process.calculated_tuc_temp t \n",
        "ON t.mat_key = m.material\n",
        "WHERE m.material_type in ('FERT','ZREP') \n",
        "and m.SourceSystemUnit = 1\n",
        "and t.tuc_pc is null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "patterns = [\n",
        "    # UVC/UCV/UC patterns (m3-m8) - Consumer unit markers\n",
        "    r\"(\\d+)\\s+uvc\",  # m3: \"24 uvc\" - number with space before uvc\n",
        "    r\"(\\d+)uvc\",     # m4: \"24uvc\" - number directly before uvc\n",
        "    r\"(\\d+)\\s+ucv\",  # m5: \"24 ucv\" - number with space before ucv\n",
        "    r\"(\\d+)ucv\",     # m6: \"24ucv\" - number directly before ucv\n",
        "    r\"(\\d+)\\s+uc\",   # m7: \"24 uc\" - number with space before uc\n",
        "    r\"(\\d+)uc\",      # m8: \"24uc\" - number directly before uc\n",
        "    \n",
        "    # Weight-based patterns (m9-m11) - Extract from product weight notation\n",
        "    #r'(\\d+(?:[*x]\\d+)+)[*x]\\d+g',  # m9: \"2*12*150g\" or \"2x12x150g\" -> \"2*12\" - multi-level before weight\n",
        "    #r'\\d+g[*x](\\d+(?:[*x]\\d+)*)',  # m10: \"56gx12x12\" -> \"12*12\" - multi-level after weight\n",
        "    #r'(?<![*x\\d])(\\d+)[*x]\\d+g',   # m11: \"24*400g\" -> \"24\" - single number before weight\n",
        "]\n",
        "\n",
        "# Generate CASE statements for pattern matching with x->* normalization\n",
        "pattern_cases = []\n",
        "for i, pattern in enumerate(patterns, start=3):\n",
        "    pattern_cases.append(f\"\"\"\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"{pattern}\", 1)) > 0 \n",
        "                         THEN replace(regexp_extract(lower(description), r\"{pattern}\", 1), 'x', '*')\n",
        "                         ELSE NULL\n",
        "                    END AS tuc_pc_m{i}\"\"\")\n",
        "\n",
        "pattern_sql = \",\".join(pattern_cases)\n",
        "coalesce_cols = \",\".join([\"tuc_pc_m1\", \"tuc_pc_m2\"] + [f\"tuc_pc_m{i}\" for i in range(3, 3 + len(patterns))])\n",
        "\n",
        "print(f\"Tuc columns: {coalesce_cols}\")\n",
        "df_tuc_marm = spark.sql(f\"\"\"\n",
        "    WITH df_0 AS (\n",
        "        -- Method 1: Get TUC from MARM (Material Unit of Measure) table\n",
        "        SELECT              a.*,\n",
        "                            b.tuc_pc as tuc_pc_m1\n",
        "        FROM                df_product_portfolio a\n",
        "        LEFT JOIN           df_uom b\n",
        "        ON                  a.mat_key = b.material\n",
        "    ),\n",
        "    df_1 AS (\n",
        "        -- Method 2: Extract TUC from ZZGLOBAL classification when it contains only digits\n",
        "        SELECT      *,\n",
        "                    CASE WHEN traded_unit_configuration not rlike '[^0-9]' \n",
        "                         THEN traded_unit_configuration \n",
        "                    END AS tuc_pc_m2              \n",
        "        FROM        df_0 \n",
        "    ),\n",
        "    df_2 AS (\n",
        "        -- Methods 3-11: Extract TUC from description using regex patterns\n",
        "        -- Covers: UVC/UCV/UC markers and weight-based notations (e.g., 24*400g, 2*12*150g, 56gx12x12)\n",
        "        -- All separators normalized to '*' (e.g., \"12x24\" becomes \"12*24\")\n",
        "        SELECT      *, {pattern_sql}\n",
        "        FROM        df_1\n",
        "    ),\n",
        "    df_3 AS (\n",
        "        -- Final selection: Add coalesced result while keeping all individual method columns for debugging\n",
        "        SELECT      material_number,\n",
        "                    description,\n",
        "                    traded_unit_configuration,\n",
        "                    {coalesce_cols} as tuc_pc_m1\n",
        "        FROM        df_2\n",
        "    )\n",
        "    SELECT * FROM df_3\n",
        "\"\"\")\n",
        "df_tuc_marm.createOrReplaceTempView('df_tuc_marm')\n",
        "df_tuc_marm.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \n",
        "\n",
        "\n",
        "SELECT traded_unit_configuration, count(*) FROM \n",
        "df_tuc_marm\n",
        "where material_number in \n",
        "('BA02N',\n",
        "'CM96H',\n",
        "'CR93M',\n",
        "'CS63A',\n",
        "'CT78D',\n",
        "'DC95J',\n",
        "'DH16H',\n",
        "'DH21T',\n",
        "'DH48A',\n",
        "'DH95F',\n",
        "'DJ31C',\n",
        "'DN17X',\n",
        "'DP75Y',\n",
        "'DR67V',\n",
        "'FA12S',\n",
        "'FA27J',\n",
        "'FA36J',\n",
        "'FB97X',\n",
        "'FC57G',\n",
        "'FE49H',\n",
        "'FE50E',\n",
        "'FF00S',\n",
        "'VC475',\n",
        "'VC939',\n",
        "'WE680',\n",
        "'YV349',\n",
        "'YV709',\n",
        "'BN96Y',\n",
        "'CF13X',\n",
        "'CJ50J',\n",
        "'CR93K',\n",
        "'CR94N',\n",
        "'CS53A',\n",
        "'CX39B',\n",
        "'DC32E',\n",
        "'DD64F',\n",
        "'DF86B',\n",
        "'DF92T',\n",
        "'DG32J',\n",
        "'DH39P',\n",
        "'DH84X',\n",
        "'DH96S',\n",
        "'DK41H',\n",
        "'DN78K',\n",
        "'DP52H',\n",
        "'FB45X',\n",
        "'FC87C',\n",
        "'FE41M',\n",
        "'FF50W',\n",
        "'CR25D',\n",
        "'CR60P',\n",
        "'CX39E',\n",
        "'DE87K',\n",
        "'DF92W',\n",
        "'DF93L',\n",
        "'DH22P',\n",
        "'DH53J',\n",
        "'DH58G',\n",
        "'DH84N',\n",
        "'DJ34H',\n",
        "'DK71J',\n",
        "'DK82G',\n",
        "'DK82H',\n",
        "'DP31K',\n",
        "'DP33N',\n",
        "'DP75W',\n",
        "'FA46C',\n",
        "'FB03W',\n",
        "'FB21B',\n",
        "'FC55B',\n",
        "'FE94W',\n",
        "'VC320',\n",
        "'BS23T',\n",
        "'BS97M',\n",
        "'CH52F',\n",
        "'CH52K',\n",
        "'CL12C',\n",
        "'DJ96W',\n",
        "'DM95A',\n",
        "'DM97D',\n",
        "'DP32T',\n",
        "'DP39E',\n",
        "'DP56A',\n",
        "'FC26E',\n",
        "'FD94H',\n",
        "'FF00B',\n",
        "'FF00H',\n",
        "'CG02B',\n",
        "'CH13E',\n",
        "'CR93G',\n",
        "'CR94C',\n",
        "'CR94P',\n",
        "'CS52S',\n",
        "'CS83Y',\n",
        "'CS85G',\n",
        "'CS85V',\n",
        "'DC31H',\n",
        "'DC31W',\n",
        "'DD21E',\n",
        "'DD63N',\n",
        "'DE87L',\n",
        "'DH58J',\n",
        "'DH95D',\n",
        "'DH95K',\n",
        "'DH95S',\n",
        "'DJ93M',\n",
        "'DP33R',\n",
        "'DP48V',\n",
        "'DP86W',\n",
        "'FA35W',\n",
        "'FB04W',\n",
        "'FC05X',\n",
        "'FC28S',\n",
        "'FC59J',\n",
        "'FE99X',\n",
        "'FF00C',\n",
        "'FF70S',\n",
        "'AV14G',\n",
        "'BR60A',\n",
        "'CV36P',\n",
        "'CX16C',\n",
        "'DA02Y',\n",
        "'DH28J',\n",
        "'DJ90B',\n",
        "'DK71Y',\n",
        "'DM96C',\n",
        "'FA09N',\n",
        "'FB07S',\n",
        "'FC55C',\n",
        "'FE45G',\n",
        "'FE99Y',\n",
        "'FF00K',\n",
        "'BA02M',\n",
        "'BP47X',\n",
        "'BR24E',\n",
        "'CS69K',\n",
        "'CS84A',\n",
        "'CT26V',\n",
        "'CX16D',\n",
        "'DE79E',\n",
        "'DH29B',\n",
        "'DK71V',\n",
        "'DP29M',\n",
        "'DP33P',\n",
        "'DP46S',\n",
        "'DP52J',\n",
        "'FA41M',\n",
        "'FB04M',\n",
        "'FB04Y',\n",
        "'FB42Y',\n",
        "'FB94G',\n",
        "'FC16J',\n",
        "'FC26P',\n",
        "'FC98D',\n",
        "'FE45E',\n",
        "'FE50B',\n",
        "'FE50F',\n",
        "'FE94V',\n",
        "'FF00A',\n",
        "'FF89S',\n",
        "'AF35P',\n",
        "'BN98E',\n",
        "'BY24F',\n",
        "'CH00N',\n",
        "'CH52J',\n",
        "'CR94E',\n",
        "'CW06J',\n",
        "'DC31J',\n",
        "'DE77A',\n",
        "'DE79H',\n",
        "'DE87H',\n",
        "'DF92X',\n",
        "'DF93G',\n",
        "'DH15R',\n",
        "'DH39F',\n",
        "'DH58E',\n",
        "'DK41R',\n",
        "'DL46R',\n",
        "'DP32X',\n",
        "'DP33D',\n",
        "'DP38N',\n",
        "'DR67H',\n",
        "'DR67P',\n",
        "'FA39H',\n",
        "'FB04S',\n",
        "'FB10S',\n",
        "'FC54R',\n",
        "'FD36E',\n",
        "'FE46W',\n",
        "'FE99T',\n",
        "'FE99V',\n",
        "'FF00D',\n",
        "'FF00F',\n",
        "'FF11E',\n",
        "'WD262')\n",
        "group by traded_unit_configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        }
      },
      "source": [
        "%%sql \n",
        "\n",
        "SELECT \n",
        "            DISTINCT m.material, m.material_type, m.Description, d.Traded_Unit_Configuration, \n",
        "            d.X_Plant_Status_Desc, d.Merchandising_Unit, d.Retail_Sales_Unit, d.Traded_Unit, \n",
        "            CASE WHEN bp.parent IS NOT NULL THEN 'X' END as product_chain_parent, \n",
        "            CASE WHEN  bc.child  IS NOT NULL THEN 'X' END as product_chain_child\n",
        "FROM        lorax_output.lorax_dim_material m \n",
        "left join   df_product_portfolio_2 t \n",
        "    ON      t.mat_key = m.material\n",
        "left join   ods_output.dim_material d \n",
        "    ON      m.material = d.material\n",
        "LEFT JOIN   df_pc_bom bp \n",
        "    ON      bp.parent = m.material\n",
        "LEFT JOIN   df_pc_bom bc \n",
        "    ON      bc.child = m.material\n",
        "WHERE       m.material_type in ('FERT','ZREP') \n",
        "    and     m.SourceSystemUnit = 1\n",
        "    and     t.tuc_pc is null "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"process\", \"target_table\": \"lorax_'''+str(EPR_SCHEME).lower()+'''_step1\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"normalized\",        \"table\":\"bom_consolidated\",                                          \"view\":\"df_bom_complex\",        \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"dim_material\",                                              \"view\":\"df_mat\",                \"project_name\":\"ods\"},\n",
        "        {\"schema\":\"raw\",               \"table\":\"mdg_product_chain_bom\",                                     \"view\":\"df_pc_bom\",             \"project_name\":\"ods\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"lorax_dim_bill_of_material_persist_history\",                \"view\":\"df_bom_hist\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"lorax_dim_bill_of_material_persist_header\",                 \"view\":\"df_bom_hdr\"},\n",
        "        {\"schema\":\"process\",           \"table\":\"lorax_fact_shipment_actuals\",                               \"view\":\"df_sales\"},\n",
        "        {\"view_name\":\"df_portfolio\", \"path\":\"/EPR/EPR_PORTFOLIO_FRANCE.csv\", \"file_format\":\"csv\", \"container\":\"files\" ,\"linked_service_name\":\"SOLUTION_ADLS_LS\", \"opertation_type\":\"Linked Service\", \"csv_options\":{ \"inferSchema\": \"true\", \"header\": \"true\"}}         \n",
        "           ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "# Cache the product chain bom for performance reason\n",
        "df_pc_bom_0 = spark.sql(\"\"\"\n",
        "    SELECT  REPLACE(LTRIM(REPLACE(a.PARENT,'0',' ')),' ','0') as parent,\n",
        "            REPLACE(LTRIM(REPLACE(a.CHILD,'0',' ')),' ','0') as child,\n",
        "            a.QUANTITY as qty,\n",
        "            a.LEVEL as lvl\n",
        "    FROM    df_pc_bom a\n",
        "\"\"\")\n",
        "df_pc_bom_0.createOrReplaceTempView('df_pc_bom_0')\n",
        "df_pc_bom_0.cache()\n",
        "\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## STEP 1. DETERMINE THE PORTFOLIO OF PRODUCTS TO CONSIDER FOR THIS REPORT. SPLIT COMPLEX vs. REGULAR MATERIALS\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"1. DETERMINE THE PORTFOLIO\")\n",
        "\n",
        "df_product_portfolio = spark.sql(\"\"\"\n",
        "WITH df_bom_cplx AS (\n",
        "    --Build the bom for complex items\n",
        "    SELECT DISTINCT\n",
        "                'COMPLEX' as src,\n",
        "                b.representative_item as complex_representative_item,\n",
        "                a.material_number,\n",
        "                a.bom_component,\n",
        "                a.component_quantity / a.base_quantity as qty\n",
        "    FROM        df_bom_complex a\n",
        "    LEFT JOIN   df_mat b\n",
        "    ON          a.material_number = b.src_agnostic_unique_id\n",
        "    WHERE       a.bom_usage = 5\n",
        "    AND         a.plant IS NOT NULL\n",
        "),\n",
        "df_cplx_mat_list AS (\n",
        "    --Get list of complex materials to be able to distinguish complex from non-complex items\n",
        "    SELECT DISTINCT\n",
        "                a.material_number\n",
        "    FROM        df_bom_cplx a\n",
        "),\n",
        "df_all_materials AS (\n",
        "    --Get the portfolio of products in scope of this scheme\n",
        "    SELECT      DISTINCT\n",
        "                a.material_number,\n",
        "                b.description,\n",
        "                b.representative_item,\n",
        "                CASE WHEN c.material_number IS NOT NULL Then 'Y' ELSE 'N' END AS complex_flag  \n",
        "    FROM        df_sales a\n",
        "    LEFT JOIN   df_mat b\n",
        "    ON          a.material_number = b.src_agnostic_unique_id\n",
        "    LEFT JOIN   df_cplx_mat_list c\n",
        "    ON          a.material_number = c.material_number\n",
        "    WHERE       a.sales_organization in ('\"\"\"+SALES_ORG_STR+\"\"\"')\n",
        "    AND         a.flow in ('\"\"\"+FLOW_STR+\"\"\"')\n",
        "    AND         year(a.actual_gi_date) >= year(current_date()) - 1\n",
        "    AND         b.traded_unit = 'X'\n",
        "),\n",
        "df_bom_hdr AS (\n",
        "    --Find corresponding BOMs in BOM Version. Level = 2 \n",
        "    SELECT      a.*\n",
        "    FROM        df_bom_hdr a\n",
        "    JOIN        df_all_materials b\n",
        "    ON          a.parent = b.material_number\n",
        "    WHERE       a.eff_valid_from_date <= date_format(concat(cast(date_format(current_date(),'y') as int) - 1,'-12-31'),'yyyy-MM-dd')\n",
        "    AND         a.eff_valid_to_date >= date_format(concat(cast(date_format(current_date(),'y') as int) - 1,'-12-31'),'yyyy-MM-dd')\n",
        "    AND         b.complex_flag = 'Y'         \n",
        "),\n",
        "df_bom_hst AS (\n",
        "    SELECT DISTINCT\n",
        "                'BOM' as src,\n",
        "                '' as complex_representative_item,\n",
        "                a.header_material as material_number,\n",
        "                split(a.path, ',')[1] as bom_component,\n",
        "                '' as qty\n",
        "    FROM        df_bom_hist a\n",
        "    JOIN        df_bom_hdr b\n",
        "    ON          a.header_material = b.parent\n",
        "    AND         a.header_plant = b.plant\n",
        "    AND         a.version = b.version\n",
        "    AND         a.level = 2\n",
        "),\n",
        "df_both AS (\n",
        "SELECT * FROM df_bom_hst\n",
        "UNION\n",
        "SELECT * FROM df_bom_cplx\n",
        "),\n",
        "df_both_1 AS (\n",
        "--Find corresponding representative items and EAN code\n",
        "SELECT      a.*,\n",
        "            b.representative_item,\n",
        "            b.ean\n",
        "FROM        df_both a\n",
        "LEFT JOIN   df_mat b\n",
        "ON          a.bom_component = b.src_agnostic_unique_id\n",
        "ORDER BY    a.material_number,\n",
        "            b.representative_item\n",
        "),\n",
        "df_pc_bom_1 AS (\n",
        "    --Grab the correct zrep using product chain BOM using the component ZREP code\n",
        "    SELECT      a.parent as parent_lvl2,\n",
        "                a.child as child_lvl2,\n",
        "                a.qty as qty_lvl2,\n",
        "                b.merchandising_unit\n",
        "    FROM        df_pc_bom_0 a\n",
        "    LEFT JOIN   df_mat b\n",
        "    ON          a.child = b.src_agnostic_unique_id\n",
        "    WHERE       a.lvl = 2\n",
        "),\n",
        "df_pc_bom_2 AS (\n",
        "    --One more iteration needed if the subsequent component ZREP is an MCU (It has another level to get to the RSU)\n",
        "    SELECT      a.*,\n",
        "                b.child as child_lvl3\n",
        "    FROM        df_pc_bom_1 a\n",
        "    LEFT JOIN   df_pc_bom_0 b\n",
        "    ON          a.child_lvl2 = b.parent\n",
        "    AND         b.lvl=3\n",
        "    AND         a.merchandising_unit = 'X'\n",
        "),\n",
        "df_joined_0 AS (\n",
        "    SELECT          a.*,\n",
        "                    b.child_lvl2,\n",
        "                    b.merchandising_unit,\n",
        "                    b.child_lvl3,\n",
        "                    coalesce(b.child_lvl3,b.child_lvl2,a.bom_component) as link_fert\n",
        "    FROM            df_both_1 a\n",
        "    LEFT JOIN       df_pc_bom_2 b\n",
        "    ON              a.bom_component = b.parent_lvl2\n",
        "    ORDER BY        a.material_number,coalesce(b.child_lvl2,a.bom_component)\n",
        "),\n",
        "df_joined AS (\n",
        "    SELECT          a.*,\n",
        "                    b.representative_item as link\n",
        "    FROM            df_joined_0 a\n",
        "    LEFT JOIN       df_mat b\n",
        "    ON              a.link_fert = b.src_agnostic_unique_id\n",
        "),\n",
        "df_final_1 AS (\n",
        "    SELECT          a.bom_component as mat_key,\n",
        "                    concat(a.material_number,'-',a.bom_component) as material_number,\n",
        "                    d.description,\n",
        "                    d.representative_item,\n",
        "                    b.ean as ean_uc_complex,\n",
        "                    'Y' as complex_flag,\n",
        "                    a.material_number as complex_material_number,\n",
        "                    c.description as complex_material_description,\n",
        "                    c.representative_item as complex_representative_item,\n",
        "                    a.link\n",
        "    FROM            df_joined a\n",
        "    LEFT JOIN       df_mat b\n",
        "    ON              a.link = b.src_agnostic_unique_id\n",
        "    LEFT JOIN       df_mat c\n",
        "    ON              a.material_number = c.src_agnostic_unique_id\n",
        "    LEFT JOIN       df_mat d\n",
        "    ON              a.bom_component = d.src_agnostic_unique_id\n",
        "    WHERE             a.src = 'BOM'\n",
        "),\n",
        "df_final_2 AS (\n",
        "    SELECT          a.*,\n",
        "                    b.bom_component as complex_pricing_material_number,\n",
        "                    b.qty as complex_tuc_qty\n",
        "    FROM            df_final_1 a\n",
        "    LEFT JOIN       df_joined b\n",
        "    ON              a.link = b.link\n",
        "    AND             a.complex_material_number = b.material_number\n",
        "    AND             b.src = 'COMPLEX'\n",
        "),\n",
        "df_all AS (\n",
        "    SELECT      a.mat_key,\n",
        "                a.representative_item,\n",
        "                a.material_number,\n",
        "                a.description,\n",
        "                a.ean_uc_complex,\n",
        "                a.complex_flag,\n",
        "                a.complex_material_number,\n",
        "                a.complex_material_description,\n",
        "                a.complex_representative_item,\n",
        "                a.complex_pricing_material_number,\n",
        "                a.complex_tuc_qty\n",
        "    FROM        df_final_2 a\n",
        "    UNION\n",
        "    SELECT      a.material_number as mat_key,\n",
        "                a.representative_item,\n",
        "                a.material_number,\n",
        "                a.description,\n",
        "                NULL as ean_uc_complex,\n",
        "                a.complex_flag,\n",
        "                NULL as complex_material_number,\n",
        "                NULL as complex_material_description,\n",
        "                NULL as complex_representative_item,\n",
        "                NULL as complex_pricing_material_number,\n",
        "                NULL as complex_tuc_qty\n",
        "    FROM        df_all_materials a\n",
        "    WHERE       a.complex_flag = 'N'\n",
        "),\n",
        "df_all_final AS (\n",
        "    SELECT      a.mat_key,\n",
        "                a.representative_item,\n",
        "                a.material_number,\n",
        "                --Adding classifications needed for product code determination\n",
        "                d.Business_Segment as business_segment,\n",
        "                d.Tech as tech,\n",
        "                d.Market_Segment as market_segment,\n",
        "                d.Supply_Segment as supply_segment,\n",
        "                d.Product_Category as product_category,\n",
        "                d.Ingredient_Variety as ingredient_variety,\n",
        "                d.Product_Type as product_type,\n",
        "                lower(a.Description) as description,\n",
        "                --Addition classifications needed for users to see in the Power BI report\n",
        "                CASE    WHEN d.Business_Segment = 'Chocolate' and d.market_segment = 'Frozen Snacks'                THEN 'ICE CREAM ITEMS'\n",
        "                        WHEN d.Business_Segment = 'Chocolate' and d.market_segment <> 'Frozen Snacks'               THEN 'CHOCO ITEMS'\n",
        "                        WHEN d.Business_Segment = 'Gum and Confections' and d.market_segment <> 'Frozen Snacks'     THEN 'GUM ITEMS'   \n",
        "                        WHEN d.Business_Segment = 'Petcare'                                                         THEN 'PETFOOD ITEMS'\n",
        "                        WHEN d.Business_Segment = 'Food'                                                            THEN 'FOOD ITEMS'\n",
        "                ELSE    d.Business_Segment\n",
        "                END as item_type,\n",
        "                d.Brand_Flag as brand,\n",
        "                d.Consumer_Pack_Format as consumer_pack_format,\n",
        "                d.Consumer_Pack_Type as consumer_pack_type,\n",
        "                d.Product_Pack_Size as pack_size,\n",
        "                replace(d.Multipack_Quantity,'-pack','') as multipack_quantity,\n",
        "                d.Traded_Unit_Configuration as traded_unit_configuration,\n",
        "                a.complex_flag,\n",
        "                a.complex_material_number,\n",
        "                a.complex_material_description,\n",
        "                a.complex_representative_item,\n",
        "                a.complex_pricing_material_number,\n",
        "                a.complex_tuc_qty,\n",
        "                a.ean_uc_complex,\n",
        "                c.uc as uc_qty_as_is\n",
        "    FROM        df_all a\n",
        "    LEFT JOIN   df_mat d\n",
        "        ON      a.mat_key = d.src_agnostic_unique_id\n",
        "    LEFT JOIN   df_portfolio c\n",
        "        ON      a.mat_key = c.material\n",
        ")\n",
        "SELECT * FROM df_all_final\n",
        "\"\"\")\n",
        "# display(df_product_portfolio)\n",
        "\n",
        "print(\"2. CREATE TABLE\")\n",
        "create_table(metadata_json, df_product_portfolio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "## ------------------------------------------------------------------------------------\n",
        "metadata_json = '''\n",
        "{\n",
        "    \"target_table\": {\"target_schema\": \"process\", \"target_table\": \"lorax_'''+str(EPR_SCHEME).lower()+'''_step2\", \"format\":\"delta\",\"opertation_type\":\"Managed Table Type\"},\n",
        "    \"source_tables\": [\n",
        "        {\"schema\":\"process\",           \"table\":\"lorax_'''+str(EPR_SCHEME).lower()+'''_step1\",               \"view\":\"df_product_portfolio\"},\n",
        "        {\"schema\":\"normalized\",        \"table\":\"material_unit_conversion\",                                  \"view\":\"df_uom\",                \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"normalized\",        \"table\":\"uom_unit_conversion\",                                       \"view\":\"df_uom_unit_conv\",      \"project_name\":\"atlas\"},\n",
        "        {\"schema\":\"output\",            \"table\":\"dim_material\",                                              \"view\":\"df_mat\",                \"project_name\":\"ods\"},\n",
        "        {\"schema\":\"raw\",               \"table\":\"mdg_product_chain_bom\",                                     \"view\":\"df_pc_bom\",             \"project_name\":\"ods\"}\n",
        "           ],\n",
        "    \"env\": \"'''+str(ENV)+'''\",\n",
        "    \"project_name\": \"'''+str(PROJECT_NAME)+'''\",\n",
        "    \"admin_schema_name\": \"'''+str(admin_schema_name)+'''\",\n",
        "    \"admin_table_name_to_output\": \"'''+str(admin_table_name_to_output)+'''\",\n",
        "    \"process_date\": \"'''+str(process_date)+'''\"\n",
        "}\n",
        "'''\n",
        "#Instantiate temp vies and define target table (with env)\n",
        "make_env_tables(metadata_json)\n",
        "\n",
        "# Cache the product chain bom for performance reason\n",
        "df_pc_bom_0 = spark.sql(\"\"\"\n",
        "    SELECT  REPLACE(LTRIM(REPLACE(a.PARENT,'0',' ')),' ','0') as parent,\n",
        "            REPLACE(LTRIM(REPLACE(a.CHILD,'0',' ')),' ','0') as child,\n",
        "            a.QUANTITY as qty,\n",
        "            a.LEVEL as lvl\n",
        "    FROM    df_pc_bom a\n",
        "\"\"\")\n",
        "df_pc_bom_0.createOrReplaceTempView('df_pc_bom_0')\n",
        "df_pc_bom_0.cache()\n",
        "\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## STEP 2. DETERMINE THE TRADED UNIT CONFIGURATION\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"1. DETERMINE THE TRADED UNIT CONFIGURATION\")\n",
        "\n",
        "# Convert UOM table to proper uom unit item (TH vs TS). Deal with IP UOM.\n",
        "df_uom = spark.sql(\"\"\"\n",
        "    WITH df_uom_unit_conv_2 AS (\n",
        "        SELECT  Unit_of_Measurement as source_uom,\n",
        "                External_Unit_of_Measurement_Commercial_Format as target_uom\n",
        "        FROM    df_uom_unit_conv\n",
        "        WHERE   language_key = 'EN'\n",
        "    ),\n",
        "\tdf_uom_1 AS (\n",
        "\t    SELECT      a.Material_Number as material,\n",
        "                    a.Alternative_Unit_Of_Measure_For_Stockkeeping_Unit,\n",
        "                    b.target_uom as uom,\n",
        "                    Denominator_For_Conversion_To_Base_Units_Of_Measure/Numerator_For_Conversion_To_Base_Units_Of_Measure as tuc_pc\n",
        "\t\tFROM        df_uom a, df_uom_unit_conv_2 b\n",
        "\t\tWHERE       a.Alternative_Unit_Of_Measure_For_Stockkeeping_Unit = b.source_uom\n",
        "        AND         b.target_uom IN ('PC','IP')\n",
        "\t),\n",
        "    df_uom_pc AS (\n",
        "        SELECT      *\n",
        "        FROM        df_uom_1\n",
        "        WHERE       uom = 'PC'\n",
        "    ),\n",
        "    df_uom_ip AS (\n",
        "        SELECT      *\n",
        "        FROM        df_uom_1\n",
        "        WHERE       uom = 'IP'\n",
        "    ),\n",
        "    df_uom_2 AS (\n",
        "        SELECT \t\t\ta.material,\n",
        "                        a.traded_unit_configuration as tuc,\n",
        "                        b.tuc_pc as tuc_pc,\n",
        "                        c.tuc_pc as tuc_ip\n",
        "        FROM            df_mat a\n",
        "        LEFT JOIN\t\tdf_uom_pc b\n",
        "        ON\t\t\t\ta.material = b.material\n",
        "        LEFT JOIN\t\tdf_uom_ip c\n",
        "        ON\t\t\t\ta.material = c.material\n",
        "        WHERE\t\t\ta.material_type = 'FERT'\n",
        "        AND             a.sourcesystemunit = 1\n",
        "        --AND             a.material = 'DJ54E'\n",
        "    )\n",
        "    SELECT              a.material,\n",
        "                        CASE WHEN a.tuc = a.tuc_ip THEN a.tuc_ip ELSE a.tuc_pc END AS tuc_pc\n",
        "    FROM                df_uom_2 a\n",
        "\"\"\")\n",
        "# display(df_uom)\n",
        "df_uom.createOrReplaceTempView('df_uom')\n",
        "df_uom.cache()\n",
        "\n",
        "df_tuc_marm = spark.sql(\"\"\"\n",
        "    WITH df_0 AS (\n",
        "        --Method 1: Trade tuc from MARM\n",
        "        SELECT              a.*,\n",
        "                            b.tuc_pc as tuc_pc_m1\n",
        "        FROM                df_product_portfolio a\n",
        "        LEFT JOIN           df_uom b\n",
        "        ON                  a.mat_key = b.material\n",
        "    ),\n",
        "    df_1 AS (\n",
        "        --Method 2: Take tuc from ZZGLOBAL tuc classification when classification is a number\n",
        "        SELECT      *,\n",
        "                    CASE WHEN traded_unit_configuration not rlike '[^0-9]' THEN traded_unit_configuration END AS tuc_pc_m2              \n",
        "        FROM        df_0 \n",
        "    ),\n",
        "    df_2 AS (\n",
        "        --Method 3: Grab from description\n",
        "        SELECT      *,\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"(\\d+)\\s+uvc\", 1)) > 0 \n",
        "                         THEN regexp_extract(lower(description), r\"(\\d+)\\s+uvc\", 1)\n",
        "                         ELSE NULL\n",
        "                    END AS tuc_pc_m3,\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"(\\d+)uvc\", 1)) > 0\n",
        "                        THEN regexp_extract(lower(description), r\"(\\d+)uvc\", 1)\n",
        "                        ELSE NULL\n",
        "                    END AS tuc_pc_m4,\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"(\\d+)\\s+ucv\", 1)) > 0\n",
        "                        THEN regexp_extract(lower(description), r\"(\\d+)\\s+ucv\", 1)\n",
        "                        ELSE NULL\n",
        "                    END AS tuc_pc_m5,\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"(\\d+)+ucv\", 1)) > 0\n",
        "                        THEN regexp_extract(lower(description), r\"(\\d+)+ucv\", 1)\n",
        "                        ELSE NULL\n",
        "                    END AS tuc_pc_m6,\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"(\\d+)\\s+uc\", 1)) > 0\n",
        "                        THEN regexp_extract(lower(description), r\"(\\d+)\\s+uc\", 1)\n",
        "                        ELSE NULL\n",
        "                    END AS tuc_pc_m7,\n",
        "                    CASE WHEN LENGTH(regexp_extract(lower(description), r\"(\\d+)+uc\", 1)) > 0\n",
        "                        THEN regexp_extract(lower(description), r\"(\\d+)+uc\", 1)\n",
        "                        ELSE NULL\n",
        "                    END AS tuc_pc_m8\n",
        "        FROM        df_1\n",
        "    ),\n",
        "    df_3 AS (\n",
        "        SELECT      a.material_number,\n",
        "                    a.description,\n",
        "                    a.traded_unit_configuration,\n",
        "                    coalesce(tuc_pc_m1,tuc_pc_m2,tuc_pc_m3,tuc_pc_m4,tuc_pc_m5,tuc_pc_m6,tuc_pc_m7,tuc_pc_m8) as tuc_pc_m1\n",
        "        FROM        df_2 a\n",
        "    )\n",
        "        SELECT * FROM df_3\n",
        "\"\"\")\n",
        "# display(df_tuc_marm)\n",
        "df_tuc_marm.createOrReplaceTempView('df_tuc_marm')\n",
        "df_tuc_marm.cache()\n",
        "# record_count = df_product_portfolio.count()\n",
        "# print(f\"The total number of records in df_tuc_marm is: {record_count}\")\n",
        "\n",
        "df_tuc_product_chain = spark.sql(\"\"\"\n",
        "    WITH df_lvl1 AS (\n",
        "        --Consider only products in the portfolio\n",
        "        SELECT  a.parent as material,\n",
        "                a.child as lvl1_child,\n",
        "                a.qty as lvl1_qty,\n",
        "                a.lvl as lvl1_lvl\n",
        "        FROM    df_pc_bom_0 a\n",
        "        JOIN    df_product_portfolio c\n",
        "        ON      c.material_number = a.parent\n",
        "    ),\n",
        "    df_lvl2 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl2_parent,\n",
        "                    b.child as lvl2_child,\n",
        "                    b.qty as lvl2_qty,\n",
        "                    b.lvl as lvl2_lvl\n",
        "        FROM        df_lvl1 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl1_child = b.parent\n",
        "            AND     a.lvl1_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_lvl3 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl3_parent,\n",
        "                    b.child as lvl3_child,\n",
        "                    b.qty as lvl3_qty,\n",
        "                    b.lvl as lvl3_lvl\n",
        "        FROM        df_lvl2 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl2_child = b.parent\n",
        "            AND     a.lvl2_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_lvl4 AS (\n",
        "        SELECT      a.*,\n",
        "                    b.parent as lvl4_parent,\n",
        "                    b.child as lvl4_child,\n",
        "                    b.qty as lvl4_qty,\n",
        "                    b.lvl as lvl4_lvl\n",
        "        FROM        df_lvl3 a\n",
        "        LEFT JOIN   df_pc_bom_0 b\n",
        "            ON      a.lvl3_child = b.parent\n",
        "            AND     a.lvl3_lvl+1 = b.lvl\n",
        "    ),\n",
        "    df_with_rsu_flag AS (\n",
        "        SELECT * FROM (\n",
        "            SELECT      a.*,\n",
        "                        b.retail_sales_unit as rsu_lvl1,\n",
        "                        c.retail_sales_unit as rsu_lvl2,\n",
        "                        d.retail_sales_unit as rsu_lvl3,\n",
        "                        e.retail_sales_unit as rsu_lvl4\n",
        "            FROM        df_lvl4 a\n",
        "            LEFT JOIN   df_mat b\n",
        "                ON      a.lvl1_child = b.src_agnostic_unique_id\n",
        "            LEFT JOIN   df_mat c\n",
        "                ON      a.lvl2_child = c.src_agnostic_unique_id\n",
        "            LEFT JOIN   df_mat d\n",
        "                ON      a.lvl3_child = d.src_agnostic_unique_id\n",
        "            LEFT JOIN   df_mat e\n",
        "                ON      a.lvl4_child = e.src_agnostic_unique_id\n",
        "        ) WHERE coalesce(rsu_lvl1,rsu_lvl2,rsu_lvl3,rsu_lvl4) IS NOT NULL\n",
        "    ),\n",
        "    df_lvl1_1_tuc_0 AS (\n",
        "        SELECT DISTINCT\n",
        "                    a.material,\n",
        "                    a.lvl1_child,\n",
        "                    a.lvl1_qty,\n",
        "                    a.rsu_lvl1,\n",
        "                    a.rsu_lvl2\n",
        "        FROM        df_with_rsu_flag a  \n",
        "    ),\n",
        "    df_lvl1_1_tuc AS (\n",
        "        SELECT      a.material,\n",
        "                    sum(lvl1_qty) as tuc_pc_m2\n",
        "        FROM        df_lvl1_1_tuc_0 a\n",
        "        WHERE       rsu_lvl1 = 'X'\n",
        "        AND         rsu_lvl2 = 'X'\n",
        "        GROUP BY    a.material\n",
        "    ),\n",
        "    df_lvl1_2_tuc AS (\n",
        "        SELECT      a.material,\n",
        "                    sum(lvl1_qty) as tuc_pc_m2\n",
        "        FROM        df_lvl1_1_tuc_0 a\n",
        "        WHERE       rsu_lvl1 = 'X'\n",
        "        AND         rsu_lvl2 IS NULL\n",
        "        AND         NOT EXISTS (SELECT 1 FROM df_lvl1_1_tuc b WHERE a.material = b.material)\n",
        "        GROUP BY    a.material\n",
        "    ),\n",
        "    df_lvl2_tuc_0 AS (\n",
        "        SELECT      a.material,\n",
        "                    max(lvl1_qty) as lvl1_qty,\n",
        "                    max(lvl2_qty) as lvl2_qty\n",
        "        FROM        df_with_rsu_flag a\n",
        "        WHERE       rsu_lvl1 IS NULL\n",
        "        AND         rsu_lvl2 = 'X'\n",
        "        GROUP BY    a.material\n",
        "    ),\n",
        "    df_lvl2_tuc AS (\n",
        "        SELECT      material,\n",
        "                    lvl1_qty * lvl2_qty as tuc_pc_m2\n",
        "        FROM        df_lvl2_tuc_0\n",
        "    ),\n",
        "    df_final AS (\n",
        "    SELECT * FROM df_lvl1_1_tuc\n",
        "    UNION\n",
        "    SELECT * FROM df_lvl1_2_tuc\n",
        "    UNION\n",
        "    SELECT * FROM df_lvl2_tuc\n",
        "    )\n",
        "    SELECT * FROM df_final\n",
        "    \"\"\")\n",
        "# display(df_tuc_product_chain)\n",
        "df_tuc_product_chain.createOrReplaceTempView('df_tuc_product_chain')\n",
        "df_tuc_product_chain.cache()\n",
        "# record_count = df_tuc_product_chain.count()\n",
        "# print(f\"The total number of records in df_tuc_product_chain is: {record_count}\")\n",
        "\n",
        "df_product_portfolio_1 = spark.sql(\"\"\"\n",
        "WITH df_0 AS (\n",
        "        SELECT      a.material_number,\n",
        "                    coalesce(a.tuc_pc_m1,b.tuc_pc_m2) as tuc_pc\n",
        "        FROM        df_tuc_marm a\n",
        "        LEFT JOIN   df_tuc_product_chain b\n",
        "        ON          a.material_number = b.material\n",
        "    ),\n",
        "    df_1 AS (\n",
        "        SELECT      a.*,\n",
        "                    coalesce (a.complex_tuc_qty,b.tuc_pc) as tuc_pc\n",
        "        FROM        df_product_portfolio a\n",
        "        LEFT JOIN   df_0 b\n",
        "        ON          a.material_number = b.material_number\n",
        "    )\n",
        "    SELECT * FROM df_1 --where a.material_number = 'DJ54E'\n",
        "\"\"\")\n",
        "# display(df_product_portfolio_1)\n",
        "\n",
        "print(\"2. CREATE TABLE\")\n",
        "create_table(metadata_json, df_product_portfolio_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \n",
        "\n",
        "SELECT m.Material, m.Traded_Unit, a.TDU_Traded_Unit, \n",
        "m.Retail_Sales_Unit, a.RSU_Retail_Sales_Unit, \n",
        "m.Merchandising_Unit, a.MCU_Merchandising_Unit, \n",
        "m.Intermediate_Product_Component, a.INT_Intermediate_Product_Component,\n",
        "m.Semi_Finished_Product, a.SFP_Semi_Finished_Product\n",
        "\n",
        " FROM df_material m \n",
        "LEFT JOIN atlas_normalized.material_attributes a \n",
        "ON m.Material = a.Material_Number\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": []
    }
  ]
}