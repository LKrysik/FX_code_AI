"""
Event Bus
=========
Central asynchronous communication channel for the application.
Production-ready with memory leak prevention and backpressure handling.
"""

import asyncio
import logging
import uuid
import time
import weakref
import random
import threading
from collections import deque
from typing import Callable, Any, Coroutine, Optional, Dict, Set, List
from weakref import WeakMethod, WeakKeyDictionary, WeakSet
from dataclasses import dataclass
from enum import Enum
from .logger import StructuredLogger
from src.infrastructure.exchanges.circuit_breaker import CircuitBreaker, CircuitBreakerOpenError


class EventPriority(Enum):
    """Event priority levels for queue management"""
    CRITICAL = 1    # Trading orders, emergency stops
    HIGH = 2        # Market data, position updates
    NORMAL = 3      # Notifications, logging
    LOW = 4         # Analytics, reporting


@dataclass
class EventMetrics:
    """Event processing metrics for monitoring"""
    total_published: int = 0
    total_processed: int = 0
    total_failed: int = 0
    total_timeouts: int = 0
    total_dropped: int = 0
    avg_processing_time_ms: float = 0.0
    peak_queue_size: int = 0
    active_subscribers: int = 0
    dead_subscribers_cleaned: int = 0
    total_unsubscribe_not_found: int = 0


@dataclass
class QueuedEvent:
    """Event with metadata for queue processing"""
    event_type: str
    data: Any
    priority: EventPriority
    timestamp: float
    trace_id: Optional[str] = None
    retry_count: int = 0


class WeakSubscriber:
    """Weak reference wrapper for event handlers with automatic cleanup"""

    def __init__(self, handler: Callable[[Any], Coroutine], event_type: str, event_bus_ref: weakref.ref):
        self.event_type = event_type
        self.creation_time = time.time()
        self.call_count = 0
        self.last_call_time = 0.0
        self.total_processing_time = 0.0
        self._error_count = 0  # Track errors for circuit breaker
        self._event_bus_ref = event_bus_ref  # Weak reference to EventBus for cleanup

        # Handle both bound methods and functions
        if hasattr(handler, '__self__'):
            # Bound method - use WeakMethod
            self.weak_ref = WeakMethod(handler, self._cleanup_callback)
            self.strong_ref = None
            self.handler_name = f"{handler.__self__.__class__.__name__}.{handler.__name__}"
        else:
            # Function or callable - use STRONG reference to prevent GC
            self.strong_ref = handler
            self.weak_ref = None  # Not used for functions
            self.handler_name = getattr(handler, '__name__', str(handler))

    def _cleanup_callback(self, weak_ref):
        """Called when the referenced object is garbage collected - remove self from subscribers"""
        # Get EventBus instance
        event_bus = self._event_bus_ref()
        if event_bus and self.event_type in event_bus._subscribers:
            # Remove this subscriber from the set
            subscribers_list = event_bus._subscribers[self.event_type]
            subscribers_list.discard(self)
            # Removed metric updates from sync cleanup callback for async safety
            # event_bus._update_metric_atomic("active_subscribers", -1)
            # event_bus._update_metric_atomic("dead_subscribers_cleaned", 1)

            # Clean up empty event type lists
            if not subscribers_list:
                del event_bus._subscribers[self.event_type]

    def get_handler(self) -> Optional[Callable[[Any], Coroutine]]:
        """Get the handler if it's still alive"""
        if self.strong_ref is not None:
            return self.strong_ref
        return self.weak_ref() if self.weak_ref else None

    def is_alive(self) -> bool:
        """Check if the handler is still alive"""
        if self.strong_ref is not None:
            return True
        return self.weak_ref() is not None if self.weak_ref else False

    def update_metrics(self, processing_time: float):
        """Update call metrics"""
        self.call_count += 1
        self.last_call_time = time.time()
        self.total_processing_time += processing_time


class EventBus:
    def __init__(self,
                 default_timeout: float = 0.1,  # ✅ TRADING FIX: Reduced from 5.0s to 100ms for trading latency
                 max_subscribers_per_event: int = 100,
                 max_queue_size: int = 10000,
                 enable_backpressure: bool = True,
                 cleanup_interval_seconds: float = 60.0,
                 worker_counts: Optional[Dict[EventPriority, int]] = None,
                 slow_handler_threshold_ms: float = 200.0,
                 slow_handler_log_interval: float = 5.0):
        """
        Initialize EventBus with production-ready safeguards.

        Args:
            default_timeout: Default timeout for event handlers (reduced to 100ms for trading latency)
            max_subscribers_per_event: Maximum subscribers per event type
            max_queue_size: Maximum size of event queue
            enable_backpressure: Enable backpressure handling
            cleanup_interval_seconds: Interval for cleaning dead subscribers
            worker_counts: Workers per priority level (None = auto-configure)
        """
        # Logger setup - create dedicated EventBus logger
        import logging
        logger_name = "event_bus"
        logger_instance = logging.getLogger(logger_name)
        # Clear any existing handlers to ensure clean setup
        logger_instance.handlers.clear()

        class EventBusLoggerConfig:
            level = "DEBUG"  # Set to DEBUG to ensure all logs are captured
            console_enabled = False  # Disable console logging for EventBus
            file_enabled = True
            structured_logging = True
            log_dir = "logs"
            max_file_size_mb = 100
            backup_count = 5
            file = None  # Removed explicit file path to use log_dir instead

        self.logger = StructuredLogger(logger_name, EventBusLoggerConfig(), "event_bus.jsonl")

        # Weak reference storage for automatic cleanup - MEMORY SAFE
        self._subscribers: Dict[str, Set[WeakSubscriber]] = {}  # Changed to set for O(1) operations

        # ✅ CRITICAL FIX: Bucketed queues per priority instead of single heap
        self._priority_queues = {
            EventPriority.CRITICAL: asyncio.Queue(maxsize=1000),
            EventPriority.HIGH: asyncio.Queue(maxsize=5000),  # Increased from 2000 for better backpressure handling
            EventPriority.NORMAL: asyncio.Queue(maxsize=5000),
            EventPriority.LOW: asyncio.Queue(maxsize=2000)
        }

        # ✅ CRITICAL FIX: Worker pool per priority for true parallel processing
        self._worker_counts = worker_counts or {
            EventPriority.CRITICAL: 4,  # 4 workers for critical events
            EventPriority.HIGH: 6,      # 6 workers for high priority (increased from 2 for better throughput)
            EventPriority.NORMAL: 1,    # 1 worker for normal
            EventPriority.LOW: 1        # 1 worker for low priority
        }

        # Legacy priority queue removed - using bucketed queues only

        # Configuration
        self.default_timeout = default_timeout
        self.max_subscribers_per_event = max_subscribers_per_event
        self.max_queue_size = max_queue_size
        self.enable_backpressure = enable_backpressure
        self.cleanup_interval_seconds = cleanup_interval_seconds

        # Metrics and monitoring
        self.metrics = EventMetrics()
        self._last_cleanup_time = time.time()
        self._processing_times: deque[float] = deque(maxlen=100)  # Reduced for memory efficiency

        # ✅ CRITICAL FIX: Multiple processor tasks per priority
        self._worker_tasks: Dict[EventPriority, List[asyncio.Task]] = {}
        self._is_processing = False
        self._shutdown_requested = False

        # Dead letter queue for failed events - reduced maxlen for memory efficiency
        self._dead_letter_queue: deque[QueuedEvent] = deque(maxlen=100)

        # ✅ CRITICAL FIX: Aggressive TTL-based cleanup for auxiliary structures (reduced to 10 seconds)
        self._rate_limiters: Dict[str, deque] = {}  # No defaultdict to prevent memory leaks
        self._rate_limiter_ttl: Dict[str, float] = {}  # TTL tracking
        self._ttl_cleanup_interval = 10  # 10 seconds TTL (very aggressive)

        # ✅ IMPORTANT FIX: Publisher-based rate limiting
        self._publisher_rate_limits: Dict[str, deque] = {}  # key: f"{event_type}:{publisher_id}"
        self._publisher_rate_limits_ttl: Dict[str, float] = {}  # TTL tracking

        # Circuit breaker for cascade failure prevention
        self._circuit_breakers: Dict[str, 'CircuitBreaker'] = {}
        self._circuit_breaker_ttl: Dict[str, float] = {}  # TTL tracking

        # Task management for production safety
        self._active_tasks: weakref.WeakSet = weakref.WeakSet()

        # Atomic metric updates - changed to asyncio.Lock for async safety
        self._metrics_lock = asyncio.Lock()

        # Synchronous lock for operations that may be called from sync contexts
        self._sync_lock = threading.RLock()

        # ✅ CRITICAL FIX: Locks for publish/cleanup synchronization with consistent hierarchy
        # Lock hierarchy (always acquire in this order to prevent deadlocks):
        # 1. _cleanup_lock (for subscriber management)
        # 2. _publish_lock (for publishing operations)
        # 3. _queue_lock (for legacy queue operations)
        # 4. _ttl_lock (for TTL structures)
        self._cleanup_lock = asyncio.Lock()  # Always acquire first
        self._publish_lock = asyncio.Lock()  # Always acquire second
        self._queue_lock = asyncio.Lock()    # Always acquire third
        self._ttl_lock = asyncio.Lock()      # For TTL structures
        self._worker_lock = asyncio.Lock()   # For worker pool management

        # ✅ CRITICAL FIX: Background cleanup task for aggressive memory management
        self._background_cleanup_task: Optional[asyncio.Task] = None

        # Worker pools will be started when needed
        self._worker_pools_started = False

        # Start background cleanup task
        self._deferred_cleanup_start = False
        self._start_background_cleanup()

        # Slow handler diagnostics configuration
        self.slow_handler_threshold_ms = slow_handler_threshold_ms
        self._slow_handler_log_interval = slow_handler_log_interval
        self._slow_handler_last_log: Dict[str, float] = {}

        self.logger.info("eventbus.initialized", {
            "max_subscribers_per_event": self.max_subscribers_per_event,
            "max_queue_size": self.max_queue_size,
            "enable_backpressure": self.enable_backpressure,
            "cleanup_interval": self.cleanup_interval_seconds,
            "default_timeout": self.default_timeout,
            "worker_counts": self._worker_counts,
            "bucketed_queues": True,
            "ttl_cleanup": True
        })

    async def _update_metric_atomic(self, metric_name: str, value: int = 1):
        """✅ CRITICAL FIX: Atomic metric updates to prevent race conditions"""
        async with self._metrics_lock:
            current_value = getattr(self.metrics, metric_name, 0)
            setattr(self.metrics, metric_name, current_value + value)

    def _start_background_cleanup(self):
        """✅ CRITICAL FIX: Start background cleanup task for aggressive memory management"""
        if self._background_cleanup_task is None or self._background_cleanup_task.done():
            try:
                asyncio.get_running_loop()
            except RuntimeError:
                self._deferred_cleanup_start = True
                return
            self._background_cleanup_task = asyncio.create_task(
                self._background_cleanup_loop(),
                name="EventBus-BackgroundCleanup"
            )
            self._active_tasks.add(self._background_cleanup_task)

    async def _background_cleanup_loop(self):
        """✅ CRITICAL FIX: Background cleanup loop for aggressive memory management"""
        try:
            while not self._shutdown_requested:
                try:
                    # Wait for cleanup interval (10 seconds - aggressive)
                    await asyncio.sleep(self._ttl_cleanup_interval)

                    # Perform aggressive cleanup
                    await self._cleanup_expired_structures()

                    # Also cleanup dead subscribers
                    await self._cleanup_dead_subscribers()

                except Exception as e:
                    self.logger.error("eventbus.background_cleanup_error", {"error": str(e)})
                    # Continue running despite errors

        except Exception as e:
            self.logger.error("eventbus.background_cleanup_loop_error", {"error": str(e)})
        finally:
            self.logger.debug("eventbus.background_cleanup_stopped", {})

    async def subscribe(self, event_type: str, handler: Callable[[Any], Coroutine], priority: EventPriority = EventPriority.NORMAL):
        """
        Subscribe a handler to an event type with automatic cleanup.

        Args:
            event_type: Event type to subscribe to
            handler: Event handler (weak reference will be created)
            priority: Event priority for queue processing

        Raises:
            ValueError: If too many subscribers for this event type or invalid input
        """
        # Input validation
        if not isinstance(event_type, str) or not event_type.strip():
            raise ValueError("event_type must be a non-empty string")
        if not callable(handler):
            raise ValueError("handler must be callable")
        if not isinstance(priority, EventPriority):
            raise ValueError("priority must be an EventPriority enum value")

        self.logger.info("eventbus.subscribe_attempt", {
            "event_type": event_type,
            "handler": getattr(handler, '__name__', str(handler)),
            "priority": priority.name
        })
        # ✅ CRITICAL FIX: Use cleanup lock for consistent synchronization
        async with self._cleanup_lock:
            # Check subscriber limits with cleanup
            await self._cleanup_dead_subscribers(event_type)

            if event_type not in self._subscribers:
                self._subscribers[event_type] = set()

            if len(self._subscribers[event_type]) >= self.max_subscribers_per_event:
                active_count = len([s for s in self._subscribers[event_type] if s.is_alive()])
                if active_count >= self.max_subscribers_per_event:
                    raise ValueError(f"Too many subscribers for event '{event_type}': {active_count}/{self.max_subscribers_per_event}")

            # Create weak subscriber with EventBus reference for automatic cleanup
            weak_subscriber = WeakSubscriber(handler, event_type, weakref.ref(self))
            self._subscribers[event_type].add(weak_subscriber)
            await self._update_metric_atomic("active_subscribers", 1)

            self.logger.info("eventbus.subscribe_completed", {
                "event_type": event_type,
                "handler": weak_subscriber.handler_name,
                "total_subscribers": len(self._subscribers[event_type])
            })

            # Removed debug logging for performance - subscription is normal operation

    async def _start_worker_pools(self):
        """✅ CRITICAL FIX: Start worker pools for each priority level with race condition fix"""
        async with self._worker_lock:
            if self._worker_pools_started:
                return

            for priority, worker_count in self._worker_counts.items():
                self._worker_tasks[priority] = []
                for worker_id in range(worker_count):
                    task = asyncio.create_task(
                        self._priority_worker(priority, worker_id),
                        name=f"EventBus-Worker-{priority.name}-{worker_id}"
                    )
                    self._worker_tasks[priority].append(task)
                    self._active_tasks.add(task)

            self._worker_pools_started = True
            self.logger.info("eventbus.worker_pools_started", {
                "total_workers": sum(self._worker_counts.values()),
                "worker_distribution": self._worker_counts
            })

    def _is_critical_error(self, error: Exception) -> bool:
        """Classify errors as critical (require attention) or ordinary"""
        critical_types = (SystemError, MemoryError, OSError, RuntimeError)
        return isinstance(error, critical_types) or 'critical' in str(type(error)).lower()

    async def _priority_worker(self, priority: EventPriority, worker_id: int):
        """✅ CRITICAL FIX: Dedicated worker for specific priority level with error classification"""
        queue = self._priority_queues[priority]
        worker_name = f"{priority.name}-{worker_id}"

        # Removed debug logging for performance - worker start is normal

        try:
            while not self._shutdown_requested:
                try:
                    # Get event from priority-specific queue with trading-optimized timeout
                    event = await asyncio.wait_for(queue.get(), timeout=1.0)  # ✅ TRADING FIX: Reduced to 1s for faster shutdown checks

                    # Process event with batch timeout
                    await self._process_queued_event_optimized(event)
                    queue.task_done()

                except asyncio.TimeoutError:
                    # Timeout is normal - allows checking shutdown
                    continue
                except Exception as e:
                    if self._is_critical_error(e):
                        self.logger.error("eventbus.worker_critical_error", {
                            "worker_name": worker_name,
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "action": "worker_may_need_restart"
                        })
                        # For critical errors, break the loop to stop this worker
                        break
                    else:
                        self.logger.warning("eventbus.worker_recoverable_error", {
                            "worker_name": worker_name,
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "action": "continuing"
                        })

        except Exception as e:
            self.logger.error("eventbus.worker_fatal_error", {
                "worker_name": worker_name,
                "error": str(e),
                "error_type": type(e).__name__,
                "action": "worker_terminated"
            })
        finally:
            # Removed debug logging for performance - worker stop is normal
            pass
    
    async def _process_queued_event_optimized(self, event: QueuedEvent):
        """✅ CRITICAL FIX: Optimized event processing with batch timeout"""
        if event.event_type not in self._subscribers:
            return

        alive_subscribers = [s for s in self._subscribers[event.event_type] if s.is_alive()]
        if not alive_subscribers:
            return

        # Use batch timeout instead of per-handler timeout
        await self._process_with_batch_timeout(event.event_type, event.data, alive_subscribers)

    async def _process_with_batch_timeout(self, event_type: str, data: Any, subscribers: List[WeakSubscriber]):
        """✅ CRITICAL FIX: Batch timeout for all handlers together with dynamic scaling"""
        start_time = time.time()

        # ✅ TRADING FIX: Dynamic timeout scaled for trading latency requirements
        # Base timeout + 10ms per subscriber, max 1s to maintain trading performance
        dynamic_timeout = min(self.default_timeout + (len(subscribers) * 0.01), 1.0)

        # Create all tasks for parallel execution
        tasks = []
        for subscriber in subscribers:
            handler = subscriber.get_handler()
            if handler:
                # No individual timeout - batch timeout handles all
                task = asyncio.create_task(
                    self._safe_handler_exec_batch(handler, data, subscriber),
                    name=f"Handler-{subscriber.handler_name}"
                )
                tasks.append(task)

        if not tasks:
            return

        try:
            # ✅ CRITICAL: Batch timeout for ALL handlers together with dynamic scaling
            done, pending = await asyncio.wait(
                tasks,
                timeout=dynamic_timeout,  # Dynamic timeout based on subscriber count
                return_when=asyncio.ALL_COMPLETED
            )
            
            # Cancel any pending tasks
            for task in pending:
                task.cancel()
                await self._update_metric_atomic("total_timeouts", 1)
                self.logger.warning("eventbus.handler_timeout", {"event_type": event_type})

            # Collect results
            success_count = 0
            for task in done:
                try:
                    await task  # Get result or exception
                    success_count += 1
                    await self._update_metric_atomic("total_processed", 1)
                except Exception as e:
                    await self._update_metric_atomic("total_failed", 1)
                    self.logger.error("eventbus.handler_failed", {"event_type": event_type, "error": str(e)})

            # Removed debug logging for performance - batch processing is hot path

        except Exception as e:
            await self._update_metric_atomic("total_failed", 1)
            self.logger.error("eventbus.batch_processing_error", {"event_type": event_type, "error": str(e)})
        
        # Update metrics
        processing_time = (time.time() - start_time) * 1000
        self._processing_times.append(processing_time)
        self._update_avg_processing_time()

    async def _safe_handler_exec_batch(self, handler: Callable[[Any], Coroutine], data: Any, subscriber: WeakSubscriber):
        """✅ CRITICAL FIX: Handler execution for batch processing (no individual timeout)"""
        start_time = time.time()
        
        try:
            # ✅ IMPORTANT: Only use circuit breaker for handlers with error history
            if self._should_use_circuit_breaker(subscriber.event_type):
                circuit_breaker = self._get_or_create_circuit_breaker(subscriber.event_type)
                await circuit_breaker.call(handler, data)
            else:
                # Direct call for stable handlers
                await handler(data)
                
            # Update success metrics
            processing_time = (time.time() - start_time) * 1000
            subscriber.update_metrics(processing_time)

            if processing_time >= self.slow_handler_threshold_ms:
                now = time.time()
                last_log = self._slow_handler_last_log.get(subscriber.handler_name, 0.0)
                if now - last_log >= self._slow_handler_log_interval:
                    total_queue_size = sum(queue.qsize() for queue in self._priority_queues.values())
                    avg_time = subscriber.total_processing_time / subscriber.call_count if subscriber.call_count else processing_time
                    self.logger.warning("eventbus.slow_handler_detected", {
                        "event_type": subscriber.event_type,
                        "handler": subscriber.handler_name,
                        "processing_time_ms": round(processing_time, 2),
                        "avg_processing_time_ms": round(avg_time, 2),
                        "call_count": subscriber.call_count,
                        "queue_size": total_queue_size,
                        "threshold_ms": self.slow_handler_threshold_ms
                    })
                    self._slow_handler_last_log[subscriber.handler_name] = now

        except Exception as e:
            # Handle failures but don't block batch
            subscriber._error_count += 1  # Track errors for circuit breaker
            self.logger.error("eventbus.handler_failed", {"handler_name": subscriber.handler_name, "error": str(e)})
            raise  # Re-raise for batch handler to count

    def _should_use_circuit_breaker(self, event_type: str) -> bool:
        """✅ OPTIMIZATION: Only use circuit breaker for error-prone handlers"""
        if event_type not in self._subscribers:
            return False

        # Check error rate from subscribers
        total_calls = 0
        total_errors = 0

        for subscriber in self._subscribers[event_type]:
            if subscriber.is_alive() and subscriber.call_count > 10:  # Only for handlers with history
                total_calls += subscriber.call_count
                total_errors += subscriber._error_count

        if total_calls == 0:
            return False

        # Use circuit breaker if error rate > 10%
        error_rate = total_errors / total_calls if total_calls > 0 else 0
        return error_rate > 0.1

    async def _get_or_create_circuit_breaker(self, event_type: str) -> 'CircuitBreaker':
        """✅ FIXED: Controlled circuit breaker creation with TTL"""
        async with self._ttl_lock:
            if event_type not in self._circuit_breakers:
                self._circuit_breakers[event_type] = CircuitBreaker(
                    failure_threshold=5,
                    timeout=60.0,
                    name=f"EventBus-{event_type}"
                )
                self._circuit_breaker_ttl[event_type] = time.time()

            return self._circuit_breakers[event_type]
    
    async def _cleanup_dead_subscribers(self, event_type: Optional[str] = None):
        """
        ✅ CRITICAL FIX: Async cleanup to prevent blocking event loop
        
        Args:
            event_type: Specific event type to clean, or None for all
        """
        events_to_clean = [event_type] if event_type else list(self._subscribers.keys())
        cleaned_count = 0
        
        for event in events_to_clean:
            if event in self._subscribers:
                alive_subscribers = {s for s in self._subscribers[event] if s.is_alive()}
                cleaned_count += len(self._subscribers[event]) - len(alive_subscribers)

                if alive_subscribers:
                    self._subscribers[event] = alive_subscribers
                else:
                    del self._subscribers[event]
                
                # ✅ CRITICAL FIX: Yield control to prevent blocking
                await asyncio.sleep(0)
        
        # Cleanup empty rate limiters to prevent memory leaks
        # Remove rate limiters for event types that no longer have subscribers and are empty
        empty_limiters = [
            et for et, limiter in self._rate_limiters.items() 
            if not limiter and et not in self._subscribers
        ]
        for et in empty_limiters:
            del self._rate_limiters[et]
            if et in self._rate_limiter_ttl:
                del self._rate_limiter_ttl[et]
            # Removed debug logging for performance - cleanup is normal operation
        
        # ✅ IMPORTANT FIX: TTL-based cleanup for circuit breakers
        await self._cleanup_expired_structures()
        
        # Cleanup circuit breakers for removed event types
        # Always check circuit breakers, not just when empty_limiters exist
        dead_circuit_breakers = [
            event_type for event_type in self._circuit_breakers.keys()
            if event_type not in self._subscribers
        ]
        for event_type in dead_circuit_breakers:
            del self._circuit_breakers[event_type]
            if event_type in self._circuit_breaker_ttl:
                del self._circuit_breaker_ttl[event_type]
            # Removed debug logging for performance - cleanup is normal operation
        
        if cleaned_count > 0:
            await self._update_metric_atomic("dead_subscribers_cleaned", cleaned_count)
            await self._update_metric_atomic("active_subscribers", -cleaned_count)
            # Removed debug logging for performance - cleanup is normal operation

    async def _cleanup_expired_structures(self):
        """✅ CRITICAL FIX: Aggressive async TTL-based cleanup for auxiliary structures"""
        async with self._ttl_lock:
            now = time.time()

            # ✅ CRITICAL FIX: Yield control during cleanup to prevent blocking
            await asyncio.sleep(0)

            # ✅ CRITICAL FIX: More aggressive cleanup - check all structures every time
            # Cleanup expired rate limiters with TTL (10 seconds)
            expired_rate_limiters = [
                key for key, last_access in self._rate_limiter_ttl.items()
                if now - last_access > self._ttl_cleanup_interval
            ]
            for key in expired_rate_limiters:
                if key in self._rate_limiters:
                    del self._rate_limiters[key]
                del self._rate_limiter_ttl[key]

            # ✅ CRITICAL FIX: Cleanup ALL circuit breakers for inactive event types
            active_event_types = set(self._subscribers.keys())
            expired_circuit_breakers = [
                event_type for event_type in self._circuit_breakers.keys()
                if event_type not in active_event_types or
                (event_type in self._circuit_breaker_ttl and
                 now - self._circuit_breaker_ttl[event_type] > self._ttl_cleanup_interval)
            ]
            for event_type in expired_circuit_breakers:
                if event_type in self._circuit_breakers:
                    del self._circuit_breakers[event_type]
                if event_type in self._circuit_breaker_ttl:
                    del self._circuit_breaker_ttl[event_type]

            # ✅ CRITICAL FIX: Aggressive cleanup of publisher rate limits
            expired_publisher_limits = []
            for key, limiter in self._publisher_rate_limits.items():
                if not limiter:  # Empty deque
                    expired_publisher_limits.append(key)
                else:
                    # Check if oldest entry is expired (10 seconds)
                    if limiter and limiter[0] < now - self._ttl_cleanup_interval:
                        # Clean old entries more aggressively
                        while limiter and limiter[0] < now - self._ttl_cleanup_interval:
                            limiter.popleft()
                        if not limiter:
                            expired_publisher_limits.append(key)

            for key in expired_publisher_limits:
                del self._publisher_rate_limits[key]
                if key in self._publisher_rate_limits_ttl:
                    del self._publisher_rate_limits_ttl[key]

            # ✅ CRITICAL FIX: More aggressive dead letter queue cleanup
            if len(self._dead_letter_queue) > 100:  # Even more aggressive - 100
                while len(self._dead_letter_queue) > 100:
                    self._dead_letter_queue.popleft()

            # ✅ CRITICAL FIX: More aggressive processing times cleanup
            if len(self._processing_times) > 200:  # Even more aggressive - 200
                while len(self._processing_times) > 200:
                    self._processing_times.popleft()
    
    def _should_cleanup(self) -> bool:
        """Check if cleanup should be performed"""
        return time.time() - self._last_cleanup_time > self.cleanup_interval_seconds
    
    async def _rate_limit_check(self, event_type: str, publisher_id: Optional[str] = None) -> bool:
        """
        ✅ IMPORTANT FIX: Publisher-based rate limiting for fairness.

        Args:
            event_type: Event type to check
            publisher_id: Publisher identifier (optional)

        Returns:
            True if event should be processed, False if rate limited
        """
        async with self._ttl_lock:
            now = time.time()

            # Global rate limiting per event type (fallback)
            if event_type in self._rate_limiters:
                rate_limiter = self._rate_limiters[event_type]
            else:
                # Only create if we have active subscribers for this event type
                if event_type not in self._subscribers:
                    return True  # No subscribers = no rate limiting needed
                rate_limiter = deque(maxlen=1000)  # Global limit: 1000/second
                self._rate_limiters[event_type] = rate_limiter
                self._rate_limiter_ttl[event_type] = now

            # Remove old entries from global limiter
            while rate_limiter and rate_limiter[0] < now - 1.0:
                rate_limiter.popleft()

            # Check global limit first
            if len(rate_limiter) >= 1000:
                self.logger.warning("eventbus.global_rate_limit_exceeded", {"event_type": event_type})
                return False

            # ✅ IMPORTANT FIX: Publisher-specific rate limiting
            if publisher_id:
                publisher_key = f"{event_type}:{publisher_id}"

                if publisher_key in self._publisher_rate_limits:
                    publisher_limiter = self._publisher_rate_limits[publisher_key]
                else:
                    publisher_limiter = deque(maxlen=100)  # Per-publisher limit: 100/second
                    self._publisher_rate_limits[publisher_key] = publisher_limiter

                # Remove old entries from publisher limiter
                while publisher_limiter and publisher_limiter[0] < now - 1.0:
                    publisher_limiter.popleft()

                # Check publisher-specific limit
                if len(publisher_limiter) >= 100:
                    self.logger.warning("eventbus.publisher_rate_limit_exceeded", {"publisher_key": publisher_key})
                    return False

                publisher_limiter.append(now)

            # Update global limiter and TTL
            rate_limiter.append(now)
            self._rate_limiter_ttl[event_type] = now
            return True

    async def publish_with_trace(self, event_type: str, data: Any, trace_id: Optional[str] = None, priority: EventPriority = EventPriority.NORMAL, publisher_id: Optional[str] = None) -> str:
        """
        Publish an event with a trace ID for end-to-end tracking.
        If no trace_id is provided, a new one is generated.
        Returns the trace_id used for the event.
        """
        if trace_id is None:
            trace_id = str(uuid.uuid4())
        
        # Ensure data is a dict to attach trace_id
        if not isinstance(data, dict):
            self.logger.warning("eventbus.trace_id_attach_failed", {"event_type": event_type, "reason": "data_not_dict"})
            await self.publish(event_type, data, priority, publisher_id)
            return trace_id

        if 'metadata' not in data:
            data['metadata'] = {}
        data['metadata']['trace_id'] = trace_id

        await self.publish(event_type, data, priority, publisher_id)
        return trace_id

    async def publish(self, event_type: str, data: Any, priority: EventPriority = EventPriority.NORMAL, publisher_id: Optional[str] = None):
        """
        ✅ CRITICAL FIX: Publish with consistent lock ordering to prevent deadlocks.

        Args:
            event_type: Event type to publish
            data: Event data
            priority: Event priority for processing order
            publisher_id: Publisher identifier for rate limiting
        """
        # Input validation
        if not isinstance(event_type, str) or not event_type.strip():
            raise ValueError("event_type must be a non-empty string")
        if not isinstance(priority, EventPriority):
            raise ValueError("priority must be an EventPriority enum value")
        if publisher_id is not None and not isinstance(publisher_id, str):
            raise ValueError("publisher_id must be a string or None")

        if self._shutdown_requested:
            self.logger.warning("eventbus.publish_shutdown_blocked", {"event_type": event_type})
            return

        # ✅ CRITICAL FIX: Start worker pools OUTSIDE of any locks to prevent deadlock
        if not self._worker_pools_started:
            await self._start_worker_pools()

        # ✅ IMPORTANT: Publisher-based rate limiting (outside lock)
        if not await self._rate_limit_check(event_type, publisher_id):
            self._update_metric_atomic("total_dropped", 1)
            return

        # ✅ SUGGESTED FIX: Get alive subscribers inside the cleanup lock to ensure consistency
        # This prevents another thread from modifying the list between lock releases.
        alive_subscribers = []

        # ✅ CRITICAL FIX: Consistent lock ordering - acquire cleanup_lock first, then publish_lock
        cleanup_needed = self._should_cleanup()

        async with self._cleanup_lock:
            if cleanup_needed:
                await self._cleanup_dead_subscribers()
                self._last_cleanup_time = time.time()

            # Get alive subscribers after cleanup but still inside the lock
            if event_type in self._subscribers:
                alive_subscribers = [s for s in self._subscribers[event_type] if s.is_alive()]

        if not alive_subscribers:
            return

        # Now acquire publish_lock for the actual publishing (no nesting)
        async with self._publish_lock:
                trace_id = data.get('metadata', {}).get('trace_id') if isinstance(data, dict) else None
                log_msg = f"Publishing event '{event_type}'"
                if trace_id:
                    log_msg += f" with trace_id '{trace_id}'"
                if publisher_id:
                    log_msg += f" from publisher '{publisher_id}'"

                # Removed debug logging for performance - publishing is hot path
                await self._update_metric_atomic("total_published", 1)

                # ✅ CRITICAL FIX: Use bucketed queues for true priority processing
                if self.enable_backpressure and len(alive_subscribers) > 10:  # Lower threshold for queue usage
                    await self._process_with_bucketed_queues(event_type, data, priority, trace_id)
                else:
                    # Direct processing for small subscriber counts with batch timeout
                    await self._process_with_batch_timeout(event_type, data, alive_subscribers)
    
    async def _process_direct(self, event_type: str, data: Any, subscribers: List[WeakSubscriber]):
        """✅ UPDATED: Process events directly with batch timeout"""
        await self._process_with_batch_timeout(event_type, data, subscribers)

    async def _process_with_bucketed_queues(self, event_type: str, data: Any, priority: EventPriority, trace_id: Optional[str] = None):
        """✅ CRITICAL FIX: Use bucketed queues for O(1) priority processing"""
        queue = self._priority_queues[priority]
        
        # Check queue capacity
        if queue.full():
            if priority == EventPriority.LOW:
                # Drop low priority events immediately
                self.logger.warning("eventbus.low_priority_dropped", {"event_type": event_type, "reason": "queue_full"})
                self.metrics.total_dropped += 1
                return
            elif priority == EventPriority.NORMAL:
                # Try to make room by clearing some LOW priority events
                await self._clear_low_priority_events()
                if queue.full():
                    self.logger.warning("eventbus.normal_priority_dropped", {"event_type": event_type, "reason": "queue_still_full"})
                    self.metrics.total_dropped += 1
                    return
            # CRITICAL and HIGH priority events always go through (will block if needed)
        
        # Create queued event
        queued_event = QueuedEvent(
            event_type=event_type,
            data=data,
            priority=priority,
            timestamp=time.time(),
            trace_id=trace_id
        )
        
        try:
            # Put in appropriate priority queue - workers will pick it up
            if priority in [EventPriority.CRITICAL, EventPriority.HIGH]:
                # High priority events can block if needed
                await queue.put(queued_event)
            else:
                # Normal/Low priority - don't block
                queue.put_nowait(queued_event)
            
            # Removed debug logging for performance - queueing is hot path
            
        except asyncio.QueueFull:
            self._update_metric_atomic("total_dropped", 1)
            self.logger.warning("eventbus.queue_full_dropped", {"priority": priority.name, "event_type": event_type})

    async def _clear_low_priority_events(self):
        """✅ OPTIMIZATION: O(1) clearing of low priority queue"""
        low_queue = self._priority_queues[EventPriority.LOW]
        dropped_count = 0
        
        # Fast clear - create new queue
        try:
            while True:
                low_queue.get_nowait()
                dropped_count += 1
        except asyncio.QueueEmpty:
            pass
        
        if dropped_count > 0:
            await self._update_metric_atomic("total_dropped", dropped_count)
            self.logger.info("eventbus.low_priority_cleared", {"dropped_count": dropped_count, "reason": "backpressure_relief"})
    
    async def _process_with_backpressure(self, event_type: str, data: Any, subscribers: List[WeakSubscriber], priority: EventPriority):
        """✅ LEGACY COMPATIBILITY: Redirect to bucketed queues for better performance"""
        await self._process_with_bucketed_queues(event_type, data, priority)
    
    
    
    def _update_avg_processing_time(self):
        """Update average processing time metric"""
        if self._processing_times:
            self.metrics.avg_processing_time_ms = sum(self._processing_times) / len(self._processing_times)

    async def _safe_handler_exec(self, handler: Callable[[Any], Coroutine], data: Any, subscriber: WeakSubscriber):
        """✅ LEGACY COMPATIBILITY: Redirect to batch execution for consistency"""
        await self._safe_handler_exec_batch(handler, data, subscriber)
    
    async def _execute_handler_with_timeout(self, handler: Callable[[Any], Coroutine], data: Any):
        """✅ LEGACY: Individual handler timeout (replaced by batch timeout)"""
        if self.default_timeout:
            try:
                await asyncio.wait_for(handler(data), timeout=self.default_timeout)
            except asyncio.TimeoutError:
                self.metrics.total_timeouts += 1
                self.logger.error("eventbus.handler_timeout_individual", {"timeout_seconds": self.default_timeout})
                
                # Add to dead letter queue for timeout events
                if isinstance(data, dict) and 'metadata' in data:
                    failed_event = QueuedEvent(
                        event_type="timeout_event",
                        data=data,
                        priority=EventPriority.LOW,
                        timestamp=time.time(),
                        trace_id=data['metadata'].get('trace_id'),
                        retry_count=data.get('retry_count', 0) + 1
                    )
                    self._dead_letter_queue.append(failed_event)
                raise  # Re-raise for circuit breaker to handle
        else:
            await handler(data)

    def unsubscribe(self, event_type: str, handler: Callable[[Any], Coroutine], silent: bool = False):
        """Unsubscribe a handler from an event type with proper cleanup.

        Args:
            event_type: Event type to unsubscribe from
            handler: Handler to remove
            silent: If True, suppress warnings for expected not-found cases
        """
        # Input validation
        if not isinstance(event_type, str) or not event_type.strip():
            raise ValueError("event_type must be a non-empty string")
        if not callable(handler):
            raise ValueError("handler must be callable")

        with self._sync_lock:
            if event_type in self._subscribers:
                subscribers_to_remove = []

                for subscriber in self._subscribers[event_type]:
                    current_handler = subscriber.get_handler()
                    if current_handler is handler:
                        subscribers_to_remove.append(subscriber)

                for subscriber in subscribers_to_remove:
                    self._subscribers[event_type].remove(subscriber)
                    # Removed metric update from sync unsubscribe for async safety
                    # self._update_metric_atomic("active_subscribers", -1)
                    # Removed debug logging for performance - unsubscribe is normal operation

                # Clean up empty event type lists
                if not self._subscribers[event_type]:
                    del self._subscribers[event_type]

                if not subscribers_to_remove:
                    self._update_metric_atomic("total_unsubscribe_not_found", 1)
                    if not silent:
                        handler_name = getattr(handler, '__name__', str(handler))
                        self.logger.debug("eventbus.unsubscribe_not_found", {"handler_name": handler_name, "event_type": event_type, "note": "normal_during_cleanup"})

    async def shutdown(self):
        """✅ CRITICAL FIX: Graceful shutdown with worker pool and cleanup task management"""
        self.logger.info("eventbus.shutdown_initiated", {})
        self._shutdown_requested = True


        if self._background_cleanup_task and not self._background_cleanup_task.done():
            self._background_cleanup_task.cancel()
            try:
                await asyncio.wait_for(self._background_cleanup_task, timeout=2.0)
            except asyncio.TimeoutError:
                self.logger.warning("eventbus.background_cleanup_timeout", {})
            except asyncio.CancelledError:
                pass

        # ✅ CRITICAL: Stop all worker pools
        for priority, workers in self._worker_tasks.items():
            self.logger.info("eventbus.stopping_workers", {"priority": priority.name, "worker_count": len(workers)})
            for worker in workers:
                if not worker.done():
                    worker.cancel()

        # Wait for workers to finish with timeout
        all_workers = []
        for workers in self._worker_tasks.values():
            all_workers.extend(workers)

        if all_workers:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*all_workers, return_exceptions=True),
                    timeout=2.0  # ✅ TRADING FIX: Faster shutdown for trading
                )
            except asyncio.TimeoutError:
                self.logger.warning("eventbus.worker_shutdown_timeout", {})
        
        # Legacy processor removed
        
        # ✅ CRITICAL: Process remaining critical events from all queues
        critical_events = []
        
        # Check all priority queues for critical events
        for priority, queue in self._priority_queues.items():
            if priority == EventPriority.CRITICAL:
                try:
                    while True:
                        event = queue.get_nowait()
                        critical_events.append(event)
                except asyncio.QueueEmpty:
                    pass
        
        # Legacy queue removed - no need to process
        
        if critical_events:
            self.logger.info("eventbus.processing_critical_events", {"count": len(critical_events)})
            for event in critical_events:
                await self._process_queued_event_optimized(event)

        # Clear all subscribers and reset metrics
        self.clear_all_subscribers()

        self.logger.info("eventbus.shutdown_completed", {
            "total_published": self.metrics.total_published,
            "total_processed": self.metrics.total_processed,
            "total_failed": self.metrics.total_failed,
            "total_dropped": self.metrics.total_dropped,
            "dead_subscribers_cleaned": self.metrics.dead_subscribers_cleaned,
            "workers_stopped": sum(len(workers) for workers in self._worker_tasks.values())
        })

    def clear_all_subscribers(self):
        """Clear all subscribers with proper cleanup."""
        # ✅ CRITICAL FIX: Use synchronous lock for thread safety
        with self._sync_lock:
            total_cleared = sum(len(subscribers) for subscribers in self._subscribers.values())
            self._subscribers.clear()
            # Removed metric update from sync clear_all_subscribers for async safety
            # self._update_metric_atomic("active_subscribers", -total_cleared)
            # Removed debug logging for performance - clearing subscribers is normal operation

    async def get_subscribers(self) -> dict[str, list[str]]:
        """Returns a dictionary of event types and their alive subscriber names."""
        # ✅ CRITICAL FIX: Use cleanup_lock for thread-safe reading
        async with self._cleanup_lock:
            result = {}
            for event_type, subscribers in self._subscribers.items():
                alive_subscribers = []
                for subscriber in subscribers:
                    if subscriber.is_alive():
                        alive_subscribers.append(subscriber.handler_name)
                if alive_subscribers:
                    result[event_type] = alive_subscribers
            return result
    
    async def get_metrics(self) -> EventMetrics:
        """Get current EventBus metrics for monitoring"""
        # ✅ CRITICAL FIX: Use cleanup_lock for thread-safe reading
        async with self._cleanup_lock:
            # Update active subscribers count
            active_count = 0
            for subscribers in self._subscribers.values():
                active_count += len([s for s in subscribers if s.is_alive()])
            self.metrics.active_subscribers = active_count

            return self.metrics
    
    def get_dead_letter_events(self) -> List[QueuedEvent]:
        """Get events from dead letter queue for inspection"""
        return list(self._dead_letter_queue)
    
    def clear_dead_letter_queue(self):
        """Clear the dead letter queue"""
        cleared_count = len(self._dead_letter_queue)
        self._dead_letter_queue.clear()
        self.logger.info("eventbus.dead_letter_cleared", {"cleared_count": cleared_count})
        
    async def get_memory_stats(self) -> Dict[str, int]:
        """✅ UPDATED: Get memory usage statistics including new structures"""
        # ✅ CRITICAL FIX: Use cleanup_lock for thread-safe reading of subscribers
        async with self._cleanup_lock:
            total_queue_size = sum(queue.qsize() for queue in self._priority_queues.values())
            active_workers = sum(
                len([w for w in workers if not w.done()])
                for workers in self._worker_tasks.values()
            )

            return {
                "subscribers_count": len(self._subscribers),
                "rate_limiters_count": len(self._rate_limiters),
                "publisher_rate_limits_count": len(self._publisher_rate_limits),
                "circuit_breakers_count": len(self._circuit_breakers),
                "legacy_queue_size": 0,  # Legacy queue removed
                "bucketed_queues_size": total_queue_size,
                "dead_letter_size": len(self._dead_letter_queue),
                "total_subscribers": sum(len(subs) for subs in self._subscribers.values()),
                "active_subscribers": sum(len([s for s in subs if s.is_alive()]) for subs in self._subscribers.values()),
                "active_workers": active_workers,
                "total_workers": sum(len(workers) for workers in self._worker_tasks.values()),
                "worker_distribution": {priority.name: len(workers) for priority, workers in self._worker_tasks.items()}
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """✅ UPDATED: Health check with new metrics"""
        # Clean up dead subscribers first
        await self._cleanup_dead_subscribers()

        memory_stats = await self.get_memory_stats()
        total_queue_size = memory_stats["bucketed_queues_size"] + memory_stats["legacy_queue_size"]

        # Check if system is healthy
        healthy = (
            not self._shutdown_requested and
            memory_stats["active_workers"] > 0 and
            total_queue_size < self.max_queue_size * 0.9
        )

        return {
            "healthy": healthy,
            "active_subscribers": memory_stats["active_subscribers"],
            "total_queue_size": total_queue_size,
            "bucketed_queues_size": memory_stats["bucketed_queues_size"],
            "legacy_queue_size": memory_stats["legacy_queue_size"],
            "dead_letter_size": memory_stats["dead_letter_size"],
            "rate_limiters_count": memory_stats["rate_limiters_count"],
            "publisher_rate_limits_count": memory_stats["publisher_rate_limits_count"],
            "circuit_breakers_count": memory_stats["circuit_breakers_count"],
            "active_workers": memory_stats["active_workers"],
            "total_workers": memory_stats["total_workers"],
            "worker_distribution": memory_stats["worker_distribution"],
            "is_processing": self._is_processing,
            "shutdown_requested": self._shutdown_requested,
            "memory_stats": memory_stats,
            "metrics": {
                "total_published": self.metrics.total_published,
                "total_processed": self.metrics.total_processed,
                "total_failed": self.metrics.total_failed,
                "total_dropped": self.metrics.total_dropped,
                "total_timeouts": self.metrics.total_timeouts,
                "avg_processing_time_ms": self.metrics.avg_processing_time_ms,
                "peak_queue_size": self.metrics.peak_queue_size
            }
        }
